{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFPzvU21vsDuyReOMQSv9U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mariam-Elbishbeashy/ANA_AI_Models/blob/main/Questionnaire-models%20%26%20datasets/Questionnaire_arabic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XP1nCoB-v9w3",
        "outputId": "e2e2c9ea-17a5-488c-e3c9-36650c17da2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "ØªÙˆÙ„ÙŠØ¯ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ANA Ø§Ù„Ù…Ø­Ø³Ù†Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø£ÙØ¶Ù„ 3 Ø´Ø®ØµÙŠØ§Øª\n",
            "============================================================\n",
            "ØªÙˆÙ„ÙŠØ¯ 100,000 Ø¹ÙŠÙ†Ø© Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©...\n",
            "  ØªÙ… ØªÙˆÙ„ÙŠØ¯ 10,000 Ø¹ÙŠÙ†Ø©...\n",
            "  ØªÙ… ØªÙˆÙ„ÙŠØ¯ 20,000 Ø¹ÙŠÙ†Ø©...\n",
            "  ØªÙ… ØªÙˆÙ„ÙŠØ¯ 30,000 Ø¹ÙŠÙ†Ø©...\n",
            "  ØªÙ… ØªÙˆÙ„ÙŠØ¯ 40,000 Ø¹ÙŠÙ†Ø©...\n",
            "  ØªÙ… ØªÙˆÙ„ÙŠØ¯ 50,000 Ø¹ÙŠÙ†Ø©...\n",
            "  ØªÙ… ØªÙˆÙ„ÙŠØ¯ 60,000 Ø¹ÙŠÙ†Ø©...\n",
            "  ØªÙ… ØªÙˆÙ„ÙŠØ¯ 70,000 Ø¹ÙŠÙ†Ø©...\n",
            "  ØªÙ… ØªÙˆÙ„ÙŠØ¯ 80,000 Ø¹ÙŠÙ†Ø©...\n",
            "  ØªÙ… ØªÙˆÙ„ÙŠØ¯ 90,000 Ø¹ÙŠÙ†Ø©...\n",
            "\n",
            "âœ… ØªÙ… Ø­ÙØ¸ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©: 'ana_dataset_probabilistic_arabic.csv'\n",
            "âœ… ØªÙ… Ø­ÙØ¸ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø£ÙØ¶Ù„ 3 Ø´Ø®ØµÙŠØ§Øª: 'ana_dataset_top3_arabic.csv'\n",
            "\n",
            "============================================================\n",
            "ØªØ­Ù„ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (Ø£ÙØ¶Ù„ 3 Ø´Ø®ØµÙŠØ§Øª)\n",
            "============================================================\n",
            "Ø¸Ù‡ÙˆØ± Ø§Ù„Ø´Ø®ØµÙŠØ§Øª ÙÙŠ Ø§Ù„Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰:\n",
            "--------------------------------------------------\n",
            "Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„             25,987 (  8.7% Ù…Ù† Ø§Ù„Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰)\n",
            "Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ              25,524 (  8.5% Ù…Ù† Ø§Ù„Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰)\n",
            "Ø§Ù„Ù†Ø§Ù‚Ø¯               25,495 (  8.5% Ù…Ù† Ø§Ù„Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰)\n",
            "Ø§Ù„Ù…ÙØ±Ø¶ÙŠ              20,087 (  6.7% Ù…Ù† Ø§Ù„Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰)\n",
            "Ø§Ù„Ø­ÙŠØ±Ø§Ù†              19,936 (  6.6% Ù…Ù† Ø§Ù„Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰)\n",
            "Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…             19,880 (  6.6% Ù…Ù† Ø§Ù„Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰)\n",
            "Ø§Ù„Ø¨Ø§Ø±Ø¯               19,834 (  6.6% Ù…Ù† Ø§Ù„Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰)\n",
            "Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„           19,815 (  6.6% Ù…Ù† Ø§Ù„Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰)\n",
            "Ø§Ù„Ø¢ÙƒÙ„                14,571 (  4.9% Ù…Ù† Ø§Ù„Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰)\n",
            "Ø§Ù„Ù„Ø§Ø¹Ø¨               14,460 (  4.8% Ù…Ù† Ø§Ù„Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰)\n",
            "Ø§Ù„Ø®Ø¬ÙˆÙ„               12,943 (  4.3% Ù…Ù† Ø§Ù„Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰)\n",
            "Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­         12,773 (  4.3% Ù…Ù† Ø§Ù„Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰)\n",
            "Ø§Ù„ÙˆØ­ÙŠØ¯               11,874 (  4.0% Ù…Ù† Ø§Ù„Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰)\n",
            "Ø§Ù„Ù…Ù‡Ù…ÙÙ„              11,671 (  3.9% Ù…Ù† Ø§Ù„Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰)\n",
            "Ø§Ù„Ù…Ø±Ù‡ÙÙ‚              11,443 (  3.8% Ù…Ù† Ø§Ù„Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰)\n",
            "Ø§Ù„Ø®Ø§ÙŠÙ               11,419 (  3.8% Ù…Ù† Ø§Ù„Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰)\n",
            "Ø§Ù„Ù…Ø¹ØªÙ…Ø¯              11,154 (  3.7% Ù…Ù† Ø§Ù„Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰)\n",
            "Ø§Ù„ØºÙŠÙˆØ±               11,134 (  3.7% Ù…Ù† Ø§Ù„Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰)\n",
            "\n",
            "Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø«Ù‚Ø©:\n",
            "Ù…ØªÙˆØ³Ø· Ø§Ù„Ø«Ù‚Ø©: 0.58\n",
            "Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© (>0.7): 246 Ø¹ÙŠÙ†Ø©\n",
            "Ø«Ù‚Ø© Ù…ØªÙˆØ³Ø·Ø© (0.4-0.7): 99,754 Ø¹ÙŠÙ†Ø©\n",
            "Ø«Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø© (<0.4): 0 Ø¹ÙŠÙ†Ø©\n",
            "\n",
            "Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ø£ÙˆÙ„ ÙˆØ§Ù„Ø«Ø§Ù†ÙŠ:\n",
            "Ù…ØªÙˆØ³Ø· Ø§Ù„ÙØ±Ù‚: 0.34\n",
            "ÙØ§Ø¦Ø² ÙˆØ§Ø¶Ø­ (ÙØ±Ù‚ > 0.2): 89,576 Ø¹ÙŠÙ†Ø©\n",
            "ØªÙ†Ø§ÙØ³ Ø´Ø¯ÙŠØ¯ (ÙØ±Ù‚ < 0.1): 0 Ø¹ÙŠÙ†Ø©\n",
            "\n",
            "Ù†ÙˆØ¹ÙŠØ© Ø§Ù„Ø´Ø®ØµÙŠØ© ÙÙŠ Ø§Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ø£ÙˆÙ„:\n",
            "  Ù…Ù†ÙÙŠ: 44,567 Ø¹ÙŠÙ†Ø© (44.6%)\n",
            "  Ù…Ø¯ÙŠØ±: 38,770 Ø¹ÙŠÙ†Ø© (38.8%)\n",
            "  Ø±Ø¬Ù„ Ø¥Ø·ÙØ§Ø¡: 16,663 Ø¹ÙŠÙ†Ø© (16.7%)\n",
            "\n",
            "Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨:\n",
            "Ø§Ù„Ù…ÙŠØ²Ø§Øª: 29 Ø¹Ù…ÙˆØ¯\n",
            "Ø§Ù„Ø¹ÙŠÙ†Ø§Øª: 100,000\n",
            "Ø§Ù„Ø£Ù‡Ø¯Ø§Ù: 3 Ø£Ø¹Ù…Ø¯Ø© (Ø§Ù„Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰)\n",
            "\n",
            "============================================================\n",
            "Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø§Ù‡Ø²Ø© Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬!\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:\n",
            "============================================================\n",
            "\n",
            "Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ (5 Ø£Ø³Ø·Ø±):\n",
            "    Q1       Q2   Q3       Q4   Q5\n",
            "0    0   51-80%  0,5  81-100%    4\n",
            "1  1,2   51-80%    5  81-100%    1\n",
            "2    3  81-100%  1,4   51-80%    3\n",
            "3    1  81-100%  1,2  81-100%  0,1\n",
            "4  0,2    0-20%    0   21-50%  4,5\n",
            "\n",
            "Ø§Ù„Ø´Ø®ØµÙŠØ§Øª Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ (5 Ø£Ø³Ø·Ø±):\n",
            "    top1_char  top1_prob   top2_char  top2_prob top3_char  top3_prob\n",
            "0      Ø§Ù„Ø®Ø¬ÙˆÙ„       0.95    Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„       0.49   Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ       0.41\n",
            "1  Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„       0.88     Ø§Ù„Ù…ÙØ±Ø¶ÙŠ       0.51    Ø§Ù„ØºÙŠÙˆØ±       0.47\n",
            "2      Ø§Ù„Ù„Ø§Ø¹Ø¨       0.92    Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„       0.53  Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…       0.32\n",
            "3      Ø§Ù„ØºÙŠÙˆØ±       0.92  Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„       0.47    Ø§Ù„Ø¨Ø§Ø±Ø¯       0.20\n",
            "4     Ø§Ù„Ù…ÙØ±Ø¶ÙŠ       0.81    Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…       0.60    Ø§Ù„Ø®Ø¬ÙˆÙ„       0.38\n",
            "\n",
            "Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£ÙˆÙ„ÙŠ Ù„Ù„Ø´Ø®ØµÙŠØ§Øª:\n",
            "----------------------------------------\n",
            "Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…            :  5,664 (5.7%)\n",
            "Ø§Ù„Ø®Ø¬ÙˆÙ„              :  5,650 (5.7%)\n",
            "Ø§Ù„Ù…Ù‡Ù…ÙÙ„             :  5,611 (5.6%)\n",
            "Ø§Ù„Ù…Ø±Ù‡ÙÙ‚             :  5,606 (5.6%)\n",
            "Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­        :  5,602 (5.6%)\n",
            "Ø§Ù„ÙˆØ­ÙŠØ¯              :  5,587 (5.6%)\n",
            "Ø§Ù„Ù…ÙØ±Ø¶ÙŠ             :  5,585 (5.6%)\n",
            "Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„          :  5,565 (5.6%)\n",
            "Ø§Ù„Ø¨Ø§Ø±Ø¯              :  5,563 (5.6%)\n",
            "Ø§Ù„Ù„Ø§Ø¹Ø¨              :  5,563 (5.6%)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "# ==================== ENHANCED CHARACTER DEFINITIONS (ARABIC) ====================\n",
        "CHARACTERS_AR = [\n",
        "    # ğŸ›¡ï¸ Ø§Ù„Ù…Ø¯ÙŠØ±ÙŠÙ† (Ø§Ù„Ø­Ø±Ø§Ø³ Ø§Ù„Ø§Ø³ØªØ¨Ø§Ù‚ÙŠÙˆÙ†)\n",
        "    \"Ø§Ù„Ù†Ø§Ù‚Ø¯\", \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\", \"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ\", \"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\",\n",
        "    \"Ø§Ù„Ø¨Ø§Ø±Ø¯\", \"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\", \"Ø§Ù„Ø­ÙŠØ±Ø§Ù†\",\n",
        "\n",
        "    # ğŸ”¥ Ø±Ø¬Ø§Ù„ Ø§Ù„Ø¥Ø·ÙØ§Ø¡ (Ø§Ù„Ø­Ø±Ø§Ø³ Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠÙˆÙ†)\n",
        "    \"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\", \"Ø§Ù„Ø¢ÙƒÙ„\", \"Ø§Ù„Ù„Ø§Ø¹Ø¨\",\n",
        "\n",
        "    # ğŸ˜¥ Ø§Ù„Ù…Ù†ÙÙŠÙŠÙ† (Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ Ø§Ù„Ø¬Ø±ÙŠØ­Ø©)\n",
        "    \"Ø§Ù„ÙˆØ­ÙŠØ¯\", \"Ø§Ù„Ø®Ø§ÙŠÙ\", \"Ø§Ù„Ù…Ù‡Ù…ÙÙ„\", \"Ø§Ù„Ø®Ø¬ÙˆÙ„\",\n",
        "    \"Ø§Ù„Ù…Ø±Ù‡ÙÙ‚\", \"Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\", \"Ø§Ù„ØºÙŠÙˆØ±\", \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\"\n",
        "]\n",
        "\n",
        "# Character archetype groups (ARABIC)\n",
        "MANAGERS_AR = [\"Ø§Ù„Ù†Ø§Ù‚Ø¯\", \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\", \"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ\", \"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\",\n",
        "               \"Ø§Ù„Ø¨Ø§Ø±Ø¯\", \"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\", \"Ø§Ù„Ø­ÙŠØ±Ø§Ù†\"]\n",
        "FIREFIGHTERS_AR = [\"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\", \"Ø§Ù„Ø¢ÙƒÙ„\", \"Ø§Ù„Ù„Ø§Ø¹Ø¨\"]\n",
        "EXILES_AR = [\"Ø§Ù„ÙˆØ­ÙŠØ¯\", \"Ø§Ù„Ø®Ø§ÙŠÙ\", \"Ø§Ù„Ù…Ù‡Ù…ÙÙ„\", \"Ø§Ù„Ø®Ø¬ÙˆÙ„\",\n",
        "             \"Ø§Ù„Ù…Ø±Ù‡ÙÙ‚\", \"Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\", \"Ø§Ù„ØºÙŠÙˆØ±\", \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\"]\n",
        "\n",
        "# English to Arabic mapping for reference\n",
        "EN_TO_AR = {\n",
        "    \"Inner Critic\": \"Ø§Ù„Ù†Ø§Ù‚Ø¯\",\n",
        "    \"Perfectionist\": \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\",\n",
        "    \"People Pleaser\": \"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ\",\n",
        "    \"Controller\": \"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\",\n",
        "    \"Stoic Part\": \"Ø§Ù„Ø¨Ø§Ø±Ø¯\",\n",
        "    \"Workaholic\": \"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\",\n",
        "    \"Confused Part\": \"Ø§Ù„Ø­ÙŠØ±Ø§Ù†\",\n",
        "    \"Procrastinator\": \"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\",\n",
        "    \"Overeater/Binger\": \"Ø§Ù„Ø¢ÙƒÙ„\",\n",
        "    \"Excessive Gamer\": \"Ø§Ù„Ù„Ø§Ø¹Ø¨\",\n",
        "    \"Lonely Part\": \"Ø§Ù„ÙˆØ­ÙŠØ¯\",\n",
        "    \"Fearful Part\": \"Ø§Ù„Ø®Ø§ÙŠÙ\",\n",
        "    \"Neglected Part\": \"Ø§Ù„Ù…Ù‡Ù…ÙÙ„\",\n",
        "    \"Ashamed Part\": \"Ø§Ù„Ø®Ø¬ÙˆÙ„\",\n",
        "    \"Overwhelmed Part\": \"Ø§Ù„Ù…Ø±Ù‡ÙÙ‚\",\n",
        "    \"Dependent Part\": \"Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\",\n",
        "    \"Jealous Part\": \"Ø§Ù„ØºÙŠÙˆØ±\",\n",
        "    \"Wounded Child\": \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\"\n",
        "}\n",
        "\n",
        "AR_TO_EN = {v: k for k, v in EN_TO_AR.items()}\n",
        "\n",
        "# ==================== ARABIC QUESTION MAPPINGS ====================\n",
        "QUESTION_MAPPINGS_AR = {\n",
        "    'Q1': {\n",
        "        0: [\"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\", \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\"],        # Ø£Ø¹Ù…Ù„ Ù‚ÙˆØ§Ø¦Ù… ÙˆØ®Ø·Ø·\n",
        "        1: [\"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\", \"Ø§Ù„Ø¨Ø§Ø±Ø¯\"],       # Ø£Ø¶ØºØ· Ø¹Ù„Ù‰ Ù†ÙØ³ÙŠ ÙˆØ£Ø®Ù„Øµ Ø§Ù„Ø­Ø§Ø¬Ø§Øª\n",
        "        2: [\"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ\", \"Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\"],         # Ø£ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ù†Ø§Ø³\n",
        "        3: [\"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\", \"Ø§Ù„Ù„Ø§Ø¹Ø¨\"],         # Ø£Ù„Ø§Ù‚ÙŠ Ø­Ø§Ø¬Ø§Øª Ø£Ù„Ù‡Ù‘ÙŠ Ø¨ÙŠÙ‡Ø§ Ø¯Ù…Ø§ØºÙŠ\n",
        "        4: [\"Ø§Ù„Ø¨Ø§Ø±Ø¯\", \"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\"],         # Ø£Ù‚ÙˆÙ„ Ù„Ù†ÙØ³ÙŠ \"Ø£Ù†Ø§ Ù‡Ù‚Ø¯Ø± Ø£ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹\"\n",
        "        5: [\"Ø§Ù„Ø­ÙŠØ±Ø§Ù†\", \"Ø§Ù„ÙˆØ­ÙŠØ¯\"]          # Ù…Ø­ØªØ§Ø¬ ÙˆÙ‚Øª Ù‡Ø§Ø¯ÙŠ Ù„ÙˆØ­Ø¯ÙŠ\n",
        "    },\n",
        "\n",
        "    'Q2_slider': {\n",
        "        '0-20%': [],  # Ø¬Ø²Ø¡ ÙƒÙ…Ø§Ù„ÙŠ Ø¨Ø³ÙŠØ·\n",
        "        '21-50%': [\"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\"],  # ÙÙŠ Ø­Ø§Ø¬Ø§Øª Ù…Ù† Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠØ©\n",
        "        '51-80%': [\"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\", \"Ø§Ù„Ù†Ø§Ù‚Ø¯\"],  # Ø¬Ø²Ø¡ ÙƒÙ…Ø§Ù„ÙŠ Ù†Ø´ÙŠØ·\n",
        "        '81-100%': [\"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\", \"Ø§Ù„Ù†Ø§Ù‚Ø¯\", \"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\"]  # Ø¬Ø²Ø¡ ÙƒÙ…Ø§Ù„ÙŠ Ù…Ø³ÙŠØ·Ø±\n",
        "    },\n",
        "\n",
        "    'Q3': {\n",
        "        0: [\"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\", \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\"],        # Ø¨Ø­Ø§ÙˆÙ„ Ø£ØµÙ„Ø­ Ø£Ùˆ Ø£Ø­Ù„Ù‘ Ø§Ù„Ø´Ø¹ÙˆØ±\n",
        "        1: [\"Ø§Ù„Ø¢ÙƒÙ„\", \"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\"],          # Ø¨Ù„Ø§Ù‚ÙŠ Ø­Ø§Ø¬Ø© Ø¢ÙƒÙ„Ù‡Ø§ØŒ Ø£Ø´ÙˆÙÙ‡Ø§ØŒ Ø£Ùˆ Ø£Ø¹Ù…Ù„Ù‡Ø§\n",
        "        2: [\"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\", \"Ø§Ù„Ø®Ø§ÙŠÙ\"],     # Ø¨Ø­Ø³ ØµØºÙŠØ±ØŒ Ø¹ÙŠÙ„ØŒ Ø£Ùˆ Ø®Ø§ÙŠÙ\n",
        "        3: [\"Ø§Ù„Ù†Ø§Ù‚Ø¯\", \"Ø§Ù„Ø¨Ø§Ø±Ø¯\"],           # Ø£Ù‚ÙˆÙ„ Ù„Ù†ÙØ³ÙŠ \"Ù…ÙŠÙ†ÙØ¹Ø´ Ø£Ø­Ø³ ÙƒØ¯Ù‡\"\n",
        "        4: [\"Ø§Ù„Ø­ÙŠØ±Ø§Ù†\", \"Ø§Ù„Ù…Ù‡Ù…ÙÙ„\"],         # Ø£Ø­Ø³ Ø¥Ù†ÙŠ Ù…Ù†ÙØµÙ„ Ø¹Ù† Ø¬Ø³Ù…ÙŠ Ø£Ùˆ Ù…Ø´Ø§Ø¹Ø±ÙŠ\n",
        "        5: [\"Ø§Ù„Ù†Ø§Ù‚Ø¯\", \"Ø§Ù„Ø®Ø¬ÙˆÙ„\"]            # Ø¨Ø²Ø¹Ù„ Ù…Ù† Ù†ÙØ³ÙŠ\n",
        "    },\n",
        "\n",
        "    'Q4_slider': {\n",
        "        '0-20%': [],  # Ø¬Ø²Ø¡ ÙˆØ­ÙŠØ¯ Ø¨Ø³ÙŠØ·\n",
        "        '21-50%': [\"Ø§Ù„ÙˆØ­ÙŠØ¯\"],  # ÙÙŠ Ø­Ø§Ø¬Ø§Øª Ù…Ù† Ø§Ù„ÙˆØ­Ø¯Ø©\n",
        "        '51-80%': [\"Ø§Ù„ÙˆØ­ÙŠØ¯\", \"Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\", \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\"],  # Ø¬Ø²Ø¡ ÙˆØ­ÙŠØ¯ Ù†Ø´ÙŠØ·\n",
        "        '81-100%': [\"Ø§Ù„ÙˆØ­ÙŠØ¯\", \"Ø§Ù„Ù…Ù‡Ù…ÙÙ„\", \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\"]  # Ø¬Ø²Ø¡ ÙˆØ­ÙŠØ¯ Ù…Ø³ÙŠØ·Ø±\n",
        "    },\n",
        "\n",
        "    'Q5': {\n",
        "        0: [\"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\", \"Ø§Ù„Ù†Ø§Ù‚Ø¯\"],          # Ø¬Ø²Ø¡ Ø¯Ø§ÙŠÙ…Ø§Ù‹ Ø¨ÙŠØ¯ÙˆÙ‘ÙŠ\n",
        "        1: [\"Ø§Ù„ÙˆØ­ÙŠØ¯\", \"Ø§Ù„Ù…Ù‡Ù…ÙÙ„\"],          # Ø¬Ø²Ø¡ Ø¨ÙŠØ­Ø³ Ø¨ÙˆØ­Ø¯Ø© Ø´Ø¯ÙŠØ¯Ø©\n",
        "        2: [\"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\", \"Ø§Ù„Ø®Ø§ÙŠÙ\"],         # Ø¬Ø²Ø¡ Ø¨ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„ØªØ­ÙƒÙ…\n",
        "        3: [\"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\", \"Ø§Ù„Ù„Ø§Ø¹Ø¨\"],         # Ø¬Ø²Ø¡ Ø¨ÙŠØªÙ‡Ø±Ø¨ Ù„Ù„Ù†Ø´Ø§Ø·Ø§Øª\n",
        "        4: [\"Ø§Ù„Ø®Ø¬ÙˆÙ„\", \"Ø§Ù„Ù†Ø§Ù‚Ø¯\"],           # Ø¬Ø²Ø¡ Ø¨ÙŠØ­Ø³ Ø¨Ø§Ù„Ø®Ø²ÙŠ\n",
        "        5: [\"Ø§Ù„Ø­ÙŠØ±Ø§Ù†\", \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\"]     # Ø¬Ø²Ø¡ Ù…ØªØ­ÙŠØ±\n",
        "    },\n",
        "\n",
        "    'Q6': {\n",
        "        0: [\"Ø§Ù„Ù†Ø§Ù‚Ø¯\", \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\"],          # \"Ù…Ø´ Ù…Ø´ÙƒÙ„Ø© ØªØ¹Ù…Ù„ ØºÙ„Ø·Ø§Øª\"\n",
        "        1: [\"Ø§Ù„Ø®Ø§ÙŠÙ\", \"Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\"],          # \"Ù…Ø´ Ù‡Ø³ÙŠØ¨Ùƒ\"\n",
        "        2: [\"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ\", \"Ø§Ù„Ø®Ø¬ÙˆÙ„\"],          # \"Ù…Ø´ Ù…Ø­ØªØ§Ø¬ ØªØ§Ø®Ø¯ Ø­Ø¨\"\n",
        "        3: [\"Ø§Ù„Ù…Ù‡Ù…ÙÙ„\", \"Ø§Ù„Ø¨Ø§Ø±Ø¯\"],          # \"Ù…Ø´Ø§Ø¹Ø±Ùƒ Ù„ÙŠÙ‡Ø§ Ù‚ÙŠÙ…Ø©\"\n",
        "        4: [\"Ø§Ù„Ø®Ø§ÙŠÙ\", \"Ø§Ù„Ù…Ø±Ù‡ÙÙ‚\"],          # \"Ø£Ù†Øª ÙÙŠ Ø£Ù…Ø§Ù† Ø¯Ù„ÙˆÙ‚ØªÙŠ\"\n",
        "        5: [\"Ø§Ù„Ø®Ø¬ÙˆÙ„\", \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\"]      # \"Ø£Ù†Ø§ Ø¨Ù‚Ø¨Ù„ ÙƒÙ„ Ø­Ø§Ø¬Ø© ÙÙŠÙƒ\"\n",
        "    },\n",
        "\n",
        "    'Q7': {\n",
        "        0: [\"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\", \"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\"],     # Ø¨Ø´ØªØºÙ„ Ø£ÙƒØªØ± ÙˆÙ„ÙØªØ±Ø§Øª Ø£Ø·ÙˆÙ„\n",
        "        1: [\"Ø§Ù„Ø¢ÙƒÙ„\", \"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\"],          # Ø¨Ø³ØªÙ†Ù‰ Ø¹Ù„Ù‰ Ø£ÙƒÙ„ Ù…Ø±ÙŠØ­ Ø£Ùˆ Ù…Ø³Ù„Ø³Ù„Ø§Øª\n",
        "        2: [\"Ø§Ù„Ø¨Ø§Ø±Ø¯\", \"Ø§Ù„Ù…Ù‡Ù…ÙÙ„\"],          # Ø¨Ø¨Ø·Ù„ Ø£Ø­Ø³ Ø¨Ø£ÙŠ Ø­Ø§Ø¬Ø©\n",
        "        3: [\"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ\", \"Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\"],         # Ø¨Ø­Ø§ÙˆÙ„ Ø£Ø±Ø¶ÙŠ ÙƒÙ„ Ø§Ù„Ù†Ø§Ø³\n",
        "        4: [\"Ø§Ù„Ù„Ø§Ø¹Ø¨\", \"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\"],         # Ø¨Ù†Ø³Ù‰ Ù†ÙØ³ÙŠ ÙÙŠ Ø£Ù„Ø¹Ø§Ø¨ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ\n",
        "        5: [\"Ø§Ù„Ù†Ø§Ù‚Ø¯\", \"Ø§Ù„Ø®Ø¬ÙˆÙ„\"]            # Ø¨Ø§Ù†ØªÙ‚Ø¯ Ù†ÙØ³ÙŠ\n",
        "    },\n",
        "\n",
        "    'Q8_slider': {\n",
        "        '0-20%': [],  # Ù†Ø´Ø§Ø· Ø¬Ø²Ø¡ Ø§Ù„Ø¥Ø·ÙØ§Ø¦ÙŠ Ø¨Ø³ÙŠØ·\n",
        "        '21-50%': [\"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\", \"Ø§Ù„Ø¢ÙƒÙ„\"],  # ÙÙŠ Ø­Ø§Ø¬Ø§Øª Ù…Ù† Ø§Ù„Ù…Ø¤Ø¬Ù„/Ø§Ù„Ø¢ÙƒÙ„\n",
        "        '51-80%': [\"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\", \"Ø§Ù„Ø¢ÙƒÙ„\", \"Ø§Ù„Ù„Ø§Ø¹Ø¨\"],  # Ø£Ù†Ù…Ø§Ø· Ø¬Ø²Ø¡ Ø§Ù„Ø¥Ø·ÙØ§Ø¦ÙŠ Ù†Ø´ÙŠØ·Ø©\n",
        "        '81-100%': [\"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\", \"Ø§Ù„Ø¢ÙƒÙ„\", \"Ø§Ù„Ù„Ø§Ø¹Ø¨\"]  # Ø¥Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø¬Ø²Ø¡ Ø§Ù„Ø¥Ø·ÙØ§Ø¦ÙŠ Ù…Ø³ÙŠØ·Ø±Ø©\n",
        "    },\n",
        "\n",
        "    'Q9': {\n",
        "        0: [\"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ\", \"Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\"],         # Ø£Ø¶Ø¹ Ø­Ø¯ÙˆØ¯ Ù…Ù† ØºÙŠØ± Ù…Ø§ Ø£Ø­Ø³ Ø¨Ø§Ù„Ø°Ù†Ø¨\n",
        "        1: [\"Ø§Ù„Ø®Ø§ÙŠÙ\", \"Ø§Ù„ÙˆØ­ÙŠØ¯\"],           # Ø£Ø«Ù‚ Ø¥Ù† Ø§Ù„Ù†Ø§Ø³ Ù…Ù‡ØªÙ…Ø© Ø¨ÙŠØ§\n",
        "        2: [\"Ø§Ù„ØºÙŠÙˆØ±\", \"Ø§Ù„Ø®Ø¬ÙˆÙ„\"],           # Ù…Ø§ Ø£Ù‚Ø§Ø±Ù†Ø´ Ù†ÙØ³ÙŠ Ø¨Ø§Ù„ØºÙŠØ±\n",
        "        3: [\"Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\", \"Ø§Ù„Ù…Ù‡Ù…ÙÙ„\"],         # Ø£Ø·Ù„Ø¨ Ø¥Ù„Ù„ÙŠ Ø£Ù†Ø§ Ù…Ø­ØªØ§Ø¬Ù‡\n",
        "        4: [\"Ø§Ù„ÙˆØ­ÙŠØ¯\", \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\"],     # Ø£Ø­Ø³ Ø¥Ù†ÙŠ Ù…ØªØ´Ø§Ù ÙˆÙØ§Ù‡ÙÙ…\n",
        "        5: [\"Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\", \"Ø§Ù„Ø®Ø§ÙŠÙ\"]           # Ø£Ø®Ù„ÙŠÙ†ÙŠ Ø£ØªÙƒÙ„ Ø¹Ù„Ù‰ Ø­Ø¯\n",
        "    },\n",
        "\n",
        "    'Q10': {\n",
        "        0: [\"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ\", \"Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\"],         # \"Ø§ØªØ£ÙƒØ¯ Ø¥Ù†Ù‡Ù… ÙŠØ¹Ø¬Ø¨ÙˆØ§ Ø¨ÙŠÙƒ\"\n",
        "        1: [\"Ø§Ù„Ø¨Ø§Ø±Ø¯\", \"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\"],         # \"Ù…Ø§ØªÙˆØ±Ù‘ÙŠØ´ Ø£ÙŠ Ù†Ù‚Ø·Ø© Ø¶Ø¹Ù\"\n",
        "        2: [\"Ø§Ù„ØºÙŠÙˆØ±\", \"Ø§Ù„Ø®Ø¬ÙˆÙ„\"],           # \"Ù‡Ù…Ø§ Ù…ØªØ­ÙƒÙ…ÙŠÙ† ÙÙŠ Ø­ÙŠØ§ØªÙ‡Ù… Ø£ÙƒØªØ± Ù…Ù†ÙŠ\"\n",
        "        3: [\"Ø§Ù„ØºÙŠÙˆØ±\", \"Ø§Ù„Ù†Ø§Ù‚Ø¯\"],           # \"ÙŠÙØ¶Ù‘Ù„ Ø£Ø¨Ù‚Ù‰ Ø²ÙŠÙ‡Ù…\"\n",
        "        4: [\"Ø§Ù„ÙˆØ­ÙŠØ¯\", \"Ø§Ù„Ø­ÙŠØ±Ø§Ù†\"],          # \"Ø£Ù†Ø§ Ù…Ø´ Ù…Ù†ØªÙ…ÙŠ Ù„ÙŠÙ‡Ù… Ù‡Ù†Ø§\"\n",
        "        5: [\"Ø§Ù„Ø®Ø¬ÙˆÙ„\", \"Ø§Ù„Ø®Ø§ÙŠÙ\"]            # \"Ù‡Ùˆ Ù„Ùˆ Ø´Ø§ÙÙˆÙ†ÙŠ Ø¥Ù„Ù„ÙŠ Ø¬ÙˆØ§\"\n",
        "    },\n",
        "\n",
        "    'Q11': {\n",
        "        0: [\"Ø§Ù„Ù†Ø§Ù‚Ø¯\", \"Ø§Ù„Ø®Ø¬ÙˆÙ„\"],           # Ø­Ø³ Ù…Ø³ØªÙ…Ø± Ø¥Ù†Ùƒ \"Ù…Ø´ ÙƒÙØ§ÙŠØ©\"\n",
        "        1: [\"Ø§Ù„ÙˆØ­ÙŠØ¯\", \"Ø§Ù„Ù…Ù‡Ù…ÙÙ„\"],          # ÙˆØ­Ø¯Ø© Ø¹Ù…ÙŠÙ‚Ø©\n",
        "        2: [\"Ø§Ù„Ø®Ø§ÙŠÙ\", \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\"],          # Ù‚Ù„Ù‚ Ù…Ù† Ø¥Ù† ÙƒÙ„ Ø­Ø§Ø¬Ø© Ù…Ù…ÙƒÙ† ØªÙ…Ø´ÙŠ ØºÙ„Ø·\n",
        "        3: [\"Ø§Ù„Ù…Ù‡Ù…ÙÙ„\", \"Ø§Ù„Ø­ÙŠØ±Ø§Ù†\"],         # Ø®Ø¯Ø± Ù†ÙØ³ÙŠ Ø£Ùˆ ÙØ±Ø§Øº\n",
        "        4: [\"Ø§Ù„Ø®Ø¬ÙˆÙ„\", \"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\"],       # Ø°Ù†Ø¨ Ø¹Ø´Ø§Ù† Ù…Ù‚Ø¯Ø±ØªØ´ Ø£Ø¹Ù…Ù„/Ø£Ø¨Ù‚Ù‰ Ø£ÙƒØªØ±\n",
        "        5: [\"Ø§Ù„Ø­ÙŠØ±Ø§Ù†\", \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\"]     # Ø­ÙŠØ±Ø© ÙÙŠ Ø¥ÙŠÙ‡ Ø§Ù„Ù„ÙŠ Ø£Ù†Ø§ Ø¹Ø§ÙŠØ²Ù‡\n",
        "    },\n",
        "\n",
        "    'Q12': {\n",
        "        0: [\"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\", \"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\"],      # Ù‚Ø¯ Ø¥ÙŠÙ‡ Ø¨Ø¯ÙˆÙ‘ÙŠ\n",
        "        1: [\"Ø§Ù„Ù…Ù‡Ù…ÙÙ„\", \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\"],    # Ù‚Ø¯ Ø¥ÙŠÙ‡ Ø£Ù†Ø§ Ù…ØªØ£Ø°ÙŠ Ø¬ÙˆØ§\n",
        "        2: [\"Ø§Ù„Ø®Ø§ÙŠÙ\", \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\"],          # Ù‚Ø¯ Ø¥ÙŠÙ‡ Ø£Ù†Ø§ Ø®Ø§ÙŠÙ Ø£ÙØ´Ù„\n",
        "        3: [\"Ø§Ù„ÙˆØ­ÙŠØ¯\", \"Ø§Ù„Ù…Ù‡Ù…ÙÙ„\"],          # Ù‚Ø¯ Ø¥ÙŠÙ‡ Ø§Ù„ÙˆØ­Ø¯Ø© Ø¹Ù†Ø¯ÙŠ Ù‚ÙˆÙŠØ©\n",
        "        4: [\"Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\", \"Ø§Ù„Ù…Ø±Ù‡ÙÙ‚\"],         # Ù‚Ø¯ Ø¥ÙŠÙ‡ Ø£Ù†Ø§ Ù…Ø­ØªØ§Ø¬ Ù…Ø³Ø§Ø¹Ø¯Ø©\n",
        "        5: [\"Ø§Ù„Ø­ÙŠØ±Ø§Ù†\", \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\"]     # Ù‚Ø¯ Ø¥ÙŠÙ‡ Ø£Ù†Ø§ Ù…ØªØ­ÙŠÙ‘Ø±\n",
        "    },\n",
        "\n",
        "    'Q13': {\n",
        "        0: [\"Ø§Ù„Ù…Ø±Ù‡ÙÙ‚\", \"Ø§Ù„Ø®Ø§ÙŠÙ\"],          # Ø¨Ø­Ø³ Ø¥Ù†ÙŠ Ù…ØºÙ…ÙˆØ±\n",
        "        1: [\"Ø§Ù„ØºÙŠÙˆØ±\", \"Ø§Ù„Ø®Ø¬ÙˆÙ„\"],           # Ø¨Ø¨Ø¯Ø£ Ø£Ù‚Ø§Ø±Ù† Ù†ÙØ³ÙŠ Ø¨Ø§Ù„Ù†Ø§Ø³\n",
        "        2: [\"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\", \"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\"],        # Ø¨Ø¨Ù‚Ù‰ Ù…Ø±ÙƒØ² Ø¬Ø§Ù…Ø¯ Ø¹Ù„Ù‰ ÙƒÙ„ Ø­Ø§Ø¬Ø©\n",
        "        3: [\"Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\", \"Ø§Ù„Ø®Ø¬ÙˆÙ„\"],          # Ø¨Ø­Ø³ Ø¥Ù†ÙŠ Ø­Ù…Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø§Ø³\n",
        "        4: [\"Ø§Ù„Ù„Ø§Ø¹Ø¨\", \"Ø§Ù„Ø­ÙŠØ±Ø§Ù†\"],          # Ø¨ØªÙ‡Ø±Ø¨ Ù„Ø¹Ø§Ù„Ù… Ø®ÙŠØ§Ù„ÙŠ\n",
        "        5: [\"Ø§Ù„Ù†Ø§Ù‚Ø¯\", \"Ø§Ù„Ø¨Ø§Ø±Ø¯\"],           # Ø¨Ø²Ø¹Ù„ Ù…Ù† Ù†ÙØ³ÙŠ\n",
        "        6: [\"Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\", \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\"],    # Ù†ÙØ³ÙŠ ÙÙŠ Ø­Ø¯ ÙŠØ§Ø®Ø¯ Ø¹Ù†ÙŠ ÙƒÙ„ Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ÙŠØ©\n",
        "        7: [\"Ø§Ù„Ù…Ù‡Ù…ÙÙ„\", \"Ø§Ù„Ø¨Ø§Ø±Ø¯\"]           # Ø¨Ø­Ø³ Ø¨Ø®ÙØ¯ÙØ±\n",
        "    }\n",
        "}\n",
        "\n",
        "# Strong signature patterns for each character (ARABIC)\n",
        "CHARACTER_SIGNATURES_AR = {\n",
        "    \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\": [\"Q2:high\", \"Q1:0\", \"Q3:0\", \"Q5:0\"],\n",
        "    \"Ø§Ù„Ù†Ø§Ù‚Ø¯\": [\"Q2:high\", \"Q3:3\", \"Q5:0\", \"Q11:0\"],\n",
        "    \"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ\": [\"Q10:0\", \"Q7:3\", \"Q9:0\", \"Q1:2\"],\n",
        "    \"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\": [\"Q1:0\", \"Q3:0\", \"Q10:1\", \"Q7:0\"],\n",
        "    \"Ø§Ù„Ø¨Ø§Ø±Ø¯\": [\"Q1:4\", \"Q3:3\", \"Q7:2\", \"Q13:7\"],\n",
        "    \"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\": [\"Q1:1\", \"Q7:0\", \"Q12:0\", \"Q2:high\"],\n",
        "    \"Ø§Ù„Ø­ÙŠØ±Ø§Ù†\": [\"Q1:5\", \"Q3:4\", \"Q5:5\", \"Q11:5\"],\n",
        "    \"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\": [\"Q8:high\", \"Q1:3\", \"Q3:1\", \"Q7:4\"],\n",
        "    \"Ø§Ù„Ø¢ÙƒÙ„\": [\"Q8:high\", \"Q3:1\", \"Q7:1\", \"Q13:1\"],\n",
        "    \"Ø§Ù„Ù„Ø§Ø¹Ø¨\": [\"Q8:high\", \"Q1:3\", \"Q7:4\", \"Q13:4\"],\n",
        "    \"Ø§Ù„ÙˆØ­ÙŠØ¯\": [\"Q4:high\", \"Q1:5\", \"Q11:1\", \"Q12:3\"],\n",
        "    \"Ø§Ù„Ø®Ø§ÙŠÙ\": [\"Q11:2\", \"Q6:1\", \"Q9:1\", \"Q13:0\"],\n",
        "    \"Ø§Ù„Ù…Ù‡Ù…ÙÙ„\": [\"Q4:high\", \"Q3:4\", \"Q6:3\", \"Q11:3\"],\n",
        "    \"Ø§Ù„Ø®Ø¬ÙˆÙ„\": [\"Q11:0\", \"Q6:5\", \"Q9:2\", \"Q10:5\"],\n",
        "    \"Ø§Ù„Ù…Ø±Ù‡ÙÙ‚\": [\"Q13:0\", \"Q12:4\", \"Q6:4\", \"Q11:2\"],\n",
        "    \"Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\": [\"Q9:5\", \"Q12:4\", \"Q10:0\", \"Q6:1\"],\n",
        "    \"Ø§Ù„ØºÙŠÙˆØ±\": [\"Q10:2\", \"Q9:2\", \"Q13:1\", \"Q10:3\"],\n",
        "    \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\": [\"Q6:5\", \"Q12:1\", \"Q3:2\", \"Q9:4\"]\n",
        "}\n",
        "\n",
        "def generate_probabilistic_dataset_arabic(num_samples=75000):\n",
        "    \"\"\"Generate Arabic dataset with probabilistic character presence\"\"\"\n",
        "\n",
        "    print(f\"ØªÙˆÙ„ÙŠØ¯ {num_samples:,} Ø¹ÙŠÙ†Ø© Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©...\")\n",
        "\n",
        "    dataset = []\n",
        "    slider_values = ['0-20%', '21-50%', '51-80%', '81-100%']\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        if i % 10000 == 0 and i > 0:\n",
        "            print(f\"  ØªÙ… ØªÙˆÙ„ÙŠØ¯ {i:,} Ø¹ÙŠÙ†Ø©...\")\n",
        "\n",
        "        responses = {}\n",
        "\n",
        "        # ===== 1. GENERATE PRIMARY CHARACTER (CORE) =====\n",
        "        primary_char = random.choice(CHARACTERS_AR)\n",
        "        primary_archetype = None\n",
        "        if primary_char in MANAGERS_AR:\n",
        "            primary_archetype = \"Ù…Ø¯ÙŠØ±\"\n",
        "        elif primary_char in FIREFIGHTERS_AR:\n",
        "            primary_archetype = \"Ø±Ø¬Ù„ Ø¥Ø·ÙØ§Ø¡\"\n",
        "        else:\n",
        "            primary_archetype = \"Ù…Ù†ÙÙŠ\"\n",
        "\n",
        "        # ===== 2. GENERATE SECONDARY CHARACTERS =====\n",
        "        secondary_chars = []\n",
        "        tertiary_chars = []\n",
        "\n",
        "        # Based on primary, likely secondary characters\n",
        "        if primary_archetype == \"Ù…Ø¯ÙŠØ±\":\n",
        "            candidate_secondaries = [c for c in CHARACTERS_AR if c != primary_char]\n",
        "            # Prefer same archetype or related exiles\n",
        "            secondary = random.choice([c for c in candidate_secondaries if\n",
        "                                      (c in MANAGERS_AR) or\n",
        "                                      (primary_char in [\"Ø§Ù„Ù†Ø§Ù‚Ø¯\", \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\"] and c in [\"Ø§Ù„Ø®Ø¬ÙˆÙ„\", \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\"])])\n",
        "            secondary_chars.append(secondary)\n",
        "\n",
        "            # Tertiary: often an exile\n",
        "            remaining = [c for c in EXILES_AR if c not in secondary_chars]\n",
        "            if remaining:\n",
        "                tertiary_chars.append(random.choice(remaining))\n",
        "\n",
        "        elif primary_archetype == \"Ø±Ø¬Ù„ Ø¥Ø·ÙØ§Ø¡\":\n",
        "            candidate_secondaries = [c for c in FIREFIGHTERS_AR if c != primary_char]\n",
        "            if candidate_secondaries:\n",
        "                secondary_chars.append(random.choice(candidate_secondaries))\n",
        "\n",
        "            # Often have a manager that's too strict\n",
        "            tertiary_chars.append(random.choice([\"Ø§Ù„Ù†Ø§Ù‚Ø¯\", \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\", \"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\"]))\n",
        "\n",
        "        else:  # Ù…Ù†ÙÙŠ\n",
        "            if primary_char in [\"Ø§Ù„ÙˆØ­ÙŠØ¯\", \"Ø§Ù„Ù…Ù‡Ù…ÙÙ„\"]:\n",
        "                secondary_chars.append(random.choice([\"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ\", \"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\"]))\n",
        "                tertiary_chars.append(random.choice([\"Ø§Ù„Ø¨Ø§Ø±Ø¯\", \"Ø§Ù„Ø­ÙŠØ±Ø§Ù†\"]))\n",
        "            elif primary_char in [\"Ø§Ù„Ø®Ø¬ÙˆÙ„\", \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\"]:\n",
        "                secondary_chars.append(random.choice([\"Ø§Ù„Ù†Ø§Ù‚Ø¯\", \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\"]))\n",
        "                tertiary_chars.append(\"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\")\n",
        "            else:\n",
        "                secondary_chars.append(random.choice(MANAGERS_AR + FIREFIGHTERS_AR))\n",
        "\n",
        "        # Ensure we have 2-4 total characters\n",
        "        all_chars = list(set([primary_char] + secondary_chars + tertiary_chars))\n",
        "        if len(all_chars) > 4:\n",
        "            all_chars = random.sample(all_chars, 4)\n",
        "        elif len(all_chars) < 2:\n",
        "            all_chars.append(random.choice([c for c in CHARACTERS_AR if c not in all_chars]))\n",
        "\n",
        "        # ===== 3. GENERATE RESPONSES BASED ON CHARACTERS =====\n",
        "        # Generate answers that strongly reflect primary character\n",
        "        for q in ['Q1', 'Q3', 'Q5', 'Q6', 'Q7', 'Q9', 'Q10', 'Q11', 'Q12', 'Q13']:\n",
        "            # Start with primary character's preferred answers\n",
        "            primary_answers = []\n",
        "            for ans, chars in QUESTION_MAPPINGS_AR.get(q, {}).items():\n",
        "                if primary_char in chars:\n",
        "                    primary_answers.append(ans)\n",
        "\n",
        "            # Add secondary characters' answers\n",
        "            secondary_answers = []\n",
        "            for char in secondary_chars:\n",
        "                for ans, chars in QUESTION_MAPPINGS_AR.get(q, {}).items():\n",
        "                    if char in chars:\n",
        "                        secondary_answers.append(ans)\n",
        "\n",
        "            # Combine with weights: primary 60%, secondary 40%\n",
        "            all_answers = []\n",
        "            if primary_answers:\n",
        "                n_primary = random.randint(1, min(2, len(primary_answers)))\n",
        "                all_answers.extend(random.sample(primary_answers, n_primary))\n",
        "\n",
        "            if secondary_answers:\n",
        "                n_secondary = random.randint(0, min(1, len(secondary_answers)))\n",
        "                if n_secondary > 0:\n",
        "                    all_answers.extend(random.sample(secondary_answers, n_secondary))\n",
        "\n",
        "            # Ensure 1-3 answers total\n",
        "            if not all_answers:\n",
        "                max_opt = 6 if q != 'Q13' else 8\n",
        "                all_answers = random.sample(range(max_opt), random.randint(1, 2))\n",
        "\n",
        "            if len(all_answers) > 3:\n",
        "                all_answers = random.sample(all_answers, 3)\n",
        "\n",
        "            responses[q] = ','.join(map(str, sorted(set(all_answers))))\n",
        "\n",
        "        # Generate slider responses\n",
        "        def get_slider_for_char(char, q_prefix):\n",
        "            \"\"\"Get slider value based on character\"\"\"\n",
        "            if char == \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\" and q_prefix == \"Q2\":\n",
        "                return random.choices(['51-80%', '81-100%'], weights=[40, 60])[0]\n",
        "            elif char == \"Ø§Ù„ÙˆØ­ÙŠØ¯\" and q_prefix == \"Q4\":\n",
        "                return random.choices(['51-80%', '81-100%'], weights=[40, 60])[0]\n",
        "            elif char in FIREFIGHTERS_AR and q_prefix == \"Q8\":\n",
        "                return random.choices(['51-80%', '81-100%'], weights=[40, 60])[0]\n",
        "            else:\n",
        "                return random.choice(slider_values)\n",
        "\n",
        "        # Sliders reflect strongest character for that trait\n",
        "        responses['Q2'] = get_slider_for_char(primary_char, \"Q2\")\n",
        "        responses['Q4'] = get_slider_for_char(primary_char, \"Q4\")\n",
        "        responses['Q8'] = get_slider_for_char(primary_char, \"Q8\")\n",
        "\n",
        "        # ===== 4. CREATE PROBABILITY TARGETS =====\n",
        "        for char in CHARACTERS_AR:\n",
        "            col_name = f'prob_{char}'\n",
        "\n",
        "            if char == primary_char:\n",
        "                # Primary: high probability\n",
        "                responses[col_name] = round(random.uniform(0.8, 0.98), 2)\n",
        "            elif char in secondary_chars:\n",
        "                # Secondary: medium-high probability\n",
        "                responses[col_name] = round(random.uniform(0.4, 0.7), 2)\n",
        "            elif char in tertiary_chars:\n",
        "                # Tertiary: medium-low probability\n",
        "                responses[col_name] = round(random.uniform(0.2, 0.5), 2)\n",
        "            else:\n",
        "                # Others: low probability, but not zero\n",
        "                responses[col_name] = round(random.uniform(0.0, 0.2), 2)\n",
        "\n",
        "        # ===== 5. ADD DERIVED FEATURES =====\n",
        "        # Feature 1: Number of answers per question\n",
        "        for q in ['Q1', 'Q3', 'Q5', 'Q6', 'Q7', 'Q9', 'Q10', 'Q11', 'Q12', 'Q13']:\n",
        "            responses[f'{q}_count'] = len(responses[q].split(','))\n",
        "\n",
        "        # Feature 2: Convert sliders to numeric\n",
        "        slider_map = {'0-20%': 0.1, '21-50%': 0.35, '51-80%': 0.65, '81-100%': 0.9}\n",
        "        responses['Q2_num'] = slider_map[responses['Q2']]\n",
        "        responses['Q4_num'] = slider_map[responses['Q4']]\n",
        "        responses['Q8_num'] = slider_map[responses['Q8']]\n",
        "\n",
        "        # Feature 3: Archetype indicators\n",
        "        responses['has_manager_indicators'] = int(\n",
        "            responses['Q2_num'] > 0.6 or  # ÙƒÙ…Ø§Ù„ÙŠØ©\n",
        "            responses.get('Q1', '').split(',').count('0') > 0 or  # ØªØ­ÙƒÙ…\n",
        "            responses.get('Q10', '').split(',').count('0') > 0   # Ø¥Ø±Ø¶Ø§Ø¡ Ø§Ù„Ù†Ø§Ø³\n",
        "        )\n",
        "\n",
        "        responses['has_exile_indicators'] = int(\n",
        "            responses['Q4_num'] > 0.6 or  # ÙˆØ­Ø¯Ø©\n",
        "            responses.get('Q11', '').split(',').count('1') > 0 or  # Ø´Ø¹ÙˆØ± Ø¨Ø§Ù„ÙˆØ­Ø¯Ø©\n",
        "            responses.get('Q13', '').split(',').count('0') > 0    # Ø¥Ø±Ù‡Ø§Ù‚\n",
        "        )\n",
        "\n",
        "        responses['has_firefighter_indicators'] = int(\n",
        "            responses['Q8_num'] > 0.6 or  # Ù‡Ø±ÙˆØ¨\n",
        "            responses.get('Q7', '').split(',').count('1') > 0 or  # Ø£ÙƒÙ„ Ù…Ø±ÙŠØ­\n",
        "            responses.get('Q7', '').split(',').count('4') > 0    # Ø£Ù„Ø¹Ø§Ø¨\n",
        "        )\n",
        "\n",
        "        dataset.append(responses)\n",
        "\n",
        "    # Create DataFrame\n",
        "    question_cols = [f'Q{i}' for i in range(1, 14)]\n",
        "    derived_cols = [col for col in dataset[0].keys() if col.startswith('Q') and '_num' in col or '_count' in col or 'has_' in col]\n",
        "    prob_cols = [f'prob_{char}' for char in CHARACTERS_AR]\n",
        "\n",
        "    all_cols = question_cols + derived_cols + prob_cols\n",
        "    df = pd.DataFrame(dataset)[all_cols]\n",
        "\n",
        "    return df, CHARACTERS_AR\n",
        "\n",
        "def create_top3_dataset_arabic(df, characters_ar):\n",
        "    \"\"\"Convert probabilistic dataset to top-3 format for training (Arabic)\"\"\"\n",
        "\n",
        "    prob_cols = [f'prob_{char}' for char in characters_ar]\n",
        "    prob_matrix = df[prob_cols].values\n",
        "\n",
        "    # Get top 3 indices for each row\n",
        "    top3_indices = np.argsort(prob_matrix, axis=1)[:, -3:][:, ::-1]\n",
        "\n",
        "    # Create new dataset with top 3 characters as strings\n",
        "    top3_data = []\n",
        "    for idx, row in df.iterrows():\n",
        "        row_data = {col: row[col] for col in df.columns if not col.startswith('prob_')}\n",
        "\n",
        "        # Get top 3 characters and their probabilities\n",
        "        top3_chars = []\n",
        "        top3_probs = []\n",
        "\n",
        "        for rank in range(3):\n",
        "            char_idx = top3_indices[idx, rank]\n",
        "            char_name = characters_ar[char_idx]\n",
        "            prob = prob_matrix[idx, char_idx]\n",
        "\n",
        "            row_data[f'top{rank+1}_char'] = char_name\n",
        "            row_data[f'top{rank+1}_prob'] = round(prob, 2)\n",
        "\n",
        "            top3_chars.append(char_name)\n",
        "            top3_probs.append(prob)\n",
        "\n",
        "        # Add confidence metrics\n",
        "        row_data['confidence_score'] = round(np.mean(top3_probs), 2)\n",
        "        row_data['margin_1_2'] = round(top3_probs[0] - top3_probs[1], 2)\n",
        "        row_data['top3_list'] = ','.join(top3_chars)\n",
        "\n",
        "        # Add English equivalents for reference\n",
        "        row_data['top1_char_en'] = AR_TO_EN.get(top3_chars[0], top3_chars[0])\n",
        "        row_data['top2_char_en'] = AR_TO_EN.get(top3_chars[1], top3_chars[1])\n",
        "        row_data['top3_char_en'] = AR_TO_EN.get(top3_chars[2], top3_chars[2])\n",
        "\n",
        "        top3_data.append(row_data)\n",
        "\n",
        "    top3_df = pd.DataFrame(top3_data)\n",
        "    return top3_df\n",
        "\n",
        "def analyze_top3_dataset_arabic(df, characters_ar):\n",
        "    \"\"\"Analyze the Arabic top-3 dataset\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ØªØ­Ù„ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© (Ø£ÙØ¶Ù„ 3 Ø´Ø®ØµÙŠØ§Øª)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Character appearance in top 3\n",
        "    char_counts = {char: 0 for char in characters_ar}\n",
        "\n",
        "    for i in range(1, 4):\n",
        "        col = f'top{i}_char'\n",
        "        counts = df[col].value_counts()\n",
        "        for char, count in counts.items():\n",
        "            char_counts[char] += count\n",
        "\n",
        "    print(\"Ø¸Ù‡ÙˆØ± Ø§Ù„Ø´Ø®ØµÙŠØ§Øª ÙÙŠ Ø§Ù„Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    sorted_chars = sorted(char_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "    for char, count in sorted_chars:\n",
        "        percentage = (count / (len(df) * 3)) * 100\n",
        "        print(f\"{char:20} {count:6,} ({percentage:5.1f}% Ù…Ù† Ø§Ù„Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰)\")\n",
        "\n",
        "    # Confidence analysis\n",
        "    print(f\"\\nØ¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø«Ù‚Ø©:\")\n",
        "    print(f\"Ù…ØªÙˆØ³Ø· Ø§Ù„Ø«Ù‚Ø©: {df['confidence_score'].mean():.2f}\")\n",
        "    print(f\"Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© (>0.7): {(df['confidence_score'] > 0.7).sum():,} Ø¹ÙŠÙ†Ø©\")\n",
        "    print(f\"Ø«Ù‚Ø© Ù…ØªÙˆØ³Ø·Ø© (0.4-0.7): {((df['confidence_score'] >= 0.4) & (df['confidence_score'] <= 0.7)).sum():,} Ø¹ÙŠÙ†Ø©\")\n",
        "    print(f\"Ø«Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø© (<0.4): {(df['confidence_score'] < 0.4).sum():,} Ø¹ÙŠÙ†Ø©\")\n",
        "\n",
        "    # Margin analysis\n",
        "    print(f\"\\nØ§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ø£ÙˆÙ„ ÙˆØ§Ù„Ø«Ø§Ù†ÙŠ:\")\n",
        "    print(f\"Ù…ØªÙˆØ³Ø· Ø§Ù„ÙØ±Ù‚: {df['margin_1_2'].mean():.2f}\")\n",
        "    print(f\"ÙØ§Ø¦Ø² ÙˆØ§Ø¶Ø­ (ÙØ±Ù‚ > 0.2): {(df['margin_1_2'] > 0.2).sum():,} Ø¹ÙŠÙ†Ø©\")\n",
        "    print(f\"ØªÙ†Ø§ÙØ³ Ø´Ø¯ÙŠØ¯ (ÙØ±Ù‚ < 0.1): {(df['margin_1_2'] < 0.1).sum():,} Ø¹ÙŠÙ†Ø©\")\n",
        "\n",
        "    # Archetype distribution in top 1\n",
        "    top1_archetypes = []\n",
        "    for char in df['top1_char']:\n",
        "        if char in MANAGERS_AR:\n",
        "            top1_archetypes.append(\"Ù…Ø¯ÙŠØ±\")\n",
        "        elif char in FIREFIGHTERS_AR:\n",
        "            top1_archetypes.append(\"Ø±Ø¬Ù„ Ø¥Ø·ÙØ§Ø¡\")\n",
        "        else:\n",
        "            top1_archetypes.append(\"Ù…Ù†ÙÙŠ\")\n",
        "\n",
        "    archetype_counts = pd.Series(top1_archetypes).value_counts()\n",
        "    print(f\"\\nÙ†ÙˆØ¹ÙŠØ© Ø§Ù„Ø´Ø®ØµÙŠØ© ÙÙŠ Ø§Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ø£ÙˆÙ„:\")\n",
        "    for archetype, count in archetype_counts.items():\n",
        "        print(f\"  {archetype}: {count:,} Ø¹ÙŠÙ†Ø© ({count/len(df):.1%})\")\n",
        "\n",
        "def prepare_model_training_arabic(df):\n",
        "    \"\"\"Prepare features and targets for model training (Arabic)\"\"\"\n",
        "\n",
        "    # Feature columns\n",
        "    question_cols = [f'Q{i}' for i in range(1, 14)]\n",
        "    derived_cols = [col for col in df.columns if col.endswith('_num') or col.endswith('_count') or ('has_' in col and 'indicator' in col)]\n",
        "    feature_cols = question_cols + derived_cols\n",
        "\n",
        "    # Target columns (top 3 characters)\n",
        "    target_cols = ['top1_char', 'top2_char', 'top3_char']\n",
        "\n",
        "    print(f\"\\nØ¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨:\")\n",
        "    print(f\"Ø§Ù„Ù…ÙŠØ²Ø§Øª: {len(feature_cols)} Ø¹Ù…ÙˆØ¯\")\n",
        "    print(f\"Ø§Ù„Ø¹ÙŠÙ†Ø§Øª: {len(df):,}\")\n",
        "    print(f\"Ø§Ù„Ø£Ù‡Ø¯Ø§Ù: {len(target_cols)} Ø£Ø¹Ù…Ø¯Ø© (Ø§Ù„Ù…Ø±Ø§ÙƒØ² Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰)\")\n",
        "\n",
        "    return df[feature_cols], df[target_cols], feature_cols\n",
        "\n",
        "# ==================== GENERATE ARABIC DATASET ====================\n",
        "print(\"=\"*60)\n",
        "print(\"ØªÙˆÙ„ÙŠØ¯ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ANA Ø§Ù„Ù…Ø­Ø³Ù†Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø£ÙØ¶Ù„ 3 Ø´Ø®ØµÙŠØ§Øª\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "DATASET_SIZE = 100000  # Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø£ÙƒØ¨Ø± Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ¹Ù„Ù…\n",
        "prob_df_ar, characters_ar = generate_probabilistic_dataset_arabic(DATASET_SIZE)\n",
        "\n",
        "# Convert to top-3 format\n",
        "top3_df_ar = create_top3_dataset_arabic(prob_df_ar, characters_ar)\n",
        "\n",
        "# Save datasets\n",
        "prob_df_ar.to_csv('ana_dataset_probabilistic_arabic.csv', index=False, encoding='utf-8-sig')\n",
        "top3_df_ar.to_csv('ana_dataset_top3_arabic.csv', index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(f\"\\nâœ… ØªÙ… Ø­ÙØ¸ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©: 'ana_dataset_probabilistic_arabic.csv'\")\n",
        "print(f\"âœ… ØªÙ… Ø­ÙØ¸ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø£ÙØ¶Ù„ 3 Ø´Ø®ØµÙŠØ§Øª: 'ana_dataset_top3_arabic.csv'\")\n",
        "\n",
        "# Analyze\n",
        "analyze_top3_dataset_arabic(top3_df_ar, characters_ar)\n",
        "\n",
        "# Prepare for model training\n",
        "X_ar, y_ar, feature_cols_ar = prepare_model_training_arabic(top3_df_ar)\n",
        "\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(\"Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø§Ù‡Ø²Ø© Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Display sample data\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nØ§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ (5 Ø£Ø³Ø·Ø±):\")\n",
        "print(top3_df_ar[['Q1', 'Q2', 'Q3', 'Q4', 'Q5']].head())\n",
        "\n",
        "print(\"\\nØ§Ù„Ø´Ø®ØµÙŠØ§Øª Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ (5 Ø£Ø³Ø·Ø±):\")\n",
        "print(top3_df_ar[['top1_char', 'top1_prob', 'top2_char', 'top2_prob', 'top3_char', 'top3_prob']].head())\n",
        "\n",
        "print(\"\\nØ§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£ÙˆÙ„ÙŠ Ù„Ù„Ø´Ø®ØµÙŠØ§Øª:\")\n",
        "print(\"-\" * 40)\n",
        "top1_counts = top3_df_ar['top1_char'].value_counts().head(10)\n",
        "for char, count in top1_counts.items():\n",
        "    print(f\"{char:20}: {count:6,} ({count/len(top3_df_ar):.1%})\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "boC9GhZqzd_Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}