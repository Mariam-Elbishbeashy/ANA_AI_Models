{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mariam-Elbishbeashy/ANA_AI_Models/blob/main/Questionnaire-models%20%26%20datasets/Questionnaire_arabic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XP1nCoB-v9w3",
        "outputId": "7bb5665c-7f5d-4125-8e90-f6cc757520a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ANA Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù†Ø© Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø£ÙØ¶Ù„ 3 Ø´Ø®ØµÙŠØ§Øª\n",
            "============================================================\n",
            "Ø¬Ø§Ø±Ù Ø¥Ù†Ø´Ø§Ø¡ 100,000 Ø¹ÙŠÙ†Ø© Ø¹Ø±Ø¨ÙŠØ© Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©...\n",
            "  ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ 10,000 Ø¹ÙŠÙ†Ø©...\n",
            "  ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ 20,000 Ø¹ÙŠÙ†Ø©...\n",
            "  ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ 30,000 Ø¹ÙŠÙ†Ø©...\n",
            "  ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ 40,000 Ø¹ÙŠÙ†Ø©...\n",
            "  ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ 50,000 Ø¹ÙŠÙ†Ø©...\n",
            "  ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ 60,000 Ø¹ÙŠÙ†Ø©...\n",
            "  ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ 70,000 Ø¹ÙŠÙ†Ø©...\n",
            "  ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ 80,000 Ø¹ÙŠÙ†Ø©...\n",
            "  ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ 90,000 Ø¹ÙŠÙ†Ø©...\n",
            "\n",
            "âœ… ØªÙ… Ø­ÙØ¸ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©: 'ana_dataset_probabilistic_ar.csv'\n",
            "âœ… ØªÙ… Ø­ÙØ¸ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ø£ÙØ¶Ù„ 3: 'ana_dataset_top3_ar.csv'\n",
            "\n",
            "============================================================\n",
            "ØªØ­Ù„ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ø£ÙØ¶Ù„ 3 Ø´Ø®ØµÙŠØ§Øª\n",
            "============================================================\n",
            "Ø¸Ù‡ÙˆØ± Ø§Ù„Ø´Ø®ØµÙŠØ§Øª ÙÙŠ Ù…Ø±Ø§ÙƒØ² Ø£ÙØ¶Ù„ 3:\n",
            "--------------------------------------------------\n",
            "Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„                  25,923 (  8.6% Ù…Ù† Ù…Ø±Ø§ÙƒØ² Ø£ÙØ¶Ù„ 3)\n",
            "Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ                   25,796 (  8.6% Ù…Ù† Ù…Ø±Ø§ÙƒØ² Ø£ÙØ¶Ù„ 3)\n",
            "Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ            25,401 (  8.5% Ù…Ù† Ù…Ø±Ø§ÙƒØ² Ø£ÙØ¶Ù„ 3)\n",
            "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø­ÙŠØ±Ø§Ù†             20,055 (  6.7% Ù…Ù† Ù…Ø±Ø§ÙƒØ² Ø£ÙØ¶Ù„ 3)\n",
            "Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…                  19,990 (  6.7% Ù…Ù† Ù…Ø±Ø§ÙƒØ² Ø£ÙØ¶Ù„ 3)\n",
            "Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„                19,950 (  6.7% Ù…Ù† Ù…Ø±Ø§ÙƒØ² Ø£ÙØ¶Ù„ 3)\n",
            "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø¯              19,750 (  6.6% Ù…Ù† Ù…Ø±Ø§ÙƒØ² Ø£ÙØ¶Ù„ 3)\n",
            "Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³             19,747 (  6.6% Ù…Ù† Ù…Ø±Ø§ÙƒØ² Ø£ÙØ¶Ù„ 3)\n",
            "Ø§Ù„Ù„Ø§Ø¹Ø¨ Ø§Ù„Ù…ÙØ±Ø·             14,546 (  4.8% Ù…Ù† Ù…Ø±Ø§ÙƒØ² Ø£ÙØ¶Ù„ 3)\n",
            "Ø§Ù„Ø¢ÙƒÙ„/Ø§Ù„Ù†Ù‡Ù…               14,498 (  4.8% Ù…Ù† Ù…Ø±Ø§ÙƒØ² Ø£ÙØ¶Ù„ 3)\n",
            "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø¬ÙˆÙ„              12,836 (  4.3% Ù…Ù† Ù…Ø±Ø§ÙƒØ² Ø£ÙØ¶Ù„ 3)\n",
            "Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­              12,827 (  4.3% Ù…Ù† Ù…Ø±Ø§ÙƒØ² Ø£ÙØ¶Ù„ 3)\n",
            "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯              11,841 (  3.9% Ù…Ù† Ù…Ø±Ø§ÙƒØ² Ø£ÙØ¶Ù„ 3)\n",
            "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡Ù…ÙÙ„             11,791 (  3.9% Ù…Ù† Ù…Ø±Ø§ÙƒØ² Ø£ÙØ¶Ù„ 3)\n",
            "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§ÙŠÙ              11,392 (  3.8% Ù…Ù† Ù…Ø±Ø§ÙƒØ² Ø£ÙØ¶Ù„ 3)\n",
            "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯             11,312 (  3.8% Ù…Ù† Ù…Ø±Ø§ÙƒØ² Ø£ÙØ¶Ù„ 3)\n",
            "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ØºÙŠÙˆØ±              11,228 (  3.7% Ù…Ù† Ù…Ø±Ø§ÙƒØ² Ø£ÙØ¶Ù„ 3)\n",
            "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚             11,117 (  3.7% Ù…Ù† Ù…Ø±Ø§ÙƒØ² Ø£ÙØ¶Ù„ 3)\n",
            "\n",
            "Ø¥Ø­ØµØ§Ø¡Ø§Øª Ø§Ù„Ø«Ù‚Ø©:\n",
            "Ù…ØªÙˆØ³Ø· Ø§Ù„Ø«Ù‚Ø©: 0.58\n",
            "Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© (>0.7): 225 Ø¹ÙŠÙ†Ø©\n",
            "Ø«Ù‚Ø© Ù…ØªÙˆØ³Ø·Ø© (0.4-0.7): 99,775 Ø¹ÙŠÙ†Ø©\n",
            "Ø«Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø© (<0.4): 0 Ø¹ÙŠÙ†Ø©\n",
            "\n",
            "Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ø£ÙˆÙ„ ÙˆØ§Ù„Ø«Ø§Ù†ÙŠ:\n",
            "Ù…ØªÙˆØ³Ø· Ø§Ù„ÙØ±Ù‚: 0.34\n",
            "ÙØ§Ø¦Ø² ÙˆØ§Ø¶Ø­ (ÙØ±Ù‚ > 0.2): 89,833 Ø¹ÙŠÙ†Ø©\n",
            "Ù…Ù†Ø§ÙØ³Ø© Ù‚Ø±ÙŠØ¨Ø© (ÙØ±Ù‚ < 0.1): 0 Ø¹ÙŠÙ†Ø©\n",
            "\n",
            "Ù†Ù…Ø· Ø§Ù„Ø´Ø®ØµÙŠØ© ÙÙŠ Ø§Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ø£ÙˆÙ„:\n",
            "  Ù…Ù†ÙÙŠ: 44,508 Ø¹ÙŠÙ†Ø© (44.5%)\n",
            "  Ù…Ø¯ÙŠØ±: 38,778 Ø¹ÙŠÙ†Ø© (38.8%)\n",
            "  Ø¥Ø·ÙØ§Ø¦ÙŠ: 16,714 Ø¹ÙŠÙ†Ø© (16.7%)\n",
            "\n",
            "Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨:\n",
            "Ø§Ù„Ù…ÙŠØ²Ø§Øª: 29 Ø¹Ù…ÙˆØ¯\n",
            "Ø§Ù„Ø¹ÙŠÙ†Ø§Øª: 100,000\n",
            "\n",
            "============================================================\n",
            "Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¬Ø§Ù‡Ø²Ø© Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬!\n",
            "============================================================\n",
            "\n",
            "Ø¹ÙŠÙ†Ø© Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:\n",
            "--------------------------------------------------\n",
            "\n",
            "Ø¹ÙŠÙ†Ø© 1:\n",
            "  Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø£ÙˆÙ„Ù‰: Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ (Ø§Ø­ØªÙ…Ø§Ù„: 0.96)\n",
            "  Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù… (Ø§Ø­ØªÙ…Ø§Ù„: 0.68)\n",
            "  Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø«Ø§Ù„Ø«Ø©: Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡Ù…ÙÙ„ (Ø§Ø­ØªÙ…Ø§Ù„: 0.34)\n",
            "  Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø©: 0.66\n",
            "  Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ø£ÙˆÙ„ ÙˆØ§Ù„Ø«Ø§Ù†ÙŠ: 0.28\n",
            "\n",
            "Ø¹ÙŠÙ†Ø© 2:\n",
            "  Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø£ÙˆÙ„Ù‰: Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ (Ø§Ø­ØªÙ…Ø§Ù„: 0.91)\n",
            "  Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø¯ (Ø§Ø­ØªÙ…Ø§Ù„: 0.69)\n",
            "  Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø«Ø§Ù„Ø«Ø©: Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯ (Ø§Ø­ØªÙ…Ø§Ù„: 0.5)\n",
            "  Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø©: 0.7\n",
            "  Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ø£ÙˆÙ„ ÙˆØ§Ù„Ø«Ø§Ù†ÙŠ: 0.22\n",
            "\n",
            "Ø¹ÙŠÙ†Ø© 3:\n",
            "  Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø£ÙˆÙ„Ù‰: Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ØºÙŠÙˆØ± (Ø§Ø­ØªÙ…Ø§Ù„: 0.83)\n",
            "  Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: Ø§Ù„Ø¢ÙƒÙ„/Ø§Ù„Ù†Ù‡Ù… (Ø§Ø­ØªÙ…Ø§Ù„: 0.64)\n",
            "  Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø«Ø§Ù„Ø«Ø©: Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡Ù…ÙÙ„ (Ø§Ø­ØªÙ…Ø§Ù„: 0.17)\n",
            "  Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø©: 0.55\n",
            "  Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ø£ÙˆÙ„ ÙˆØ§Ù„Ø«Ø§Ù†ÙŠ: 0.19\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "# ==================== ØªØ¹Ø±ÙŠÙ Ø§Ù„Ø´Ø®ØµÙŠØ§Øª Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ù…ØµØ±ÙŠ ====================\n",
        "CHARACTERS_AR = [\n",
        "    # ğŸ›¡ï¸ Ø§Ù„Ù…Ø¯ÙŠØ±ÙŠÙ† (Ø§Ù„Ø­Ù…Ø§Ø© Ø§Ù„Ø§Ø³ØªØ¨Ø§Ù‚ÙŠÙŠÙ†)\n",
        "    \"Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ\", \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\", \"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³\", \"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\",\n",
        "    \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø¯\", \"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø­ÙŠØ±Ø§Ù†\",\n",
        "\n",
        "    # ğŸ”¥ Ø±Ø¬Ø§Ù„ Ø§Ù„Ø¥Ø·ÙØ§Ø¡ (Ø§Ù„Ø­Ù…Ø§Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠÙŠÙ†)\n",
        "    \"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\", \"Ø§Ù„Ø¢ÙƒÙ„/Ø§Ù„Ù†Ù‡Ù…\", \"Ø§Ù„Ù„Ø§Ø¹Ø¨ Ø§Ù„Ù…ÙØ±Ø·\",\n",
        "\n",
        "    # ğŸ˜¥ Ø§Ù„Ù…Ù†ÙÙŠÙŠÙ† (Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ Ø§Ù„Ù…Ø¬Ø±ÙˆØ­Ø©)\n",
        "    \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§ÙŠÙ\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡Ù…ÙÙ„\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø¬ÙˆÙ„\",\n",
        "    \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ØºÙŠÙˆØ±\", \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\"\n",
        "]\n",
        "\n",
        "# Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø£Ù†Ù…Ø§Ø·\n",
        "MANAGERS_AR = [\"Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ\", \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\", \"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³\", \"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\",\n",
        "               \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø¯\", \"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø­ÙŠØ±Ø§Ù†\"]\n",
        "\n",
        "FIREFIGHTERS_AR = [\"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\", \"Ø§Ù„Ø¢ÙƒÙ„/Ø§Ù„Ù†Ù‡Ù…\", \"Ø§Ù„Ù„Ø§Ø¹Ø¨ Ø§Ù„Ù…ÙØ±Ø·\"]\n",
        "\n",
        "EXILES_AR = [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§ÙŠÙ\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡Ù…ÙÙ„\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø¬ÙˆÙ„\",\n",
        "             \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ØºÙŠÙˆØ±\", \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\"]\n",
        "\n",
        "# ==================== ØªØ¹ÙŠÙŠÙ†Ø§Øª Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ù…ØµØ±ÙŠ ====================\n",
        "QUESTION_MAPPINGS_AR = {\n",
        "    'Q1': {\n",
        "        0: [\"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\", \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\"],               # Ø¹Ù…Ù„ Ù‚ÙˆØ§Ø¦Ù… ÙˆØ®Ø·Ø·\n",
        "        1: [\"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø¯\"],       # Ø§Ù„Ø¶ØºØ· ÙˆØ§Ù„Ø¥Ù†Ø¬Ø§Ø²\n",
        "        2: [\"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\"],   # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†\n",
        "        3: [\"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\", \"Ø§Ù„Ù„Ø§Ø¹Ø¨ Ø§Ù„Ù…ÙØ±Ø·\"],        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¥Ù„Ù‡Ø§Ø¡\n",
        "        4: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø¯\", \"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\"],         # \"Ø£Ù†Ø§ Ù‡Ù‚Ø¯Ø± Ø£ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹\"\n",
        "        5: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø­ÙŠØ±Ø§Ù†\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯\"]     # ÙˆÙ‚Øª Ù‡Ø§Ø¯ÙŠ Ù„ÙˆØ­Ø¯ÙŠ\n",
        "    },\n",
        "\n",
        "    'Q2_slider': {\n",
        "        '0-20%': [],  # Ø¨Ø³ÙŠØ·\n",
        "        '21-50%': [\"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\"],  # Ø¨Ø¹Ø¶ Ø§Ù„ØµÙØ§Øª\n",
        "        '51-80%': [\"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\", \"Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ\"],  # Ù†Ø´ÙŠØ·\n",
        "        '81-100%': [\"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\", \"Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ\", \"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\"]  # Ù…Ø³ÙŠØ·Ø±\n",
        "    },\n",
        "\n",
        "    'Q3': {\n",
        "        0: [\"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\", \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\"],               # Ø§Ù„Ø¥ØµÙ„Ø§Ø­ Ø§Ù„ÙÙˆØ±ÙŠ\n",
        "        1: [\"Ø§Ù„Ø¢ÙƒÙ„/Ø§Ù„Ù†Ù‡Ù…\", \"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\"],           # Ø§Ù„Ø¥Ù„Ù‡Ø§Ø¡\n",
        "        2: [\"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§ÙŠÙ\"],      # Ø§Ù„Ø´Ø¹ÙˆØ± Ø¨Ø§Ù„ØµØºØ±\n",
        "        3: [\"Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø¯\"],    # \"Ù…ÙŠÙ†ÙØ¹Ø´ Ø£Ø­Ø³ ÙƒØ¯Ù‡\"\n",
        "        4: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø­ÙŠØ±Ø§Ù†\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡Ù…ÙÙ„\"],    # Ø§Ù„Ø§Ù†ÙØµØ§Ù„\n",
        "        5: [\"Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø¬ÙˆÙ„\"]     # Ø§Ù„ØºØ¶Ø¨ Ù…Ù† Ø§Ù„Ù†ÙØ³\n",
        "    },\n",
        "\n",
        "    'Q4_slider': {\n",
        "        '0-20%': [],  # Ø¨Ø³ÙŠØ·\n",
        "        '21-50%': [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯\"],  # Ø¨Ø¹Ø¶ Ø§Ù„ØµÙØ§Øª\n",
        "        '51-80%': [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\", \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\"],  # Ù†Ø´ÙŠØ·\n",
        "        '81-100%': [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡Ù…ÙÙ„\", \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\"]  # Ù…Ø³ÙŠØ·Ø±\n",
        "    },\n",
        "\n",
        "    'Q5': {\n",
        "        0: [\"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\", \"Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ\"],        # Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\n",
        "        1: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡Ù…ÙÙ„\"],    # Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯\n",
        "        2: [\"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§ÙŠÙ\"],         # Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…ØªØ­ÙƒÙ…\n",
        "        3: [\"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\", \"Ø§Ù„Ù„Ø§Ø¹Ø¨ Ø§Ù„Ù…ÙØ±Ø·\"],        # Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù‡Ø§Ø±Ø¨\n",
        "        4: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø¬ÙˆÙ„\", \"Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ\"],   # Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø¬ÙˆÙ„\n",
        "        5: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø­ÙŠØ±Ø§Ù†\", \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\"]     # Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø­ÙŠØ±Ø§Ù†\n",
        "    },\n",
        "\n",
        "    'Q6': {\n",
        "        0: [\"Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ\", \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\"],        # \"Ù…Ø´ Ù…Ø´ÙƒÙ„Ø© Ø§Ù†Ùƒ ØªØºÙ„Ø·\"\n",
        "        1: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§ÙŠÙ\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\"],   # \"Ø£Ù†Ø§ Ù…Ø´ Ù‡Ø³ÙŠØ¨Ùƒ Ù„ÙˆØ­Ø¯Ùƒ\"\n",
        "        2: [\"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø¬ÙˆÙ„\"],    # \"Ù…Ø´ Ù…Ø­ØªØ§Ø¬ ØªØ­Ø§ÙˆÙ„ Ø¹Ù„Ø´Ø§Ù† ØªØ§Ø®Ø¯ Ø­Ø¨\"\n",
        "        3: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡Ù…ÙÙ„\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø¯\"],    # \"Ù…Ø´Ø§Ø¹Ø±Ùƒ Ù„ÙŠÙ‡Ø§ Ù‚ÙŠÙ…Ø©\"\n",
        "        4: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§ÙŠÙ\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚\"],    # \"Ø£Ù†Øª ÙÙŠ Ø£Ù…Ø§Ù† Ø¯Ù„ÙˆÙ‚ØªÙŠ\"\n",
        "        5: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø¬ÙˆÙ„\", \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\"]      # \"Ø£Ù†Ø§ Ø¨Ù‚Ø¨Ù„ ÙƒÙ„ Ø­Ø§Ø¬Ø© ÙÙŠÙƒ\"\n",
        "    },\n",
        "\n",
        "    'Q7': {\n",
        "        0: [\"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\", \"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\"],           # Ø§Ù„Ø¹Ù…Ù„ Ø£ÙƒØ«Ø±\n",
        "        1: [\"Ø§Ù„Ø¢ÙƒÙ„/Ø§Ù„Ù†Ù‡Ù…\", \"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\"],           # Ø§Ù„Ø·Ø¹Ø§Ù… Ø§Ù„Ù…Ø±ÙŠØ­/Ø§Ù„Ù…Ø³Ù„Ø³Ù„Ø§Øª\n",
        "        2: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø¯\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡Ù…ÙÙ„\"],    # Ø§Ù„Ø§Ø³ØªØ³Ù„Ø§Ù… Ø§Ù„Ù†ÙØ³ÙŠ\n",
        "        3: [\"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\"],   # Ø¥Ø±Ø¶Ø§Ø¡ Ø§Ù„Ø¬Ù…ÙŠØ¹\n",
        "        4: [\"Ø§Ù„Ù„Ø§Ø¹Ø¨ Ø§Ù„Ù…ÙØ±Ø·\", \"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\"],        # Ø£Ù„Ø¹Ø§Ø¨ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ\n",
        "        5: [\"Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø¬ÙˆÙ„\"]    # Ø§Ù†ØªÙ‚Ø§Ø¯ Ø§Ù„Ù†ÙØ³\n",
        "    },\n",
        "\n",
        "    'Q8_slider': {\n",
        "        '0-20%': [],  # Ù†Ø´Ø§Ø· Ø¥Ø·ÙØ§Ø¦ÙŠ Ø¨Ø³ÙŠØ·\n",
        "        '21-50%': [\"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\", \"Ø§Ù„Ø¢ÙƒÙ„/Ø§Ù„Ù†Ù‡Ù…\"],  # Ø¨Ø¹Ø¶ Ø§Ù„ØµÙØ§Øª\n",
        "        '51-80%': [\"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\", \"Ø§Ù„Ø¢ÙƒÙ„/Ø§Ù„Ù†Ù‡Ù…\", \"Ø§Ù„Ù„Ø§Ø¹Ø¨ Ø§Ù„Ù…ÙØ±Ø·\"],  # Ø£Ù†Ù…Ø§Ø· Ù†Ø´ÙŠØ·Ø©\n",
        "        '81-100%': [\"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\", \"Ø§Ù„Ø¢ÙƒÙ„/Ø§Ù„Ù†Ù‡Ù…\", \"Ø§Ù„Ù„Ø§Ø¹Ø¨ Ø§Ù„Ù…ÙØ±Ø·\"]  # Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ù…Ø³ÙŠØ·Ø±Ø©\n",
        "    },\n",
        "\n",
        "    'Q9': {\n",
        "        0: [\"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\"],   # ÙˆØ¶Ø¹ Ø§Ù„Ø­Ø¯ÙˆØ¯\n",
        "        1: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§ÙŠÙ\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯\"],     # Ø§Ù„Ø«Ù‚Ø© ÙÙŠ Ø§Ù‡ØªÙ…Ø§Ù… Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†\n",
        "        2: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ØºÙŠÙˆØ±\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø¬ÙˆÙ„\"],     # Ø¹Ø¯Ù… Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©\n",
        "        3: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡Ù…ÙÙ„\"],   # Ø·Ù„Ø¨ Ø§Ù„Ø§Ø­ØªÙŠØ§Ø¬Ø§Øª\n",
        "        4: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯\", \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\"],     # Ø§Ù„Ø´Ø¹ÙˆØ± Ø¨Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø© ÙˆØ§Ù„ÙÙ‡Ù…\n",
        "        5: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§ÙŠÙ\"]     # Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø£Ø­Ø¯\n",
        "    },\n",
        "\n",
        "    'Q10': {\n",
        "        0: [\"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\"],   # \"Ù„Ø§Ø²Ù… ÙŠØ¹Ø¬Ø¨ÙˆØ§ Ø¨ÙŠÙƒ\"\n",
        "        1: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø¯\", \"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\"],         # \"Ù…Ø§ØªÙˆØ±Ù‘ÙŠØ´ Ø£ÙŠ Ù†Ù‚Ø·Ø© Ø¶Ø¹Ù\"\n",
        "        2: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ØºÙŠÙˆØ±\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø¬ÙˆÙ„\"],     # \"Ù‡Ù…Ø§ Ù…ØªØ­ÙƒÙ…ÙŠÙ† ÙÙŠ Ø­ÙŠØ§ØªÙ‡Ù… Ø£ÙƒØªØ± Ù…Ù†ÙŠ\"\n",
        "        3: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ØºÙŠÙˆØ±\", \"Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ\"],   # \"Ø¹Ø§ÙŠØ² Ø£Ø¨Ù‚Ù‰ Ø²ÙŠÙ‡Ù…\"\n",
        "        4: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø­ÙŠØ±Ø§Ù†\"],    # \"Ø£Ù†Ø§ Ù…Ø´ Ù…Ù†ØªÙ…ÙŠ Ù„ÙŠÙ‡Ù… Ù‡Ù†Ø§\"\n",
        "        5: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø¬ÙˆÙ„\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§ÙŠÙ\"]      # \"Ù„Ùˆ Ø´Ø§ÙÙˆØ§ Ø­Ù‚ÙŠÙ‚ØªÙŠ Ù‡ÙŠØ¹Ù…Ù„ÙˆØ§ Ø¥ÙŠÙ‡ØŸ\"\n",
        "    },\n",
        "\n",
        "    'Q11': {\n",
        "        0: [\"Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø¬ÙˆÙ„\"],   # \"Ù…Ø´ ÙƒÙØ§ÙŠØ©\"\n",
        "        1: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡Ù…ÙÙ„\"],    # ÙˆØ­Ø¯Ø© Ø¹Ù…ÙŠÙ‚Ø©\n",
        "        2: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§ÙŠÙ\", \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\"],          # Ù‚Ù„Ù‚ Ù…Ù† Ø§Ù„ÙØ´Ù„\n",
        "        3: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡Ù…ÙÙ„\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø­ÙŠØ±Ø§Ù†\"],   # Ø®Ø¯Ø± Ù†ÙØ³ÙŠ Ø£Ùˆ ÙØ±Ø§Øº\n",
        "        4: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø¬ÙˆÙ„\", \"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\"],       # Ø°Ù†Ø¨\n",
        "        5: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø­ÙŠØ±Ø§Ù†\", \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\"]     # Ø­ÙŠØ±Ø©\n",
        "    },\n",
        "\n",
        "    'Q12': {\n",
        "        0: [\"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\", \"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\"],            # Ù…Ø¯Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©\n",
        "        1: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡Ù…ÙÙ„\", \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\"],    # Ù…Ø¯Ù‰ Ø§Ù„Ø£Ù„Ù…\n",
        "        2: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§ÙŠÙ\", \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\"],          # Ø§Ù„Ø®ÙˆÙ Ù…Ù† Ø§Ù„ÙØ´Ù„\n",
        "        3: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡Ù…ÙÙ„\"],    # Ù‚ÙˆØ© Ø§Ù„ÙˆØ­Ø¯Ø©\n",
        "        4: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚\"],   # Ø§Ù„Ø­Ø§Ø¬Ø© Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©\n",
        "        5: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø­ÙŠØ±Ø§Ù†\", \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\"]     # Ù…Ø¯Ù‰ Ø§Ù„Ø­ÙŠØ±Ø©\n",
        "    },\n",
        "\n",
        "    'Q13': {\n",
        "        0: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§ÙŠÙ\"],    # Ø§Ù„Ø´Ø¹ÙˆØ± Ø¨Ø§Ù„Ø¥ØºØ±Ø§Ù‚\n",
        "        1: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ØºÙŠÙˆØ±\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø¬ÙˆÙ„\"],     # Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†\n",
        "        2: [\"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\", \"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\"],               # Ø§Ù„ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡\n",
        "        3: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø¬ÙˆÙ„\"],    # Ø§Ù„Ø´Ø¹ÙˆØ± Ø¨Ø§Ù„Ø­Ù…ÙˆÙ„Ø©\n",
        "        4: [\"Ø§Ù„Ù„Ø§Ø¹Ø¨ Ø§Ù„Ù…ÙØ±Ø·\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø­ÙŠØ±Ø§Ù†\"],   # Ø§Ù„Ù‡Ø±ÙˆØ¨ Ù„Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø®ÙŠØ§Ù„ÙŠ\n",
        "        5: [\"Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø¯\"],   # Ø§Ù„ØºØ¶Ø¨ Ù…Ù† Ø§Ù„Ù†ÙØ³\n",
        "        6: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\", \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\"],    # Ø§Ù„Ø±ØºØ¨Ø© ÙÙŠ Ø§Ù„Ø±Ø¹Ø§ÙŠØ©\n",
        "        7: [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡Ù…ÙÙ„\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø¯\"]      # Ø§Ù„Ø®Ø¯Ø± ÙˆØ¹Ø¯Ù… Ø§Ù„Ø¥Ø­Ø³Ø§Ø³\n",
        "    }\n",
        "}\n",
        "\n",
        "# Ø£Ù†Ù…Ø§Ø· Ù‚ÙˆÙŠØ© Ù„ÙƒÙ„ Ø´Ø®ØµÙŠØ© (Ù…Ø¤Ø´Ø±Ø§Øª Ø±Ø¦ÙŠØ³ÙŠØ©)\n",
        "CHARACTER_SIGNATURES_AR = {\n",
        "    \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\": [\"Q2:high\", \"Q1:0\", \"Q3:0\", \"Q5:0\"],\n",
        "    \"Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ\": [\"Q2:high\", \"Q3:3\", \"Q5:0\", \"Q11:0\"],\n",
        "    \"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³\": [\"Q10:0\", \"Q7:3\", \"Q9:0\", \"Q1:2\"],\n",
        "    \"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\": [\"Q1:0\", \"Q3:0\", \"Q10:1\", \"Q7:0\"],\n",
        "    \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø¯\": [\"Q1:4\", \"Q3:3\", \"Q7:2\", \"Q13:7\"],\n",
        "    \"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\": [\"Q1:1\", \"Q7:0\", \"Q12:0\", \"Q2:high\"],\n",
        "    \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø­ÙŠØ±Ø§Ù†\": [\"Q1:5\", \"Q3:4\", \"Q5:5\", \"Q11:5\"],\n",
        "    \"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\": [\"Q8:high\", \"Q1:3\", \"Q3:1\", \"Q7:4\"],\n",
        "    \"Ø§Ù„Ø¢ÙƒÙ„/Ø§Ù„Ù†Ù‡Ù…\": [\"Q8:high\", \"Q3:1\", \"Q7:1\", \"Q13:1\"],\n",
        "    \"Ø§Ù„Ù„Ø§Ø¹Ø¨ Ø§Ù„Ù…ÙØ±Ø·\": [\"Q8:high\", \"Q1:3\", \"Q7:4\", \"Q13:4\"],\n",
        "    \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯\": [\"Q4:high\", \"Q1:5\", \"Q11:1\", \"Q12:3\"],\n",
        "    \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§ÙŠÙ\": [\"Q11:2\", \"Q6:1\", \"Q9:1\", \"Q13:0\"],\n",
        "    \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡Ù…ÙÙ„\": [\"Q4:high\", \"Q3:4\", \"Q6:3\", \"Q11:3\"],\n",
        "    \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø¬ÙˆÙ„\": [\"Q11:0\", \"Q6:5\", \"Q9:2\", \"Q10:5\"],\n",
        "    \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚\": [\"Q13:0\", \"Q12:4\", \"Q6:4\", \"Q11:2\"],\n",
        "    \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\": [\"Q9:5\", \"Q12:4\", \"Q10:0\", \"Q6:1\"],\n",
        "    \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ØºÙŠÙˆØ±\": [\"Q10:2\", \"Q9:2\", \"Q13:1\", \"Q10:3\"],\n",
        "    \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\": [\"Q6:5\", \"Q12:1\", \"Q3:2\", \"Q9:4\"]\n",
        "}\n",
        "\n",
        "def generate_probabilistic_dataset_ar(num_samples=75000):\n",
        "    \"\"\"Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ù…Ø¹ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø´Ø®ØµÙŠØ§Øª\"\"\"\n",
        "\n",
        "    print(f\"Ø¬Ø§Ø±Ù Ø¥Ù†Ø´Ø§Ø¡ {num_samples:,} Ø¹ÙŠÙ†Ø© Ø¹Ø±Ø¨ÙŠØ© Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©...\")\n",
        "\n",
        "    dataset = []\n",
        "    slider_values = ['0-20%', '21-50%', '51-80%', '81-100%']\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        if i % 10000 == 0 and i > 0:\n",
        "            print(f\"  ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {i:,} Ø¹ÙŠÙ†Ø©...\")\n",
        "\n",
        "        responses = {}\n",
        "\n",
        "        # ===== 1. ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (Ø§Ù„Ø¬ÙˆÙ‡Ø±ÙŠØ©) =====\n",
        "        primary_char = random.choice(CHARACTERS_AR)\n",
        "        primary_archetype = None\n",
        "        if primary_char in MANAGERS_AR:\n",
        "            primary_archetype = \"Ù…Ø¯ÙŠØ±\"\n",
        "        elif primary_char in FIREFIGHTERS_AR:\n",
        "            primary_archetype = \"Ø¥Ø·ÙØ§Ø¦ÙŠ\"\n",
        "        else:\n",
        "            primary_archetype = \"Ù…Ù†ÙÙŠ\"\n",
        "\n",
        "        # ===== 2. ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø´Ø®ØµÙŠØ§Øª Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ© =====\n",
        "        secondary_chars = []\n",
        "        tertiary_chars = []\n",
        "\n",
        "        # Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©ØŒ Ø§Ù„Ø´Ø®ØµÙŠØ§Øª Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ© Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©\n",
        "        if primary_archetype == \"Ù…Ø¯ÙŠØ±\":\n",
        "            # Ø§Ù„Ù…Ø¯ÙŠØ±ÙˆÙ† ØºØ§Ù„Ø¨Ù‹Ø§ Ù…Ø§ ÙŠØ¹Ù…Ù„ÙˆÙ† Ù…Ø¹ Ù…Ø¯ÙŠØ±ÙŠÙ† Ø¢Ø®Ø±ÙŠÙ† Ø£Ùˆ Ù„Ø¯ÙŠÙ‡Ù… Ø£Ø¬Ø²Ø§Ø¡ Ù…Ù†ÙÙŠØ©\n",
        "            candidate_secondaries = [c for c in CHARACTERS_AR if c != primary_char]\n",
        "            # ØªÙØ¶ÙŠÙ„ Ù†ÙØ³ Ø§Ù„Ù†Ù…Ø· Ø£Ùˆ Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ Ø§Ù„Ù…Ù†ÙÙŠØ© Ø°Ø§Øª Ø§Ù„ØµÙ„Ø©\n",
        "            secondary = random.choice([c for c in candidate_secondaries if\n",
        "                                      (c in MANAGERS_AR) or\n",
        "                                      (primary_char in [\"Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ\", \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\"] and c in [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø¬ÙˆÙ„\", \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\"])])\n",
        "            secondary_chars.append(secondary)\n",
        "\n",
        "            # Ø§Ù„Ø«Ø§Ù„Ø«ÙŠØ©: ØºØ§Ù„Ø¨Ù‹Ø§ Ø¬Ø²Ø¡ Ù…Ù†ÙÙŠ\n",
        "            remaining = [c for c in EXILES_AR if c not in secondary_chars]\n",
        "            if remaining:\n",
        "                tertiary_chars.append(random.choice(remaining))\n",
        "\n",
        "        elif primary_archetype == \"Ø¥Ø·ÙØ§Ø¦ÙŠ\":\n",
        "            # Ø±Ø¬Ø§Ù„ Ø§Ù„Ø¥Ø·ÙØ§Ø¡ ØºØ§Ù„Ø¨Ù‹Ø§ Ù…Ø§ ÙŠØ¹Ù…Ù„ÙˆÙ† Ù…Ø¹Ù‹Ø§ Ø£Ùˆ Ù„Ø¯ÙŠÙ‡Ù… Ù…Ø¯ÙŠØ±ÙŠÙ† ÙŠØ­ÙØ²ÙˆÙ†Ù‡Ù…\n",
        "            candidate_secondaries = [c for c in FIREFIGHTERS_AR if c != primary_char]\n",
        "            if candidate_secondaries:\n",
        "                secondary_chars.append(random.choice(candidate_secondaries))\n",
        "\n",
        "            # ØºØ§Ù„Ø¨Ù‹Ø§ Ù„Ø¯ÙŠÙ‡Ù… Ù…Ø¯ÙŠØ± ØµØ§Ø±Ù… Ø¬Ø¯Ù‹Ø§\n",
        "            tertiary_chars.append(random.choice([\"Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ\", \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\", \"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\"]))\n",
        "\n",
        "        else:  # Ù…Ù†ÙÙŠ\n",
        "            # Ø§Ù„Ù…Ù†ÙÙŠÙˆÙ† ØºØ§Ù„Ø¨Ù‹Ø§ ÙŠØ£ØªÙˆÙ† Ù…Ø¹ Ø­Ù…Ø§Ø© (Ù…Ø¯ÙŠØ±ÙŠÙ†/Ø¥Ø·ÙØ§Ø¦ÙŠÙŠÙ†)\n",
        "            if primary_char in [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡Ù…ÙÙ„\"]:\n",
        "                secondary_chars.append(random.choice([\"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³\", \"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\"]))\n",
        "                tertiary_chars.append(random.choice([\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø¯\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø­ÙŠØ±Ø§Ù†\"]))\n",
        "            elif primary_char in [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø¬ÙˆÙ„\", \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\"]:\n",
        "                secondary_chars.append(random.choice([\"Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ\", \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\"]))\n",
        "                tertiary_chars.append(\"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\")\n",
        "            else:\n",
        "                secondary_chars.append(random.choice(MANAGERS_AR + FIREFIGHTERS_AR))\n",
        "\n",
        "        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ 2-4 Ø´Ø®ØµÙŠØ§Øª Ø¥Ø¬Ù…Ø§Ù„Ø§Ù‹\n",
        "        all_chars = list(set([primary_char] + secondary_chars + tertiary_chars))\n",
        "        if len(all_chars) > 4:\n",
        "            all_chars = random.sample(all_chars, 4)\n",
        "        elif len(all_chars) < 2:\n",
        "            all_chars.append(random.choice([c for c in CHARACTERS_AR if c not in all_chars]))\n",
        "\n",
        "        # ===== 3. ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø®ØµÙŠØ§Øª =====\n",
        "        # ØªÙˆÙ„ÙŠØ¯ Ø¥Ø¬Ø§Ø¨Ø§Øª ØªØ¹ÙƒØ³ Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø¨Ù‚ÙˆØ©\n",
        "        for q in ['Q1', 'Q3', 'Q5', 'Q6', 'Q7', 'Q9', 'Q10', 'Q11', 'Q12', 'Q13']:\n",
        "            # Ø§Ù„Ø¨Ø¯Ø¡ Ø¨Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ù…ÙØ¶Ù„Ø© Ù„Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
        "            primary_answers = []\n",
        "            for ans, chars in QUESTION_MAPPINGS_AR.get(q, {}).items():\n",
        "                if primary_char in chars:\n",
        "                    primary_answers.append(ans)\n",
        "\n",
        "            # Ø¥Ø¶Ø§ÙØ© Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ø´Ø®ØµÙŠØ§Øª Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ©\n",
        "            secondary_answers = []\n",
        "            for char in secondary_chars:\n",
        "                for ans, chars in QUESTION_MAPPINGS_AR.get(q, {}).items():\n",
        "                    if char in chars:\n",
        "                        secondary_answers.append(ans)\n",
        "\n",
        "            # Ø§Ù„Ø¯Ù…Ø¬ Ù…Ø¹ Ø£ÙˆØ²Ø§Ù†: Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© 60ÙªØŒ Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ© 40Ùª\n",
        "            all_answers = []\n",
        "            if primary_answers:\n",
        "                n_primary = random.randint(1, min(2, len(primary_answers)))\n",
        "                all_answers.extend(random.sample(primary_answers, n_primary))\n",
        "\n",
        "            if secondary_answers:\n",
        "                n_secondary = random.randint(0, min(1, len(secondary_answers)))\n",
        "                if n_secondary > 0:\n",
        "                    all_answers.extend(random.sample(secondary_answers, n_secondary))\n",
        "\n",
        "            # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ 1-3 Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¥Ø¬Ù…Ø§Ù„Ø§Ù‹\n",
        "            if not all_answers:\n",
        "                max_opt = 6 if q != 'Q13' else 8\n",
        "                all_answers = random.sample(range(max_opt), random.randint(1, 2))\n",
        "\n",
        "            if len(all_answers) > 3:\n",
        "                all_answers = random.sample(all_answers, 3)\n",
        "\n",
        "            responses[q] = ','.join(map(str, sorted(set(all_answers))))\n",
        "\n",
        "        # ØªÙˆÙ„ÙŠØ¯ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø£Ø´Ø±Ø·Ø© Ø§Ù„ØªÙ…Ø±ÙŠØ±\n",
        "        def get_slider_for_char_ar(char, q_prefix):\n",
        "            \"\"\"Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚ÙŠÙ…Ø© Ø´Ø±ÙŠØ· Ø§Ù„ØªÙ…Ø±ÙŠØ± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø®ØµÙŠØ©\"\"\"\n",
        "            if char == \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\" and q_prefix == \"Q2\":\n",
        "                return random.choices(['51-80%', '81-100%'], weights=[40, 60])[0]\n",
        "            elif char == \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯\" and q_prefix == \"Q4\":\n",
        "                return random.choices(['51-80%', '81-100%'], weights=[40, 60])[0]\n",
        "            elif char in FIREFIGHTERS_AR and q_prefix == \"Q8\":\n",
        "                return random.choices(['51-80%', '81-100%'], weights=[40, 60])[0]\n",
        "            else:\n",
        "                return random.choice(slider_values)\n",
        "\n",
        "        # Ø£Ø´Ø±Ø·Ø© Ø§Ù„ØªÙ…Ø±ÙŠØ± ØªØ¹ÙƒØ³ Ø£Ù‚ÙˆÙ‰ Ø´Ø®ØµÙŠØ© Ù„ØªÙ„Ùƒ Ø§Ù„Ø³Ù…Ø©\n",
        "        responses['Q2'] = get_slider_for_char_ar(primary_char, \"Q2\")\n",
        "        responses['Q4'] = get_slider_for_char_ar(primary_char, \"Q4\")\n",
        "        responses['Q8'] = get_slider_for_char_ar(primary_char, \"Q8\")\n",
        "\n",
        "        # ===== 4. Ø¥Ù†Ø´Ø§Ø¡ Ø£Ù‡Ø¯Ø§Ù Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© =====\n",
        "        # Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: 80-100Ùª Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©\n",
        "        # Ø§Ù„Ø´Ø®ØµÙŠØ§Øª Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ©: 40-70Ùª Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©\n",
        "        # Ø§Ù„Ø´Ø®ØµÙŠØ§Øª Ø§Ù„Ø«Ø§Ù„Ø«ÙŠØ©: 20-50Ùª Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©\n",
        "        # Ø§Ù„Ø¢Ø®Ø±ÙˆÙ†: 0-20Ùª Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ©\n",
        "\n",
        "        for char in CHARACTERS_AR:\n",
        "            col_name = f'prob_{char.replace(\" \", \"_\").replace(\"/\", \"_\")}'\n",
        "\n",
        "            if char == primary_char:\n",
        "                # Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©: Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø¹Ø§Ù„ÙŠØ©\n",
        "                responses[col_name] = round(random.uniform(0.8, 0.98), 2)\n",
        "            elif char in secondary_chars:\n",
        "                # Ø§Ù„Ø«Ø§Ù†ÙˆÙŠØ©: Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ù…ØªÙˆØ³Ø·Ø©-Ø¹Ø§Ù„ÙŠØ©\n",
        "                responses[col_name] = round(random.uniform(0.4, 0.7), 2)\n",
        "            elif char in tertiary_chars:\n",
        "                # Ø§Ù„Ø«Ø§Ù„Ø«ÙŠØ©: Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ù…ØªÙˆØ³Ø·Ø©-Ù…Ù†Ø®ÙØ¶Ø©\n",
        "                responses[col_name] = round(random.uniform(0.2, 0.5), 2)\n",
        "            else:\n",
        "                # Ø§Ù„Ø¢Ø®Ø±ÙˆÙ†: Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ù…Ù†Ø®ÙØ¶Ø©ØŒ Ù„ÙƒÙ† Ù„ÙŠØ³Øª ØµÙØ±\n",
        "                responses[col_name] = round(random.uniform(0.0, 0.2), 2)\n",
        "\n",
        "        # ===== 5. Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø´ØªÙ‚Ø© =====\n",
        "        # Ø§Ù„Ù…ÙŠØ²Ø© 1: Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ù„ÙƒÙ„ Ø³Ø¤Ø§Ù„\n",
        "        for q in ['Q1', 'Q3', 'Q5', 'Q6', 'Q7', 'Q9', 'Q10', 'Q11', 'Q12', 'Q13']:\n",
        "            responses[f'{q}_count'] = len(responses[q].split(','))\n",
        "\n",
        "        # Ø§Ù„Ù…ÙŠØ²Ø© 2: ØªØ­ÙˆÙŠÙ„ Ø£Ø´Ø±Ø·Ø© Ø§Ù„ØªÙ…Ø±ÙŠØ± Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù…\n",
        "        slider_map = {'0-20%': 0.1, '21-50%': 0.35, '51-80%': 0.65, '81-100%': 0.9}\n",
        "        responses['Q2_num'] = slider_map[responses['Q2']]\n",
        "        responses['Q4_num'] = slider_map[responses['Q4']]\n",
        "        responses['Q8_num'] = slider_map[responses['Q8']]\n",
        "\n",
        "        # Ø§Ù„Ù…ÙŠØ²Ø© 3: Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø£Ù†Ù…Ø§Ø·\n",
        "        responses['has_manager_indicators'] = int(\n",
        "            responses['Q2_num'] > 0.6 or  # Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠØ©\n",
        "            responses.get('Q1', '').split(',').count('0') > 0 or  # Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\n",
        "            responses.get('Q10', '').split(',').count('0') > 0   # Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³\n",
        "        )\n",
        "\n",
        "        responses['has_exile_indicators'] = int(\n",
        "            responses['Q4_num'] > 0.6 or  # Ø§Ù„ÙˆØ­Ø¯Ø©\n",
        "            responses.get('Q11', '').split(',').count('1') > 0 or  # Ø´Ø¹ÙˆØ± Ø§Ù„ÙˆØ­Ø¯Ø©\n",
        "            responses.get('Q13', '').split(',').count('0') > 0    # Ø§Ù„Ø¥Ø±Ù‡Ø§Ù‚\n",
        "        )\n",
        "\n",
        "        responses['has_firefighter_indicators'] = int(\n",
        "            responses['Q8_num'] > 0.6 or  # Ø§Ù„Ù‡Ø±ÙˆØ¨\n",
        "            responses.get('Q7', '').split(',').count('1') > 0 or  # Ø§Ù„Ø£ÙƒÙ„ Ø§Ù„Ù…Ø±ÙŠØ­\n",
        "            responses.get('Q7', '').split(',').count('4') > 0    # Ø§Ù„Ø£Ù„Ø¹Ø§Ø¨\n",
        "        )\n",
        "\n",
        "        dataset.append(responses)\n",
        "\n",
        "    # Ø¥Ù†Ø´Ø§Ø¡ DataFrame\n",
        "    question_cols = [f'Q{i}' for i in range(1, 14)]\n",
        "    derived_cols = [col for col in dataset[0].keys() if col.startswith('Q') and '_num' in col or '_count' in col or 'has_' in col]\n",
        "    prob_cols = [f'prob_{char.replace(\" \", \"_\").replace(\"/\", \"_\")}' for char in CHARACTERS_AR]\n",
        "\n",
        "    all_cols = question_cols + derived_cols + prob_cols\n",
        "    df = pd.DataFrame(dataset)[all_cols]\n",
        "\n",
        "    return df, CHARACTERS_AR\n",
        "\n",
        "def create_top3_dataset_ar(df, characters):\n",
        "    \"\"\"ØªØ­ÙˆÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ Ø£ÙØ¶Ù„ 3 Ù„Ù„ØªØ¯Ø±ÙŠØ¨\"\"\"\n",
        "\n",
        "    prob_cols = [f'prob_{char.replace(\" \", \"_\").replace(\"/\", \"_\")}' for char in characters]\n",
        "    prob_matrix = df[prob_cols].values\n",
        "\n",
        "    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ 3 Ù…Ø¤Ø´Ø±Ø§Øª Ù„ÙƒÙ„ ØµÙ\n",
        "    top3_indices = np.argsort(prob_matrix, axis=1)[:, -3:][:, ::-1]\n",
        "\n",
        "    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© Ù…Ø¹ Ø£ÙØ¶Ù„ 3 Ø´Ø®ØµÙŠØ§Øª ÙƒØ³Ù„Ø§Ø³Ù„ Ù†ØµÙŠØ©\n",
        "    top3_data = []\n",
        "    for idx, row in df.iterrows():\n",
        "        row_data = {col: row[col] for col in df.columns if not col.startswith('prob_')}\n",
        "\n",
        "        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ 3 Ø´Ø®ØµÙŠØ§Øª ÙˆØ§Ø­ØªÙ…Ø§Ù„Ø§ØªÙ‡Ø§\n",
        "        top3_chars = []\n",
        "        top3_probs = []\n",
        "\n",
        "        for rank in range(3):\n",
        "            char_idx = top3_indices[idx, rank]\n",
        "            char_name = characters[char_idx]\n",
        "            prob = prob_matrix[idx, char_idx]\n",
        "\n",
        "            row_data[f'top{rank+1}_char'] = char_name\n",
        "            row_data[f'top{rank+1}_prob'] = round(prob, 2)\n",
        "\n",
        "            top3_chars.append(char_name)\n",
        "            top3_probs.append(prob)\n",
        "\n",
        "        # Ø¥Ø¶Ø§ÙØ© Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø«Ù‚Ø©\n",
        "        row_data['confidence_score'] = round(np.mean(top3_probs), 2)\n",
        "        row_data['margin_1_2'] = round(top3_probs[0] - top3_probs[1], 2)\n",
        "        row_data['top3_list'] = ','.join(top3_chars)\n",
        "\n",
        "        top3_data.append(row_data)\n",
        "\n",
        "    top3_df = pd.DataFrame(top3_data)\n",
        "    return top3_df\n",
        "\n",
        "def analyze_top3_dataset_ar(df, characters):\n",
        "    \"\"\"ØªØ­Ù„ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ø£ÙØ¶Ù„ 3\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ØªØ­Ù„ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ø£ÙØ¶Ù„ 3 Ø´Ø®ØµÙŠØ§Øª\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Ø¸Ù‡ÙˆØ± Ø§Ù„Ø´Ø®ØµÙŠØ§Øª ÙÙŠ Ø£ÙØ¶Ù„ 3\n",
        "    char_counts = {char: 0 for char in characters}\n",
        "\n",
        "    for i in range(1, 4):\n",
        "        col = f'top{i}_char'\n",
        "        counts = df[col].value_counts()\n",
        "        for char, count in counts.items():\n",
        "            char_counts[char] += count\n",
        "\n",
        "    print(\"Ø¸Ù‡ÙˆØ± Ø§Ù„Ø´Ø®ØµÙŠØ§Øª ÙÙŠ Ù…Ø±Ø§ÙƒØ² Ø£ÙØ¶Ù„ 3:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    sorted_chars = sorted(char_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "    for char, count in sorted_chars:\n",
        "        percentage = (count / (len(df) * 3)) * 100\n",
        "        print(f\"{char:25} {count:6,} ({percentage:5.1f}% Ù…Ù† Ù…Ø±Ø§ÙƒØ² Ø£ÙØ¶Ù„ 3)\")\n",
        "\n",
        "    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø«Ù‚Ø©\n",
        "    print(f\"\\nØ¥Ø­ØµØ§Ø¡Ø§Øª Ø§Ù„Ø«Ù‚Ø©:\")\n",
        "    print(f\"Ù…ØªÙˆØ³Ø· Ø§Ù„Ø«Ù‚Ø©: {df['confidence_score'].mean():.2f}\")\n",
        "    print(f\"Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© (>0.7): {(df['confidence_score'] > 0.7).sum():,} Ø¹ÙŠÙ†Ø©\")\n",
        "    print(f\"Ø«Ù‚Ø© Ù…ØªÙˆØ³Ø·Ø© (0.4-0.7): {((df['confidence_score'] >= 0.4) & (df['confidence_score'] <= 0.7)).sum():,} Ø¹ÙŠÙ†Ø©\")\n",
        "    print(f\"Ø«Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø© (<0.4): {(df['confidence_score'] < 0.4).sum():,} Ø¹ÙŠÙ†Ø©\")\n",
        "\n",
        "    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‡Ø§Ù…Ø´\n",
        "    print(f\"\\nØ§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ø£ÙˆÙ„ ÙˆØ§Ù„Ø«Ø§Ù†ÙŠ:\")\n",
        "    print(f\"Ù…ØªÙˆØ³Ø· Ø§Ù„ÙØ±Ù‚: {df['margin_1_2'].mean():.2f}\")\n",
        "    print(f\"ÙØ§Ø¦Ø² ÙˆØ§Ø¶Ø­ (ÙØ±Ù‚ > 0.2): {(df['margin_1_2'] > 0.2).sum():,} Ø¹ÙŠÙ†Ø©\")\n",
        "    print(f\"Ù…Ù†Ø§ÙØ³Ø© Ù‚Ø±ÙŠØ¨Ø© (ÙØ±Ù‚ < 0.1): {(df['margin_1_2'] < 0.1).sum():,} Ø¹ÙŠÙ†Ø©\")\n",
        "\n",
        "    # ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ù†Ù…Ø§Ø· ÙÙŠ Ø§Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ø£ÙˆÙ„\n",
        "    top1_archetypes = []\n",
        "    for char in df['top1_char']:\n",
        "        if char in MANAGERS_AR:\n",
        "            top1_archetypes.append(\"Ù…Ø¯ÙŠØ±\")\n",
        "        elif char in FIREFIGHTERS_AR:\n",
        "            top1_archetypes.append(\"Ø¥Ø·ÙØ§Ø¦ÙŠ\")\n",
        "        else:\n",
        "            top1_archetypes.append(\"Ù…Ù†ÙÙŠ\")\n",
        "\n",
        "    archetype_counts = pd.Series(top1_archetypes).value_counts()\n",
        "    print(f\"\\nÙ†Ù…Ø· Ø§Ù„Ø´Ø®ØµÙŠØ© ÙÙŠ Ø§Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ø£ÙˆÙ„:\")\n",
        "    for archetype, count in archetype_counts.items():\n",
        "        print(f\"  {archetype}: {count:,} Ø¹ÙŠÙ†Ø© ({count/len(df):.1%})\")\n",
        "\n",
        "def prepare_model_training_ar(df):\n",
        "    \"\"\"ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…ÙŠØ²Ø§Øª ÙˆØ§Ù„Ø£Ù‡Ø¯Ø§Ù Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\"\"\"\n",
        "\n",
        "    # Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø£Ø³Ø¦Ù„Ø©\n",
        "    question_cols = [f'Q{i}' for i in range(1, 14)]\n",
        "    derived_cols = [col for col in df.columns if col.endswith('_num') or col.endswith('_count') or ('has_' in col and 'indicator' in col)]\n",
        "    feature_cols = question_cols + derived_cols\n",
        "\n",
        "    # Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø£Ù‡Ø¯Ø§Ù (Ø£ÙØ¶Ù„ 3 Ø´Ø®ØµÙŠØ§Øª)\n",
        "    target_cols = ['top1_char', 'top2_char', 'top3_char']\n",
        "\n",
        "    print(f\"\\nØ¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨:\")\n",
        "    print(f\"Ø§Ù„Ù…ÙŠØ²Ø§Øª: {len(feature_cols)} Ø¹Ù…ÙˆØ¯\")\n",
        "    print(f\"Ø§Ù„Ø¹ÙŠÙ†Ø§Øª: {len(df):,}\")\n",
        "\n",
        "    return df[feature_cols], df[target_cols], feature_cols\n",
        "\n",
        "# ==================== Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù†Ø© ====================\n",
        "print(\"=\"*60)\n",
        "print(\"Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ANA Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù†Ø© Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø£ÙØ¶Ù„ 3 Ø´Ø®ØµÙŠØ§Øª\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "DATASET_SIZE = 100000  # Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø£ÙƒØ¨Ø± Ù„ØªØ¹Ù„Ù… Ø£ÙØ¶Ù„\n",
        "prob_df_ar, characters_ar = generate_probabilistic_dataset_ar(DATASET_SIZE)\n",
        "\n",
        "# Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ Ø£ÙØ¶Ù„ 3\n",
        "top3_df_ar = create_top3_dataset_ar(prob_df_ar, characters_ar)\n",
        "\n",
        "# Ø­ÙØ¸ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "prob_df_ar.to_csv('ana_dataset_probabilistic_ar.csv', index=False, encoding='utf-8-sig')\n",
        "top3_df_ar.to_csv('ana_dataset_top3_ar.csv', index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(f\"\\nâœ… ØªÙ… Ø­ÙØ¸ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©: 'ana_dataset_probabilistic_ar.csv'\")\n",
        "print(f\"âœ… ØªÙ… Ø­ÙØ¸ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ø£ÙØ¶Ù„ 3: 'ana_dataset_top3_ar.csv'\")\n",
        "\n",
        "# Ø§Ù„ØªØ­Ù„ÙŠÙ„\n",
        "analyze_top3_dataset_ar(top3_df_ar, characters_ar)\n",
        "\n",
        "# Ø§Ù„ØªØ­Ø¶ÙŠØ± Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\n",
        "X_ar, y_ar, feature_cols_ar = prepare_model_training_ar(top3_df_ar)\n",
        "\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(\"Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¬Ø§Ù‡Ø²Ø© Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Ø¹Ø±Ø¶ Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "print(\"\\nØ¹ÙŠÙ†Ø© Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:\")\n",
        "print(\"-\" * 50)\n",
        "sample = top3_df_ar.head(3)\n",
        "for i, (_, row) in enumerate(sample.iterrows()):\n",
        "    print(f\"\\nØ¹ÙŠÙ†Ø© {i+1}:\")\n",
        "    print(f\"  Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø£ÙˆÙ„Ù‰: {row['top1_char']} (Ø§Ø­ØªÙ…Ø§Ù„: {row['top1_prob']})\")\n",
        "    print(f\"  Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: {row['top2_char']} (Ø§Ø­ØªÙ…Ø§Ù„: {row['top2_prob']})\")\n",
        "    print(f\"  Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø«Ø§Ù„Ø«Ø©: {row['top3_char']} (Ø§Ø­ØªÙ…Ø§Ù„: {row['top3_prob']})\")\n",
        "    print(f\"  Ø¯Ø±Ø¬Ø© Ø§Ù„Ø«Ù‚Ø©: {row['confidence_score']}\")\n",
        "    print(f\"  Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ø£ÙˆÙ„ ÙˆØ§Ù„Ø«Ø§Ù†ÙŠ: {row['margin_1_2']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boC9GhZqzd_Z",
        "outputId": "3cc9159a-11ad-48db-9913-777f08b7c715"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "ğŸš€ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¬Ø§Ù‡Ø² Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø´Ø®ØµÙŠØ§Øª (Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©)\n",
            "================================================================================\n",
            "\n",
            "1. ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©...\n",
            "   âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: 100,000 Ø¹ÙŠÙ†Ø©\n",
            "\n",
            "2. Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©...\n",
            "\n",
            "3. ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠ Ø§Ù„Ø«Ø§Ø¨Øª Ù…Ø¹ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù„ÙØ¦Ø§Øª...\n",
            "\n",
            "================================================================================\n",
            "ğŸš€ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠ Ø§Ù„Ø«Ø§Ø¨Øª Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\n",
            "================================================================================\n",
            "   Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª...\n",
            "   ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ 18 ÙØ¦Ø© ÙØ±ÙŠØ¯Ø© ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
            "   ØªØµÙ†ÙŠÙ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ù†Ù…Ø§Ø·...\n",
            "   ØªÙˆØ²ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ù†Ù…Ø§Ø·:\n",
            "mixed         44274\n",
            "ambiguous     28511\n",
            "very_clear    21628\n",
            "clear          5587\n",
            "Name: count, dtype: int64\n",
            "   ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ...\n",
            "   ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¬Ù…ÙŠØ¹...\n",
            "   Ø¯Ù‚Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ: 95.80%\n",
            "   ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø§Ø­ØªÙŠØ§Ø· Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© Ø¨Ø§Ù„Ø£Ù†Ù…Ø§Ø·...\n",
            "     ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ mixed (44,274 Ø¹ÙŠÙ†Ø©)...\n",
            "       Ø¯Ù‚Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: 97.00%\n",
            "     ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ ambiguous (28,511 Ø¹ÙŠÙ†Ø©)...\n",
            "       Ø¯Ù‚Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: 95.00%\n",
            "     ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ very_clear (21,628 Ø¹ÙŠÙ†Ø©)...\n",
            "       Ø¯Ù‚Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: 91.00%\n",
            "     ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ clear (5,587 Ø¹ÙŠÙ†Ø©)...\n",
            "       Ø¯Ù‚Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: 88.00%\n",
            "   âœ… ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ù†Ø¬Ø§Ø­\n",
            "   âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ fixed_production_character_predictor_ar.pkl\n",
            "\n",
            "================================================================================\n",
            "ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠ Ø§Ù„Ø«Ø§Ø¨Øª Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\n",
            "================================================================================\n",
            "\n",
            "ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "1. Ø§Ø®ØªØ¨Ø§Ø± 1: Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ Ø§Ù„ÙˆØ§Ø¶Ø­ Ø¬Ø¯Ø§Ù‹\n",
            "----------------------------------------\n",
            "Ø§Ù„ØªÙ†Ø¨Ø¤: Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ (85.0%)\n",
            "Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·: VERY_CLEAR\n",
            "ØªØµÙ†ÙŠÙ Ø§Ù„Ø«Ù‚Ø©: Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©\n",
            "Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ÙŠÙ‚ÙŠÙ†: ÙŠÙ‚ÙŠÙ† Ø¹Ø§Ù„ÙŠ Ø¬Ø¯Ø§Ù‹\n",
            "Ø£ÙØ¶Ù„ 3: Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ, Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…, Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\n",
            "Ù‚ÙˆØ© Ø§Ù„Ø£Ø¯Ù„Ø©: VERY_STRONG\n",
            "Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø£Ø¯Ù„Ø©: 1.00\n",
            "âœ… Ø§Ù„Ø«Ù‚Ø© ÙˆØ§Ù‚Ø¹ÙŠØ© Ù„Ù†Ù…Ø· very_clear\n",
            "\n",
            "2. Ø§Ø®ØªØ¨Ø§Ø± 2: Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³ Ø§Ù„ÙˆØ§Ø¶Ø­\n",
            "----------------------------------------\n",
            "Ø§Ù„ØªÙ†Ø¨Ø¤: Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯ (87.6%)\n",
            "Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·: VERY_CLEAR\n",
            "ØªØµÙ†ÙŠÙ Ø§Ù„Ø«Ù‚Ø©: Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©\n",
            "Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ÙŠÙ‚ÙŠÙ†: ÙŠÙ‚ÙŠÙ† Ø¹Ø§Ù„ÙŠ Ø¬Ø¯Ø§Ù‹\n",
            "Ø£ÙØ¶Ù„ 3: Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯, Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³, Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚\n",
            "Ù‚ÙˆØ© Ø§Ù„Ø£Ø¯Ù„Ø©: STRONG\n",
            "Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø£Ø¯Ù„Ø©: 0.57\n",
            "âœ… Ø§Ù„Ø«Ù‚Ø© ÙˆØ§Ù‚Ø¹ÙŠØ© Ù„Ù†Ù…Ø· very_clear\n",
            "\n",
            "3. Ø§Ø®ØªØ¨Ø§Ø± 3: Ù…Ø®ØªÙ„Ø· (Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ + Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…)\n",
            "----------------------------------------\n",
            "Ø§Ù„ØªÙ†Ø¨Ø¤: Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ (85.0%)\n",
            "Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·: VERY_CLEAR\n",
            "ØªØµÙ†ÙŠÙ Ø§Ù„Ø«Ù‚Ø©: Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©\n",
            "Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ÙŠÙ‚ÙŠÙ†: ÙŠÙ‚ÙŠÙ† Ø¹Ø§Ù„ÙŠ Ø¬Ø¯Ø§Ù‹\n",
            "Ø£ÙØ¶Ù„ 3: Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ, Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…, Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\n",
            "Ù‚ÙˆØ© Ø§Ù„Ø£Ø¯Ù„Ø©: VERY_STRONG\n",
            "Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø£Ø¯Ù„Ø©: 1.00\n",
            "âœ… Ø§Ù„Ø«Ù‚Ø© ÙˆØ§Ù‚Ø¹ÙŠØ© Ù„Ù†Ù…Ø· very_clear\n",
            "\n",
            "4. Ø§Ø®ØªØ¨Ø§Ø± 4: ØºØ§Ù…Ø¶ (Ø£Ù†Ù…Ø§Ø· Ù…ØªØ¹Ø¯Ø¯Ø©)\n",
            "----------------------------------------\n",
            "Ø§Ù„ØªÙ†Ø¨Ø¤: Ø§Ù„Ø¢ÙƒÙ„/Ø§Ù„Ù†Ù‡Ù… (58.2%)\n",
            "Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·: MIXED\n",
            "ØªØµÙ†ÙŠÙ Ø§Ù„Ø«Ù‚Ø©: Ø«Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø©\n",
            "Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ÙŠÙ‚ÙŠÙ†: ÙŠÙ‚ÙŠÙ† Ù…Ù†Ø®ÙØ¶\n",
            "Ø£ÙØ¶Ù„ 3: Ø§Ù„Ø¢ÙƒÙ„/Ø§Ù„Ù†Ù‡Ù…, Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚, Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ØºÙŠÙˆØ±\n",
            "Ù‚ÙˆØ© Ø§Ù„Ø£Ø¯Ù„Ø©: STRONG\n",
            "Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø£Ø¯Ù„Ø©: 0.45\n",
            "âš ï¸  ØªØ­Ø°ÙŠØ±Ø§Øª:\n",
            "   â€¢ Ø§Ø®ØªÙŠØ§Ø±Ø§Øª Ø¥Ø¬Ù…Ø§Ù„ÙŠØ© ÙƒØ«ÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹ (34)ØŒ Ù‚Ø¯ ØªÙƒÙˆÙ† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª ØºÙŠØ± Ø¯Ù‚ÙŠÙ‚Ø©\n",
            "âœ… Ø§Ù„Ø«Ù‚Ø© ÙˆØ§Ù‚Ø¹ÙŠØ© Ù„Ù†Ù…Ø· mixed\n",
            "\n",
            "5. Ø§Ø®ØªØ¨Ø§Ø± 5: Ù…Ø®ØªÙ„Ø· Ø¬Ø¯Ø§Ù‹ (ÙƒÙ„ Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª)\n",
            "----------------------------------------\n",
            "Ø§Ù„ØªÙ†Ø¨Ø¤: Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ØºÙŠÙˆØ± (50.6%)\n",
            "Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·: MIXED\n",
            "ØªØµÙ†ÙŠÙ Ø§Ù„Ø«Ù‚Ø©: Ø«Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø©\n",
            "Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ÙŠÙ‚ÙŠÙ†: ÙŠÙ‚ÙŠÙ† Ù…Ù†Ø®ÙØ¶\n",
            "Ø£ÙØ¶Ù„ 3: Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ØºÙŠÙˆØ±, Ø§Ù„Ø¢ÙƒÙ„/Ø§Ù„Ù†Ù‡Ù…, Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚\n",
            "Ù‚ÙˆØ© Ø§Ù„Ø£Ø¯Ù„Ø©: VERY_STRONG\n",
            "Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø£Ø¯Ù„Ø©: 1.00\n",
            "âš ï¸  ØªØ­Ø°ÙŠØ±Ø§Øª:\n",
            "   â€¢ Ø§Ù„Ø³Ø¤Ø§Ù„ Q1: Ø§Ø®ØªÙŠØ§Ø±Ø§Øª ÙƒØ«ÙŠØ±Ø© (6)ØŒ Ù‚Ø¯ ØªØ´ÙŠØ± Ø¥Ù„Ù‰ Ø§Ù„ØªØ±Ø¯Ø¯\n",
            "   â€¢ Ø§Ù„Ø³Ø¤Ø§Ù„ Q3: Ø§Ø®ØªÙŠØ§Ø±Ø§Øª ÙƒØ«ÙŠØ±Ø© (6)ØŒ Ù‚Ø¯ ØªØ´ÙŠØ± Ø¥Ù„Ù‰ Ø§Ù„ØªØ±Ø¯Ø¯\n",
            "   â€¢ Ø§Ù„Ø³Ø¤Ø§Ù„ Q5: Ø§Ø®ØªÙŠØ§Ø±Ø§Øª ÙƒØ«ÙŠØ±Ø© (6)ØŒ Ù‚Ø¯ ØªØ´ÙŠØ± Ø¥Ù„Ù‰ Ø§Ù„ØªØ±Ø¯Ø¯\n",
            "   â€¢ Ø§Ù„Ø³Ø¤Ø§Ù„ Q7: Ø§Ø®ØªÙŠØ§Ø±Ø§Øª ÙƒØ«ÙŠØ±Ø© (6)ØŒ Ù‚Ø¯ ØªØ´ÙŠØ± Ø¥Ù„Ù‰ Ø§Ù„ØªØ±Ø¯Ø¯\n",
            "   â€¢ Ø§Ø®ØªÙŠØ§Ø±Ø§Øª Ø¥Ø¬Ù…Ø§Ù„ÙŠØ© ÙƒØ«ÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹ (65)ØŒ Ù‚Ø¯ ØªÙƒÙˆÙ† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª ØºÙŠØ± Ø¯Ù‚ÙŠÙ‚Ø©\n",
            "   â€¢ ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø£Ù†Ù…Ø§Ø· Ù…ØªÙ†Ø§Ù‚Ø¶Ø©: Ø§Ù„Ù…ØªØ­ÙƒÙ…-Ø§Ù„Ù…Ø¹ØªÙ…Ø¯, Ø§Ù„Ø¨Ø§Ø±Ø¯-Ø§Ù„Ù…Ø±Ù‡Ù‚\n",
            "   â€¢ Ø£Ø³Ø¦Ù„Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ù…Ø¹ ØªØ­Ø¯ÙŠØ¯ Ù…Ø¹Ø¸Ù… Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª: Q1, Q3, Q5, Q6, Q7, Q9, Q10, Q11, Q12\n",
            "âœ… Ø§Ù„Ø«Ù‚Ø© ÙˆØ§Ù‚Ø¹ÙŠØ© Ù„Ù†Ù…Ø· mixed\n",
            "\n",
            "================================================================================\n",
            "ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡\n",
            "================================================================================\n",
            "\n",
            "ğŸ¯ Ù…Ù„Ø®Øµ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:\n",
            "   Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª: 5\n",
            "   Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù†Ø§Ø¬Ø­Ø©: 5\n",
            "   Ø§Ù„Ø«Ù‚Ø© Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠØ©: 5/5 (100.0%)\n",
            "\n",
            "ğŸ” ØªÙˆØ²ÙŠØ¹ Ù‚ÙˆØ© Ø§Ù„Ø£Ø¯Ù„Ø©:\n",
            "   VERY_STRONG    : 3 Ø§Ø®ØªØ¨Ø§Ø± (60.0%)\n",
            "   STRONG         : 2 Ø§Ø®ØªØ¨Ø§Ø± (40.0%)\n",
            "\n",
            "ğŸ“Š ØªÙˆØ²ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ù†Ù…Ø§Ø·:\n",
            "   VERY_CLEAR     : 3 Ø§Ø®ØªØ¨Ø§Ø± (60.0%)\n",
            "   MIXED          : 2 Ø§Ø®ØªØ¨Ø§Ø± (40.0%)\n",
            "\n",
            "ğŸ“ˆ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø«Ù‚Ø© Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·:\n",
            "   VERY_CLEAR     : 85.9% (Ø§Ù„Ù…Ø¯Ù‰: 85.0%-87.6%)\n",
            "   MIXED          : 54.4% (Ø§Ù„Ù…Ø¯Ù‰: 50.6%-58.2%)\n",
            "\n",
            "ğŸ† Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¹Ø§Ù…:\n",
            "   âœ… Ù…Ù…ØªØ§Ø²: Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ø«Ù‚Ø© ØªØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ø¬ÙŠØ¯\n",
            "   ğŸš€ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¥Ù†ØªØ§Ø¬ Ø¨Ø«Ù‚Ø© ÙˆØ§Ù‚Ø¹ÙŠØ©\n",
            "\n",
            "ğŸ’¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª:\n",
            "   1. Ù…Ø±Ø§Ù‚Ø¨Ø© Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ø«Ù‚Ø© ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬\n",
            "   2. Ø¬Ù…Ø¹ Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø­ÙˆÙ„ Ø¯Ù‚Ø© Ø§Ù„ØªÙ†Ø¨Ø¤\n",
            "   3. Ø¶Ø¨Ø· Ù†Ø·Ø§Ù‚Ø§Øª Ø§Ù„Ø«Ù‚Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙØ¹Ù„ÙŠ\n",
            "   4. Ø§Ù„ØªÙÙƒÙŠØ± ÙÙŠ Ø§Ø®ØªØ¨Ø§Ø± A/B Ù„Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ù…Ø®ØªÙ„ÙØ©\n",
            "\n",
            "================================================================================\n",
            "ğŸ‘¤ Ø§Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠ: Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ù…Ø³ØªØ®Ø¯Ù… Ø¹Ø´ÙˆØ§Ø¦ÙŠ\n",
            "================================================================================\n",
            "\n",
            "Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªØ¹Ø±ÙŠÙ Ù…Ø³ØªØ®Ø¯Ù… Ø¹Ø´ÙˆØ§Ø¦ÙŠ...\n",
            "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡ÙŠÙ…Ù† Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ (Ù„Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠ): Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\n",
            "\n",
            "ØªØ´ØºÙŠÙ„ Ø§Ù„ØªÙ†Ø¨Ø¤...\n",
            "----------------------------------------\n",
            "ğŸ¯ Ø§Ù„ØªÙ†Ø¨Ø¤:\n",
            "   Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø£ÙˆÙ„Ù‰: Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\n",
            "   Ø§Ù„Ø«Ù‚Ø©: 85.0% (Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©)\n",
            "   Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·: VERY_CLEAR\n",
            "   Ø§Ù„ÙŠÙ‚ÙŠÙ†: ÙŠÙ‚ÙŠÙ† Ø¹Ø§Ù„ÙŠ Ø¬Ø¯Ø§Ù‹\n",
            "   Ø£ÙØ¶Ù„ 3: Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ, Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…, Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\n",
            "\n",
            "ğŸ” Ø§Ù„ØªØ­Ù„ÙŠÙ„:\n",
            "   Ù‚ÙˆØ© Ø§Ù„Ø£Ø¯Ù„Ø©: VERY_STRONG\n",
            "   ÙˆØ¶ÙˆØ­ Ø§Ù„Ù†Ù…Ø·: 1.00\n",
            "   Ø¯Ø±Ø¬Ø© Ø§Ù„ØºÙ…ÙˆØ¶: 0.20\n",
            "\n",
            "âœ… ØµØ­ÙŠØ­: ØªÙ… Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡ÙŠÙ…Ù† Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ!\n",
            "\n",
            "ğŸ“Š Ø£ÙØ¶Ù„ 5 Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª:\n",
            "   Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ             : 46.8%\n",
            "   Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…            : 20.0%\n",
            "   Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„          : 14.7%\n",
            "   Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø¯        : 4.0%\n",
            "   Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ØºÙŠÙˆØ±        : 3.9%\n",
            "\n",
            "================================================================================\n",
            "âœ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠ Ø§Ù„Ø«Ø§Ø¨Øª Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…ÙƒØªÙ…Ù„\n",
            "================================================================================\n",
            "\n",
            "Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„Ù…Ù†ÙØ°Ø©:\n",
            "âœ… Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù„ÙØ¦Ø§Øª Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù€ 18 Ø´Ø®ØµÙŠØ©\n",
            "âœ… Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ø«Ù‚Ø© Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠØ© Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·\n",
            "âœ… ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚ÙˆØ© Ø§Ù„Ù‚Ø§Ø¦Ù… Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¯Ù„Ø©\n",
            "âœ… Ù†Ø¸Ø§Ù… ØªØ¯Ù‚ÙŠÙ‚ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª ÙˆØ§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª\n",
            "âœ… ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù†ÙØ³ÙŠØ©\n",
            "âœ… Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¥Ù†ØªØ§Ø¬ Ù…Ø¹ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù„Ø£Ø®Ø·Ø§Ø¡\n",
            "\n",
            "ğŸš€ Ø¬Ø§Ù‡Ø² Ù„Ù„Ù†Ø´Ø±!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import random\n",
        "import time\n",
        "from typing import Dict, List, Tuple, Any, Optional\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ğŸš€ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¬Ø§Ù‡Ø² Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù„Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø´Ø®ØµÙŠØ§Øª (Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ==================== 1. ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ====================\n",
        "print(\"\\n1. ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©...\")\n",
        "df = pd.read_csv('ana_dataset_top3_ar.csv')\n",
        "print(f\"   âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {len(df):,} Ø¹ÙŠÙ†Ø©\")\n",
        "\n",
        "# ==================== 2. ØªØ¹Ø±ÙŠÙ Ø§Ù„Ø´Ø®ØµÙŠØ§Øª Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ====================\n",
        "CHARACTERS_AR = [\n",
        "    # ğŸ›¡ï¸ Ø§Ù„Ù…Ø¯ÙŠØ±ÙŠÙ† (Ø§Ù„Ø­Ù…Ø§Ø© Ø§Ù„Ø§Ø³ØªØ¨Ø§Ù‚ÙŠÙŠÙ†)\n",
        "    \"Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ\", \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\", \"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³\", \"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\",\n",
        "    \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø¯\", \"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø­ÙŠØ±Ø§Ù†\",\n",
        "    # ğŸ”¥ Ø±Ø¬Ø§Ù„ Ø§Ù„Ø¥Ø·ÙØ§Ø¡ (Ø§Ù„Ø­Ù…Ø§Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ÙŠÙŠÙ†)\n",
        "    \"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\", \"Ø§Ù„Ø¢ÙƒÙ„/Ø§Ù„Ù†Ù‡Ù…\", \"Ø§Ù„Ù„Ø§Ø¹Ø¨ Ø§Ù„Ù…ÙØ±Ø·\",\n",
        "    # ğŸ˜¥ Ø§Ù„Ù…Ù†ÙÙŠÙŠÙ† (Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ Ø§Ù„Ù…Ø¬Ø±ÙˆØ­Ø©)\n",
        "    \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§ÙŠÙ\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡Ù…ÙÙ„\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø¬ÙˆÙ„\",\n",
        "    \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ØºÙŠÙˆØ±\", \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\"\n",
        "]\n",
        "\n",
        "MANAGERS_AR = [\"Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ\", \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\", \"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³\", \"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\",\n",
        "               \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø¯\", \"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø­ÙŠØ±Ø§Ù†\"]\n",
        "\n",
        "FIREFIGHTERS_AR = [\"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\", \"Ø§Ù„Ø¢ÙƒÙ„/Ø§Ù„Ù†Ù‡Ù…\", \"Ø§Ù„Ù„Ø§Ø¹Ø¨ Ø§Ù„Ù…ÙØ±Ø·\"]\n",
        "\n",
        "EXILES_AR = [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§ÙŠÙ\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡Ù…ÙÙ„\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø¬ÙˆÙ„\",\n",
        "             \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ØºÙŠÙˆØ±\", \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\"]\n",
        "\n",
        "# ==================== 3. ØªÙˆÙ‚ÙŠØ¹Ø§Øª Ø§Ù„Ø´Ø®ØµÙŠØ§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© (Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©) ====================\n",
        "CHARACTER_SIGNATURES_AR = {\n",
        "    \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\": {\n",
        "        \"must_have\": [\"Q1:0\", \"Q2:high\", \"Q3:0\"],\n",
        "        \"often_have\": [\"Q5:0\", \"Q7:0\", \"Q10:1\", \"Q13:2\", \"Q12:0\"],\n",
        "        \"contradicts\": [\"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\", \"Ø§Ù„Ù„Ø§Ø¹Ø¨ Ø§Ù„Ù…ÙØ±Ø·\", \"Ø§Ù„Ø¢ÙƒÙ„/Ø§Ù„Ù†Ù‡Ù…\"],\n",
        "        \"confidence_range\": (0.80, 0.95),\n",
        "        \"evidence_weight\": 1.4\n",
        "    },\n",
        "    \"Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ\": {\n",
        "        \"must_have\": [\"Q3:3\", \"Q11:0\"],\n",
        "        \"often_have\": [\"Q2:high\", \"Q5:0\", \"Q7:5\", \"Q13:5\", \"Q6:5\"],\n",
        "        \"contradicts\": [\"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\"],\n",
        "        \"confidence_range\": (0.75, 0.90),\n",
        "        \"evidence_weight\": 1.3\n",
        "    },\n",
        "    \"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³\": {\n",
        "        \"must_have\": [\"Q1:2\", \"Q10:0\"],\n",
        "        \"often_have\": [\"Q7:3\", \"Q9:0\", \"Q13:3\", \"Q5:2\", \"Q11:4\"],\n",
        "        \"contradicts\": [\"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø¯\"],\n",
        "        \"confidence_range\": (0.75, 0.90),\n",
        "        \"evidence_weight\": 1.3\n",
        "    },\n",
        "    \"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\": {\n",
        "        \"must_have\": [\"Q1:0\", \"Q3:0\"],\n",
        "        \"often_have\": [\"Q7:0\", \"Q10:1\", \"Q13:2\", \"Q12:0\"],\n",
        "        \"contradicts\": [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\", \"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\"],\n",
        "        \"confidence_range\": (0.70, 0.85),\n",
        "        \"evidence_weight\": 1.2\n",
        "    },\n",
        "    \"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\": {\n",
        "        \"must_have\": [\"Q1:3\", \"Q8:high\"],\n",
        "        \"often_have\": [\"Q3:1\", \"Q7:4\", \"Q13:4\", \"Q5:3\"],\n",
        "        \"contradicts\": [\"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\", \"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\", \"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\"],\n",
        "        \"confidence_range\": (0.70, 0.85),\n",
        "        \"evidence_weight\": 1.2\n",
        "    },\n",
        "    \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯\": {\n",
        "        \"must_have\": [\"Q4:high\", \"Q11:1\"],\n",
        "        \"often_have\": [\"Q1:5\", \"Q12:3\", \"Q13:0\", \"Q9:4\", \"Q6:3\"],\n",
        "        \"contradicts\": [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø¯\", \"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\"],\n",
        "        \"confidence_range\": (0.80, 0.95),\n",
        "        \"evidence_weight\": 1.4\n",
        "    },\n",
        "    \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§ÙŠÙ\": {\n",
        "        \"must_have\": [\"Q6:1\", \"Q11:2\"],\n",
        "        \"often_have\": [\"Q9:1\", \"Q13:0\", \"Q12:2\", \"Q3:2\"],\n",
        "        \"contradicts\": [\"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø¯\"],\n",
        "        \"confidence_range\": (0.70, 0.85),\n",
        "        \"evidence_weight\": 1.1\n",
        "    },\n",
        "    \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚\": {\n",
        "        \"must_have\": [\"Q6:4\", \"Q13:0\"],\n",
        "        \"often_have\": [\"Q11:2\", \"Q12:4\", \"Q5:4\", \"Q9:5\"],\n",
        "        \"contradicts\": [\"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø¯\"],\n",
        "        \"confidence_range\": (0.65, 0.80),\n",
        "        \"evidence_weight\": 1.1\n",
        "    },\n",
        "    \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\": {\n",
        "        \"must_have\": [\"Q9:5\", \"Q12:4\"],\n",
        "        \"often_have\": [\"Q6:1\", \"Q10:0\", \"Q13:3\", \"Q7:3\"],\n",
        "        \"contradicts\": [\"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø¯\", \"Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ\"],\n",
        "        \"confidence_range\": (0.65, 0.80),\n",
        "        \"evidence_weight\": 1.0\n",
        "    },\n",
        "    \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø¬ÙˆÙ„\": {\n",
        "        \"must_have\": [\"Q11:0\", \"Q6:5\"],\n",
        "        \"often_have\": [\"Q9:2\", \"Q10:5\", \"Q13:1\", \"Q7:5\"],\n",
        "        \"contradicts\": [\"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³\", \"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\"],\n",
        "        \"confidence_range\": (0.70, 0.85),\n",
        "        \"evidence_weight\": 1.1\n",
        "    },\n",
        "    \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡Ù…ÙÙ„\": {\n",
        "        \"must_have\": [\"Q3:4\", \"Q11:3\"],\n",
        "        \"often_have\": [\"Q6:3\", \"Q4:high\", \"Q13:7\", \"Q1:5\"],\n",
        "        \"contradicts\": [\"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\", \"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³\"],\n",
        "        \"confidence_range\": (0.70, 0.85),\n",
        "        \"evidence_weight\": 1.1\n",
        "    },\n",
        "    \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø­ÙŠØ±Ø§Ù†\": {\n",
        "        \"must_have\": [\"Q1:5\", \"Q5:5\"],\n",
        "        \"often_have\": [\"Q3:4\", \"Q11:5\", \"Q13:4\", \"Q12:5\"],\n",
        "        \"contradicts\": [\"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\", \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\"],\n",
        "        \"confidence_range\": (0.50, 0.70),\n",
        "        \"evidence_weight\": 0.9\n",
        "    },\n",
        "    \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø¯\": {\n",
        "        \"must_have\": [\"Q1:4\", \"Q7:2\"],\n",
        "        \"often_have\": [\"Q3:3\", \"Q13:7\", \"Q10:1\"],\n",
        "        \"contradicts\": [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚\", \"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³\"],\n",
        "        \"confidence_range\": (0.65, 0.80),\n",
        "        \"evidence_weight\": 1.0\n",
        "    },\n",
        "    \"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\": {\n",
        "        \"must_have\": [\"Q1:1\", \"Q7:0\"],\n",
        "        \"often_have\": [\"Q12:0\", \"Q2:high\", \"Q10:1\"],\n",
        "        \"contradicts\": [\"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\", \"Ø§Ù„Ù„Ø§Ø¹Ø¨ Ø§Ù„Ù…ÙØ±Ø·\", \"Ø§Ù„Ø¢ÙƒÙ„/Ø§Ù„Ù†Ù‡Ù…\"],\n",
        "        \"confidence_range\": (0.70, 0.85),\n",
        "        \"evidence_weight\": 1.2\n",
        "    },\n",
        "    \"Ø§Ù„Ø¢ÙƒÙ„/Ø§Ù„Ù†Ù‡Ù…\": {\n",
        "        \"must_have\": [\"Q7:1\", \"Q8:high\"],\n",
        "        \"often_have\": [\"Q3:1\", \"Q13:1\", \"Q11:4\"],\n",
        "        \"contradicts\": [\"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\", \"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\"],\n",
        "        \"confidence_range\": (0.65, 0.80),\n",
        "        \"evidence_weight\": 1.0\n",
        "    },\n",
        "    \"Ø§Ù„Ù„Ø§Ø¹Ø¨ Ø§Ù„Ù…ÙØ±Ø·\": {\n",
        "        \"must_have\": [\"Q7:4\", \"Q8:high\"],\n",
        "        \"often_have\": [\"Q1:3\", \"Q13:4\", \"Q3:1\"],\n",
        "        \"contradicts\": [\"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\", \"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\", \"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\"],\n",
        "        \"confidence_range\": (0.65, 0.80),\n",
        "        \"evidence_weight\": 1.0\n",
        "    },\n",
        "    \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\": {\n",
        "        \"must_have\": [\"Q3:2\", \"Q6:5\"],\n",
        "        \"often_have\": [\"Q9:4\", \"Q12:1\", \"Q11:0\"],\n",
        "        \"contradicts\": [\"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø¯\"],\n",
        "        \"confidence_range\": (0.70, 0.85),\n",
        "        \"evidence_weight\": 1.1\n",
        "    },\n",
        "    \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ØºÙŠÙˆØ±\": {\n",
        "        \"must_have\": [\"Q10:2\", \"Q13:1\"],\n",
        "        \"often_have\": [\"Q9:2\", \"Q10:3\", \"Q11:0\"],\n",
        "        \"contradicts\": [\"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\"],\n",
        "        \"confidence_range\": (0.60, 0.75),\n",
        "        \"evidence_weight\": 0.8\n",
        "    }\n",
        "}\n",
        "\n",
        "# ==================== 4. Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ø´Ø®ØµÙŠØ§Øª (Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©) ====================\n",
        "CHARACTER_RELATIONSHIPS_AR = {\n",
        "    \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\": {\n",
        "        \"strong_relations\": [\"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\", \"Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ\", \"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\"],\n",
        "        \"common_companions\": [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø¬ÙˆÙ„\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§ÙŠÙ\"],\n",
        "        \"rare_with\": [\"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\", \"Ø§Ù„Ù„Ø§Ø¹Ø¨ Ø§Ù„Ù…ÙØ±Ø·\", \"Ø§Ù„Ø¢ÙƒÙ„/Ø§Ù„Ù†Ù‡Ù…\"]\n",
        "    },\n",
        "    \"Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ\": {\n",
        "        \"strong_relations\": [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø¬ÙˆÙ„\", \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§ÙŠÙ\"],\n",
        "        \"common_companions\": [\"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\", \"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\"],\n",
        "        \"rare_with\": [\"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\"]\n",
        "    },\n",
        "    \"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³\": {\n",
        "        \"strong_relations\": [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§ÙŠÙ\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚\"],\n",
        "        \"common_companions\": [\"Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø¬ÙˆÙ„\"],\n",
        "        \"rare_with\": [\"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø¯\", \"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\"]\n",
        "    },\n",
        "    \"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\": {\n",
        "        \"strong_relations\": [\"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\", \"Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø¯\"],\n",
        "        \"common_companions\": [\"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§ÙŠÙ\"],\n",
        "        \"rare_with\": [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\", \"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø­ÙŠØ±Ø§Ù†\"]\n",
        "    },\n",
        "    \"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\": {\n",
        "        \"strong_relations\": [\"Ø§Ù„Ù„Ø§Ø¹Ø¨ Ø§Ù„Ù…ÙØ±Ø·\", \"Ø§Ù„Ø¢ÙƒÙ„/Ø§Ù„Ù†Ù‡Ù…\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§ÙŠÙ\"],\n",
        "        \"common_companions\": [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø­ÙŠØ±Ø§Ù†\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚\"],\n",
        "        \"rare_with\": [\"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\", \"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\", \"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\"]\n",
        "    },\n",
        "    \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯\": {\n",
        "        \"strong_relations\": [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡Ù…ÙÙ„\", \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§ÙŠÙ\"],\n",
        "        \"common_companions\": [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\"],\n",
        "        \"rare_with\": [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø¯\", \"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\", \"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\"]\n",
        "    },\n",
        "    \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§ÙŠÙ\": {\n",
        "        \"strong_relations\": [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯\"],\n",
        "        \"common_companions\": [\"Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø¬ÙˆÙ„\"],\n",
        "        \"rare_with\": [\"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø¯\", \"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\"]\n",
        "    },\n",
        "    \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚\": {\n",
        "        \"strong_relations\": [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§ÙŠÙ\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø­ÙŠØ±Ø§Ù†\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡Ù…ÙÙ„\"],\n",
        "        \"common_companions\": [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯\"],\n",
        "        \"rare_with\": [\"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø¯\", \"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\"]\n",
        "    },\n",
        "    \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\": {\n",
        "        \"strong_relations\": [\"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§ÙŠÙ\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚\"],\n",
        "        \"common_companions\": [\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡Ù…ÙÙ„\"],\n",
        "        \"rare_with\": [\"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø¯\", \"Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ\"]\n",
        "    }\n",
        "}\n",
        "\n",
        "# ==================== 5. Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø© (Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©) ====================\n",
        "print(\"\\n2. Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©...\")\n",
        "\n",
        "class AdvancedFeatureEngineerAr:\n",
        "    \"\"\"ÙŠÙÙ†Ø´Ø¦ Ù…ÙŠØ²Ø§Øª Ø°Ø§Øª Ù…Ø¹Ù†Ù‰ Ù†ÙØ³ÙŠ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.slider_map = {'0-20%': 0.1, '21-50%': 0.35, '51-80%': 0.65, '81-100%': 0.9}\n",
        "        self.characters = CHARACTERS_AR\n",
        "\n",
        "    def create_features(self, df):\n",
        "        \"\"\"Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ù…Ù† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ø£ÙˆÙ„ÙŠØ©\"\"\"\n",
        "        features = pd.DataFrame(index=df.index)\n",
        "\n",
        "        # 1. Ø§Ù„ØªØ­ÙˆÙŠÙ„Ø§Øª Ø§Ù„Ø¹Ø¯Ø¯ÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©\n",
        "        for q in ['Q2', 'Q4', 'Q8']:\n",
        "            if q in df.columns:\n",
        "                features[f'{q}_num'] = df[q].map(self.slider_map).fillna(0.5)\n",
        "\n",
        "        # 2. Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø¹Ø¯ Ù…Ø¹ Ù…Ø¹Ù†Ù‰ Ù†ÙØ³ÙŠ\n",
        "        for q in ['Q1', 'Q3', 'Q5', 'Q6', 'Q7', 'Q9', 'Q10', 'Q11', 'Q12', 'Q13']:\n",
        "            if q in df.columns:\n",
        "                features[f'{q}_count'] = df[q].apply(lambda x: len(str(x).split(',')) if pd.notna(x) else 0)\n",
        "\n",
        "        # 3. Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ù†ÙØ³ÙŠØ©\n",
        "        # Ø§Ù„Ø¨Ø¯Ø¡ Ø¨Ù‚ÙŠÙ… ØµÙØ±ÙŠØ© Ø£ÙˆÙ„Ø§Ù‹\n",
        "        features['perfectionism_score'] = 0\n",
        "        features['loneliness_score'] = 0\n",
        "        features['escapism_score'] = 0\n",
        "        features['self_criticism_score'] = 0\n",
        "        features['social_focus_score'] = 0\n",
        "        features['control_score'] = 0\n",
        "        features['vulnerability_score'] = 0\n",
        "\n",
        "        # 4. Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
        "        key_options = {\n",
        "            'Q1': ['0', '2', '3', '5'],\n",
        "            'Q3': ['0', '1', '2', '3', '4'],\n",
        "            'Q5': ['0', '1', '2', '3', '4', '5'],\n",
        "            'Q7': ['0', '1', '3', '4', '5'],\n",
        "            'Q10': ['0', '1', '2', '3', '4'],\n",
        "            'Q11': ['0', '1', '2', '3', '4', '5'],\n",
        "            'Q12': ['0', '1', '2', '3', '4', '5'],\n",
        "            'Q13': ['0', '1', '2', '4', '5', '7']\n",
        "        }\n",
        "\n",
        "        for q, options in key_options.items():\n",
        "            if q in df.columns:\n",
        "                for option in options:\n",
        "                    col_name = f'{q}_opt_{option}'\n",
        "                    features[col_name] = df[q].apply(\n",
        "                        lambda x: 1 if option in str(x).split(',') else 0\n",
        "                    )\n",
        "\n",
        "        # 5. Ù…Ø¤Ø´Ø±Ø§Øª ÙˆØ¶ÙˆØ­ Ø§Ù„Ø£Ù†Ù…Ø§Ø·\n",
        "        features['clear_perfectionist'] = (\n",
        "            (features['Q2_num'] > 0.8) &\n",
        "            (features.get('Q1_opt_0', 0) == 1) &\n",
        "            (features.get('Q3_opt_0', 0) == 1)\n",
        "        ).astype(int)\n",
        "\n",
        "        features['clear_people_pleaser'] = (\n",
        "            (features.get('Q1_opt_2', 0) == 1) &\n",
        "            (features.get('Q10_opt_0', 0) == 1) &\n",
        "            (features.get('Q7_opt_3', 0) == 1)\n",
        "        ).astype(int)\n",
        "\n",
        "        features['clear_procrastinator'] = (\n",
        "            (features['Q8_num'] > 0.8) &\n",
        "            (features.get('Q1_opt_3', 0) == 1) &\n",
        "            (features.get('Q7_opt_4', 0) == 1)\n",
        "        ).astype(int)\n",
        "\n",
        "        features['clear_lonely'] = (\n",
        "            (features['Q4_num'] > 0.8) &\n",
        "            (features.get('Q11_opt_1', 0) == 1) &\n",
        "            (features.get('Q12_opt_3', 0) == 1)\n",
        "        ).astype(int)\n",
        "\n",
        "        features['clear_inner_critic'] = (\n",
        "            (features.get('Q3_opt_3', 0) == 1) &\n",
        "            (features.get('Q11_opt_0', 0) == 1) &\n",
        "            (features.get('Q7_opt_5', 0) == 1)\n",
        "        ).astype(int)\n",
        "\n",
        "        # 6. Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ø±Ø¬Ø§Øª Ø§Ù„Ù†ÙØ³ÙŠØ© Ø¨Ø¹Ø¯ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª\n",
        "        features['perfectionism_score'] = (\n",
        "            features['Q2_num'].fillna(0) * 0.4 +\n",
        "            features.get('Q1_opt_0', 0) * 0.3 +\n",
        "            features.get('Q3_opt_0', 0) * 0.3\n",
        "        )\n",
        "\n",
        "        features['loneliness_score'] = (\n",
        "            features['Q4_num'].fillna(0) * 0.5 +\n",
        "            features.get('Q11_opt_1', 0) * 0.3 +\n",
        "            features.get('Q12_opt_3', 0) * 0.2\n",
        "        )\n",
        "\n",
        "        features['escapism_score'] = (\n",
        "            features['Q8_num'].fillna(0) * 0.5 +\n",
        "            features.get('Q1_opt_3', 0) * 0.2 +\n",
        "            features.get('Q7_opt_4', 0) * 0.2 +\n",
        "            features.get('Q7_opt_1', 0) * 0.1\n",
        "        )\n",
        "\n",
        "        features['self_criticism_score'] = (\n",
        "            features.get('Q11_opt_0', 0) * 0.4 +\n",
        "            features.get('Q7_opt_5', 0) * 0.3 +\n",
        "            features.get('Q3_opt_3', 0) * 0.3\n",
        "        )\n",
        "\n",
        "        features['social_focus_score'] = (\n",
        "            features.get('Q1_opt_2', 0) * 0.4 +\n",
        "            features.get('Q10_opt_0', 0) * 0.3 +\n",
        "            features.get('Q7_opt_3', 0) * 0.3\n",
        "        )\n",
        "\n",
        "        features['control_score'] = (\n",
        "            features.get('Q1_opt_0', 0) * 0.4 +\n",
        "            features.get('Q7_opt_0', 0) * 0.3 +\n",
        "            features.get('Q10_opt_1', 0) * 0.3\n",
        "        )\n",
        "\n",
        "        features['vulnerability_score'] = (\n",
        "            features.get('Q3_opt_2', 0) * 0.3 +\n",
        "            features.get('Q6_opt_1', 0) * 0.3 +\n",
        "            features.get('Q9_opt_4', 0) * 0.2 +\n",
        "            features.get('Q13_opt_0', 0) * 0.2\n",
        "        )\n",
        "\n",
        "        # 6. Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØºÙ…ÙˆØ¶ ÙˆØ§Ù„ÙˆØ¶ÙˆØ­\n",
        "        clear_pattern_cols = [c for c in features.columns if c.startswith('clear_')]\n",
        "        if clear_pattern_cols:\n",
        "            features['clear_pattern_count'] = features[clear_pattern_cols].sum(axis=1)\n",
        "            features['has_clear_pattern'] = (features['clear_pattern_count'] > 0).astype(int)\n",
        "            features['has_multiple_clear'] = (features['clear_pattern_count'] > 1).astype(int)\n",
        "\n",
        "        # 7. Ù…Ù‚Ø§ÙŠÙŠØ³ ØªÙ†Ø§Ø³Ù‚ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø§Øª\n",
        "        slider_cols = [c for c in features.columns if c.endswith('_num')]\n",
        "        if len(slider_cols) > 1:\n",
        "            features['slider_consistency'] = 1 - features[slider_cols].std(axis=1).fillna(0)\n",
        "\n",
        "        count_cols = [c for c in features.columns if c.endswith('_count')]\n",
        "        if len(count_cols) > 1:\n",
        "            features['selection_consistency'] = 1 - (features[count_cols].std(axis=1) / 3).fillna(0)\n",
        "\n",
        "        # 8. Ù‡ÙŠÙ…Ù†Ø© Ø§Ù„Ø£Ù†Ù…Ø§Ø·\n",
        "        manager_indicators = features.get('clear_perfectionist', 0) + \\\n",
        "                           features.get('clear_people_pleaser', 0) + \\\n",
        "                           features.get('clear_inner_critic', 0) + \\\n",
        "                           features.get('Q1_opt_0', 0)\n",
        "\n",
        "        firefighter_indicators = features.get('clear_procrastinator', 0) + \\\n",
        "                                (features['Q8_num'] > 0.7).astype(int)\n",
        "\n",
        "        exile_indicators = features.get('clear_lonely', 0) + \\\n",
        "                          (features['Q4_num'] > 0.7).astype(int)\n",
        "\n",
        "        total_indicators = manager_indicators + firefighter_indicators + exile_indicators\n",
        "        features['archetype_clarity'] = np.where(\n",
        "            total_indicators > 0,\n",
        "            np.maximum(manager_indicators, np.maximum(firefighter_indicators, exile_indicators)) / total_indicators,\n",
        "            0.5\n",
        "        )\n",
        "\n",
        "        # 9. Ø¯Ø±Ø¬Ø© Ø§Ù„ØºÙ…ÙˆØ¶ Ø§Ù„ÙƒÙ„ÙŠ (0 = ÙˆØ§Ø¶Ø­ Ø¬Ø¯Ø§Ù‹ØŒ 1 = ØºØ§Ù…Ø¶ Ø¬Ø¯Ø§Ù‹)\n",
        "        features['total_ambiguity'] = (\n",
        "            (features.get('clear_pattern_count', 0) == 0).astype(float) * 0.3 +\n",
        "            features.get('has_multiple_clear', 0).astype(float) * 0.3 +\n",
        "            (features.get('slider_consistency', 0.5) < 0.7).astype(float) * 0.2 +\n",
        "            (features.get('selection_consistency', 0.5) < 0.6).astype(float) * 0.2\n",
        "        )\n",
        "\n",
        "        # 10. Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ØªÙˆØªØ± Ø§Ù„Ù†ÙØ³ÙŠ\n",
        "        features['perfection_vs_procrastination'] = (\n",
        "            features['perfectionism_score'] * features['escapism_score']\n",
        "        )\n",
        "\n",
        "        features['control_vs_vulnerability'] = (\n",
        "            features['control_score'] * features['vulnerability_score']\n",
        "        )\n",
        "\n",
        "        features['inner_conflict_score'] = (\n",
        "            features['perfection_vs_procrastination'] * 0.4 +\n",
        "            features['control_vs_vulnerability'] * 0.3 +\n",
        "            features['total_ambiguity'] * 0.3\n",
        "        )\n",
        "\n",
        "        # ØªØ¹Ø¨Ø¦Ø© Ø§Ù„Ù‚ÙŠÙ… NaN\n",
        "        features = features.fillna(0)\n",
        "\n",
        "        # Ù‚Øµ Ø§Ù„Ù‚ÙŠÙ… Ø¥Ù„Ù‰ Ù†Ø·Ø§Ù‚Ø§Øª Ù…Ø¹Ù‚ÙˆÙ„Ø©\n",
        "        for col in features.columns:\n",
        "            if features[col].dtype in ['float64', 'float32']:\n",
        "                features[col] = np.clip(features[col], 0, 1)\n",
        "\n",
        "        return features\n",
        "\n",
        "# ==================== 6. Ù…ØµÙ†Ù Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ù†Ù…Ø§Ø· (Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©) ====================\n",
        "class PatternTypeClassifierAr:\n",
        "    \"\"\"ÙŠØµÙ†Ù Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¥Ù„Ù‰ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù†ÙØ³ÙŠØ© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.pattern_types = ['very_clear', 'clear', 'mixed', 'ambiguous', 'very_mixed', 'contradictory']\n",
        "\n",
        "    def classify(self, features: pd.Series) -> str:\n",
        "        \"\"\"ØªØµÙ†ÙŠÙ Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø· Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙŠØ²Ø§Øª\"\"\"\n",
        "\n",
        "        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©\n",
        "        clear_count = features.get('clear_pattern_count', 0)\n",
        "        ambiguity = features.get('total_ambiguity', 0.5)\n",
        "        conflict = features.get('inner_conflict_score', 0)\n",
        "        clarity = features.get('archetype_clarity', 0.5)\n",
        "\n",
        "        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ØªÙ†Ø§Ù‚Ø¶Ø© Ø£ÙˆÙ„Ø§Ù‹\n",
        "        if conflict > 0.7:\n",
        "            return 'contradictory'\n",
        "\n",
        "        # ÙˆØ§Ø¶Ø­ Ø¬Ø¯Ø§Ù‹: Ù†Ù…Ø· ÙˆØ§Ø­Ø¯ Ù…Ù‡ÙŠÙ…Ù† Ù…Ø¹ ØºÙ…ÙˆØ¶ Ù…Ù†Ø®ÙØ¶\n",
        "        if clear_count == 1 and ambiguity < 0.3 and clarity > 0.8:\n",
        "            return 'very_clear'\n",
        "\n",
        "        # ÙˆØ§Ø¶Ø­: Ù†Ù…Ø· ÙˆØ§Ø¶Ø­ Ù„ÙƒÙ† Ù…Ø¹ Ø¨Ø¹Ø¶ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯\n",
        "        if clear_count == 1 and ambiguity < 0.4 and clarity > 0.6:\n",
        "            return 'clear'\n",
        "\n",
        "        # Ù…Ø®ØªÙ„Ø·: Ø£Ù†Ù…Ø§Ø· ÙˆØ§Ø¶Ø­Ø© Ù…ØªØ¹Ø¯Ø¯Ø©\n",
        "        if clear_count >= 2 and ambiguity < 0.6:\n",
        "            return 'mixed'\n",
        "\n",
        "        # Ù…Ø®ØªÙ„Ø· Ø¬Ø¯Ø§Ù‹: Ø£Ù†Ù…Ø§Ø· ÙƒØ«ÙŠØ±Ø© Ù…Ø¹ ØºÙ…ÙˆØ¶ Ø¹Ø§Ù„Ù\n",
        "        if clear_count >= 3 or ambiguity > 0.8:\n",
        "            return 'very_mixed'\n",
        "\n",
        "        # ØºØ§Ù…Ø¶: ØºÙŠØ± ÙˆØ§Ø¶Ø­ Ù„ÙƒÙ† Ù„ÙŠØ³ Ù…Ø®ØªÙ„Ø·Ø§Ù‹ Ø¨Ø´Ø¯Ø©\n",
        "        if clear_count == 0 and ambiguity > 0.4:\n",
        "            return 'ambiguous'\n",
        "\n",
        "        # Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù…Ø®ØªÙ„Ø·\n",
        "        return 'mixed'\n",
        "\n",
        "# ==================== 7. Ù…Ø¯Ù‚Ù‚ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª (Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©) ====================\n",
        "class AnswerValidatorAr:\n",
        "    \"\"\"ÙŠÙØ¯Ù‚Ù‚ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„Ù„ØªÙ†Ø§Ø³Ù‚ Ø§Ù„Ù†ÙØ³ÙŠ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.slider_map = {'0-20%': 0.1, '21-50%': 0.35, '51-80%': 0.65, '81-100%': 0.9}\n",
        "\n",
        "    def validate(self, user_answers: Dict) -> List[str]:\n",
        "        \"\"\"ØªØ¯Ù‚ÙŠÙ‚ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ØŒ Ø¥Ø±Ø¬Ø§Ø¹ Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª/Ø§Ù„Ù…Ø´ÙƒÙ„Ø§Øª\"\"\"\n",
        "        warnings = []\n",
        "\n",
        "        # 1. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙƒØ«Ø±Ø© Ø§Ù„Ø§Ø®ØªÙŠØ§Ø±Ø§Øª (Ø±Ø¨Ù…Ø§ ØªØ´ÙŠØ± Ø¥Ù„Ù‰ Ø¶ÙˆØ¶Ø§Ø¡)\n",
        "        total_selections = 0\n",
        "        for q, val in user_answers.items():\n",
        "            if isinstance(val, str):\n",
        "                selections = len(val.split(','))\n",
        "                total_selections += selections\n",
        "\n",
        "                # ÙØ­ÙˆØµØ§Øª Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ÙØ±Ø¯ÙŠ\n",
        "                if selections > 3 and q in ['Q1', 'Q3', 'Q5', 'Q7']:\n",
        "                    warnings.append(f\"Ø§Ù„Ø³Ø¤Ø§Ù„ {q}: Ø§Ø®ØªÙŠØ§Ø±Ø§Øª ÙƒØ«ÙŠØ±Ø© ({selections})ØŒ Ù‚Ø¯ ØªØ´ÙŠØ± Ø¥Ù„Ù‰ Ø§Ù„ØªØ±Ø¯Ø¯\")\n",
        "\n",
        "        if total_selections > 30:\n",
        "            warnings.append(f\"Ø§Ø®ØªÙŠØ§Ø±Ø§Øª Ø¥Ø¬Ù…Ø§Ù„ÙŠØ© ÙƒØ«ÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹ ({total_selections})ØŒ Ù‚Ø¯ ØªÙƒÙˆÙ† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª ØºÙŠØ± Ø¯Ù‚ÙŠÙ‚Ø©\")\n",
        "\n",
        "        # 2. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØªÙ†Ø§Ø³Ù‚ Ø£Ø´Ø±Ø·Ø© Ø§Ù„ØªÙ…Ø±ÙŠØ±\n",
        "        slider_values = {}\n",
        "        for q in ['Q2', 'Q4', 'Q8']:\n",
        "            if q in user_answers:\n",
        "                slider_values[q] = self.slider_map.get(user_answers[q], 0.5)\n",
        "\n",
        "        if len(slider_values) == 3:\n",
        "            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¹Ø¯Ù… Ø§Ù„ØªÙ†Ø§Ø³Ù‚ Ø§Ù„Ø´Ø¯ÙŠØ¯\n",
        "            values = list(slider_values.values())\n",
        "            if max(values) - min(values) > 0.8:\n",
        "                warnings.append(\"Ø¹Ø¯Ù… ØªÙ†Ø§Ø³Ù‚ Ø´Ø¯ÙŠØ¯ ÙÙŠ Ø§Ø³ØªØ¬Ø§Ø¨Ø§Øª Ø£Ø´Ø±Ø·Ø© Ø§Ù„ØªÙ…Ø±ÙŠØ±\")\n",
        "\n",
        "        # 3. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ØªÙ†Ø§Ù‚Ø¶Ø©\n",
        "        contradictory_pairs = self._check_contradictions(user_answers)\n",
        "        if contradictory_pairs:\n",
        "            warnings.append(f\"ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø£Ù†Ù…Ø§Ø· Ù…ØªÙ†Ø§Ù‚Ø¶Ø©: {', '.join(contradictory_pairs)}\")\n",
        "\n",
        "        # 4. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù†Ù…Ø§Ø· \"Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ÙƒÙ„\"\n",
        "        all_selected_questions = []\n",
        "        for q in ['Q1', 'Q3', 'Q5', 'Q6', 'Q7', 'Q9', 'Q10', 'Q11', 'Q12']:\n",
        "            if q in user_answers:\n",
        "                if len(user_answers[q].split(',')) >= 5:  # Ù…Ø¹Ø¸Ù… Ø£Ùˆ ÙƒÙ„ Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ù…Ø­Ø¯Ø¯Ø©\n",
        "                    all_selected_questions.append(q)\n",
        "\n",
        "        if len(all_selected_questions) >= 3:\n",
        "            warnings.append(f\"Ø£Ø³Ø¦Ù„Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ù…Ø¹ ØªØ­Ø¯ÙŠØ¯ Ù…Ø¹Ø¸Ù… Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª: {', '.join(all_selected_questions)}\")\n",
        "\n",
        "        return warnings\n",
        "\n",
        "    def _check_contradictions(self, user_answers: Dict) -> List[str]:\n",
        "        \"\"\"Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ø§Ù„Ù†ÙØ³ÙŠØ© Ø§Ù„Ù…ØªÙ†Ø§Ù‚Ø¶Ø©\"\"\"\n",
        "        contradictions = []\n",
        "\n",
        "        # ØªÙ†Ø§Ù‚Ø¶ Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ + Ø§Ù„Ù…Ø¤Ø¬Ù„\n",
        "        if self._has_perfectionist_signature(user_answers) and self._has_procrastinator_signature(user_answers):\n",
        "            contradictions.append(\"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ-Ø§Ù„Ù…Ø¤Ø¬Ù„\")\n",
        "\n",
        "        # ØªÙ†Ø§Ù‚Ø¶ Ø§Ù„Ù…ØªØ­ÙƒÙ… + Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\n",
        "        if self._has_controller_signature(user_answers) and self._has_dependent_signature(user_answers):\n",
        "            contradictions.append(\"Ø§Ù„Ù…ØªØ­ÙƒÙ…-Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\")\n",
        "\n",
        "        # ØªÙ†Ø§Ù‚Ø¶ Ø§Ù„Ø¨Ø§Ø±Ø¯ + Ø§Ù„Ù…Ø±Ù‡Ù‚\n",
        "        if self._has_stoic_signature(user_answers) and self._has_overwhelmed_signature(user_answers):\n",
        "            contradictions.append(\"Ø§Ù„Ø¨Ø§Ø±Ø¯-Ø§Ù„Ù…Ø±Ù‡Ù‚\")\n",
        "\n",
        "        return contradictions\n",
        "\n",
        "    def _has_perfectionist_signature(self, answers):\n",
        "        q2_val = self.slider_map.get(answers.get('Q2', '21-50%'), 0.35)\n",
        "        q1_has_0 = '0' in str(answers.get('Q1', '')).split(',')\n",
        "        return q2_val > 0.8 and q1_has_0\n",
        "\n",
        "    def _has_procrastinator_signature(self, answers):\n",
        "        q8_val = self.slider_map.get(answers.get('Q8', '21-50%'), 0.35)\n",
        "        q1_has_3 = '3' in str(answers.get('Q1', '')).split(',')\n",
        "        return q8_val > 0.8 and q1_has_3\n",
        "\n",
        "    def _has_controller_signature(self, answers):\n",
        "        q1_has_0 = '0' in str(answers.get('Q1', '')).split(',')\n",
        "        q3_has_0 = '0' in str(answers.get('Q3', '')).split(',')\n",
        "        return q1_has_0 and q3_has_0\n",
        "\n",
        "    def _has_dependent_signature(self, answers):\n",
        "        q9_has_5 = '5' in str(answers.get('Q9', '')).split(',')\n",
        "        q12_has_4 = '4' in str(answers.get('Q12', '')).split(',')\n",
        "        return q9_has_5 or q12_has_4\n",
        "\n",
        "    def _has_stoic_signature(self, answers):\n",
        "        q1_has_4 = '4' in str(answers.get('Q1', '')).split(',')\n",
        "        q7_has_2 = '2' in str(answers.get('Q7', '')).split(',')\n",
        "        return q1_has_4 and q7_has_2\n",
        "\n",
        "    def _has_overwhelmed_signature(self, answers):\n",
        "        q6_has_4 = '4' in str(answers.get('Q6', '')).split(',')\n",
        "        q13_has_0 = '0' in str(answers.get('Q13', '')).split(',')\n",
        "        return q6_has_4 or q13_has_0\n",
        "\n",
        "# ==================== 8. ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø«Ø§Ø¨Øª Ù…Ø¹ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù„ÙØ¦Ø§Øª (Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©) ====================\n",
        "print(\"\\n3. ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠ Ø§Ù„Ø«Ø§Ø¨Øª Ù…Ø¹ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù„ÙØ¦Ø§Øª...\")\n",
        "\n",
        "class FixedProductionPredictorAr:\n",
        "    \"\"\"Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø«Ø§Ø¨Øª Ù…Ø¹ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù„ÙØ¦Ø§Øª Ù„ÙƒÙ„ Ø§Ù„Ø´Ø®ØµÙŠØ§Øª Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\"\"\"\n",
        "\n",
        "    def __init__(self, train_on_data=True):\n",
        "        self.characters = CHARACTERS_AR\n",
        "        self.feature_engineer = AdvancedFeatureEngineerAr()\n",
        "        self.pattern_classifier = PatternTypeClassifierAr()\n",
        "        self.answer_validator = AnswerValidatorAr()\n",
        "\n",
        "        # Ø¥Ù†Ø´Ø§Ø¡ ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø´Ø®ØµÙŠØ© Ø¥Ù„Ù‰ Ø§Ù„ÙÙ‡Ø±Ø³\n",
        "        self.char_to_idx = {char: idx for idx, char in enumerate(self.characters)}\n",
        "        self.idx_to_char = {idx: char for idx, char in enumerate(self.characters)}\n",
        "\n",
        "        if train_on_data:\n",
        "            self._train_models()\n",
        "        else:\n",
        "            self.models = None\n",
        "            self.scalers = None\n",
        "\n",
        "    def _train_models(self):\n",
        "        \"\"\"ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…Ø¹ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù„ÙØ¦Ø§Øª\"\"\"\n",
        "        print(\"   Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª...\")\n",
        "\n",
        "        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª\n",
        "        features = self.feature_engineer.create_features(df)\n",
        "\n",
        "        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù‡Ø¯Ù Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù€ 18\n",
        "        y = df['top1_char'].map(self.char_to_idx).fillna(0).astype(int)\n",
        "\n",
        "        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù€ 18\n",
        "        unique_classes = np.unique(y)\n",
        "        print(f\"   ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(unique_classes)} ÙØ¦Ø© ÙØ±ÙŠØ¯Ø© ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\")\n",
        "\n",
        "        # ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ù†Ù…Ø§Ø·\n",
        "        print(\"   ØªØµÙ†ÙŠÙ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ù†Ù…Ø§Ø·...\")\n",
        "        pattern_types = features.apply(self.pattern_classifier.classify, axis=1)\n",
        "        features['pattern_type'] = pattern_types\n",
        "\n",
        "        print(\"   ØªÙˆØ²ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ù†Ù…Ø§Ø·:\")\n",
        "        print(pattern_types.value_counts())\n",
        "\n",
        "        # ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø±Ø¦ÙŠØ³ÙŠ (Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª)\n",
        "        print(\"   ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ...\")\n",
        "        X_main = features.drop('pattern_type', axis=1, errors='ignore')\n",
        "\n",
        "        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…ØªØ³Ù‚Ø©\n",
        "        self.feature_columns = list(X_main.columns)\n",
        "\n",
        "        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ø§Ø®ØªØ¨Ø§Ø±\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X_main, y, test_size=0.2, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        # Ù‚ÙŠØ§Ø³ Ø§Ù„Ù…ÙŠØ²Ø§Øª\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        # ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¬Ù…ÙŠØ¹\n",
        "        print(\"   ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¬Ù…ÙŠØ¹...\")\n",
        "\n",
        "        # Ø§Ø³ØªØ®Ø¯Ø§Ù… RandomForest Ø§Ù„Ø°ÙŠ ÙŠØªØ¹Ø§Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„ Ù…Ø¹ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©\n",
        "        rf_model = RandomForestClassifier(\n",
        "            n_estimators=200,\n",
        "            max_depth=15,\n",
        "            min_samples_split=5,\n",
        "            min_samples_leaf=2,\n",
        "            max_features='sqrt',\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "            class_weight='balanced'\n",
        "        )\n",
        "\n",
        "        # Ø§Ø³ØªØ®Ø¯Ø§Ù… GradientBoosting ÙƒØ§Ø­ØªÙŠØ§Ø·ÙŠ\n",
        "        gb_model = GradientBoostingClassifier(\n",
        "            n_estimators=150,\n",
        "            max_depth=6,\n",
        "            learning_rate=0.05,\n",
        "            subsample=0.8,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # Ø§Ù„ØªØ¬Ù…ÙŠØ¹\n",
        "        ensemble_model = VotingClassifier(\n",
        "            estimators=[('rf', rf_model), ('gb', gb_model)],\n",
        "            voting='soft',\n",
        "            weights=[0.7, 0.3]\n",
        "        )\n",
        "\n",
        "        ensemble_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "        # Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\n",
        "        y_pred = ensemble_model.predict(X_test_scaled)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        print(f\"   Ø¯Ù‚Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ: {accuracy:.2%}\")\n",
        "\n",
        "        # ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬\n",
        "        self.model = ensemble_model\n",
        "        self.scaler = scaler\n",
        "        self.pattern_distribution = pattern_types.value_counts().to_dict()\n",
        "\n",
        "        # ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø§Ø­ØªÙŠØ§Ø· Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© Ø¨Ø§Ù„Ø£Ù†Ù…Ø§Ø· ÙÙ‚Ø· Ù„Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©\n",
        "        print(\"   ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø§Ø­ØªÙŠØ§Ø· Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© Ø¨Ø§Ù„Ø£Ù†Ù…Ø§Ø·...\")\n",
        "        self.pattern_models = {}\n",
        "        self.pattern_scalers = {}\n",
        "\n",
        "        # Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙÙ‚Ø· Ù„Ù„Ø£Ù†Ù…Ø§Ø· Ø°Ø§Øª Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø§Ù„ÙƒØ§ÙÙŠØ©\n",
        "        for pattern_type, count in pattern_types.value_counts().items():\n",
        "            if count > 1000:  # ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¹ÙŠÙ†Ø§Øª ÙƒØ§ÙÙŠØ©\n",
        "                print(f\"     ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ {pattern_type} ({count:,} Ø¹ÙŠÙ†Ø©)...\")\n",
        "\n",
        "                # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ù„Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…Ø·\n",
        "                pattern_indices = pattern_types[pattern_types == pattern_type].index\n",
        "\n",
        "                if len(pattern_indices) > 100:\n",
        "                    X_pattern = X_main.loc[pattern_indices]\n",
        "                    y_pattern = y.loc[pattern_indices]\n",
        "\n",
        "                    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù„Ø¯ÙŠÙ†Ø§ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙØ¦Ø§Øª\n",
        "                    unique_in_pattern = np.unique(y_pattern)\n",
        "                    if len(unique_in_pattern) > 5:  # ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ÙØ¦Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø©\n",
        "                        # ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø³ÙŠØ·\n",
        "                        pattern_scaler = StandardScaler()\n",
        "                        X_pattern_scaled = pattern_scaler.fit_transform(X_pattern)\n",
        "\n",
        "                        pattern_model = RandomForestClassifier(\n",
        "                            n_estimators=100,\n",
        "                            max_depth=10,\n",
        "                            random_state=42,\n",
        "                            n_jobs=-1,\n",
        "                            class_weight='balanced'\n",
        "                        )\n",
        "\n",
        "                        pattern_model.fit(X_pattern_scaled, y_pattern)\n",
        "\n",
        "                        self.pattern_models[pattern_type] = pattern_model\n",
        "                        self.pattern_scalers[pattern_type] = pattern_scaler\n",
        "\n",
        "                        # Ø§Ø®ØªØ¨Ø§Ø± Ø³Ø±ÙŠØ¹\n",
        "                        if len(pattern_indices) > 200:\n",
        "                            test_size = min(100, len(pattern_indices) // 5)\n",
        "                            test_acc = pattern_model.score(\n",
        "                                pattern_scaler.transform(X_pattern.iloc[:test_size]),\n",
        "                                y_pattern.iloc[:test_size]\n",
        "                            )\n",
        "                            print(f\"       Ø¯Ù‚Ø© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {test_acc:.2%}\")\n",
        "\n",
        "        print(\"   âœ… ØªÙ… ØªØ¯Ø±ÙŠØ¨ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ù†Ø¬Ø§Ø­\")\n",
        "\n",
        "    def predict_with_confidence(self, user_answers: Dict) -> Dict:\n",
        "        \"\"\"Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù…Ø¹ Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ø«Ù‚Ø©\"\"\"\n",
        "\n",
        "        # Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØ¯Ù‚ÙŠÙ‚ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª\n",
        "        validation_warnings = self.answer_validator.validate(user_answers)\n",
        "\n",
        "        # Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª\n",
        "        df_input = pd.DataFrame([user_answers])\n",
        "        features = self.feature_engineer.create_features(df_input)\n",
        "\n",
        "        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø¬Ù…ÙŠØ¹ Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…ÙŠØ²Ø§Øª\n",
        "        for col in self.feature_columns:\n",
        "            if col not in features.columns:\n",
        "                features[col] = 0\n",
        "\n",
        "        features = features[self.feature_columns]\n",
        "\n",
        "        # Ø§Ù„Ø®Ø·ÙˆØ© 3: ØªØµÙ†ÙŠÙ Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·\n",
        "        pattern_type = self.pattern_classifier.classify(features.iloc[0])\n",
        "\n",
        "        # Ø§Ù„Ø®Ø·ÙˆØ© 4: Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ\n",
        "        X_scaled = self.scaler.transform(features)\n",
        "\n",
        "        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ\n",
        "        if hasattr(self.model, 'predict_proba'):\n",
        "            probabilities = self.model.predict_proba(X_scaled)[0]\n",
        "        else:\n",
        "            # Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¯Ø§Ù„Ø© Ø§Ù„Ù‚Ø±Ø§Ø± Ø£Ùˆ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ø¨Ø³ÙŠØ·\n",
        "            probabilities = np.zeros(len(self.characters))\n",
        "            prediction = self.model.predict(X_scaled)[0]\n",
        "            probabilities[prediction] = 0.8\n",
        "            # Ø¥Ø¶Ø§ÙØ© Ø¨Ø¹Ø¶ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ù„Ù„Ø´Ø®ØµÙŠØ§Øª Ø§Ù„Ù…Ù…Ø§Ø«Ù„Ø©\n",
        "            for i in range(len(probabilities)):\n",
        "                if i != prediction:\n",
        "                    probabilities[i] = 0.2 / (len(probabilities) - 1)\n",
        "\n",
        "        # Ø§Ù„Ø®Ø·ÙˆØ© 5: Ø¶Ø¨Ø· Ø§Ù„Ø«Ù‚Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·\n",
        "        base_confidence = np.max(probabilities)\n",
        "        top_idx = np.argmax(probabilities)\n",
        "        top_character = self.idx_to_char[top_idx]\n",
        "\n",
        "        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ 3 Ø´Ø®ØµÙŠØ§Øª\n",
        "        top_3_indices = np.argsort(probabilities)[-3:][::-1]\n",
        "        top_3_characters = [self.idx_to_char[idx] for idx in top_3_indices]\n",
        "\n",
        "        # Ø§Ù„Ø®Ø·ÙˆØ© 6: Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ø«Ù‚Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·\n",
        "        calibrated_confidence = self._calibrate_confidence(\n",
        "            base_confidence, pattern_type, features.iloc[0]\n",
        "        )\n",
        "\n",
        "        # Ø§Ù„Ø®Ø·ÙˆØ© 7: ØªÙˆÙ„ÙŠØ¯ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ù„Ø©\n",
        "        evidence_analysis = self._analyze_evidence(top_character, user_answers)\n",
        "\n",
        "        # Ø§Ù„Ø®Ø·ÙˆØ© 8: ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªÙŠØ¬Ø©\n",
        "        result = {\n",
        "            'prediction': {\n",
        "                'top_character': top_character,\n",
        "                'top_3_characters': top_3_characters,\n",
        "                'confidence': round(calibrated_confidence, 3),\n",
        "                'confidence_percent': f\"{calibrated_confidence:.1%}\",\n",
        "                'confidence_label': self._get_confidence_label(calibrated_confidence),\n",
        "                'pattern_type': pattern_type,\n",
        "                'certainty_level': self._get_certainty_level(calibrated_confidence, pattern_type)\n",
        "            },\n",
        "            'analysis': {\n",
        "                'evidence_strength': evidence_analysis['strength'],\n",
        "                'evidence_score': evidence_analysis['score'],\n",
        "                'pattern_clarity': features.iloc[0].get('archetype_clarity', 0.5),\n",
        "                'ambiguity_score': features.iloc[0].get('total_ambiguity', 0.5)\n",
        "            },\n",
        "            'warnings': {\n",
        "                'validation_warnings': validation_warnings,\n",
        "                'has_warnings': len(validation_warnings) > 0\n",
        "            },\n",
        "            'raw_data': {\n",
        "                'all_probabilities': {self.idx_to_char[i]: float(prob)\n",
        "                                     for i, prob in enumerate(probabilities)},\n",
        "                'pattern_type': pattern_type\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _calibrate_confidence(self, base_confidence: float, pattern_type: str,\n",
        "                            features: pd.Series) -> float:\n",
        "        \"\"\"Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ø«Ù‚Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·\"\"\"\n",
        "\n",
        "        clarity = features.get('archetype_clarity', 0.5)\n",
        "        ambiguity = features.get('total_ambiguity', 0.5)\n",
        "\n",
        "        # Ø§Ù„ØªØ¹Ø¯ÙŠÙ„Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·\n",
        "        if pattern_type == 'very_clear':\n",
        "            target_min, target_max = 0.85, 0.95\n",
        "            if clarity > 0.9:\n",
        "                base = 0.9\n",
        "            elif clarity > 0.8:\n",
        "                base = 0.85\n",
        "            else:\n",
        "                base = 0.8\n",
        "\n",
        "        elif pattern_type == 'clear':\n",
        "            target_min, target_max = 0.75, 0.88\n",
        "            if clarity > 0.8:\n",
        "                base = 0.82\n",
        "            elif clarity > 0.7:\n",
        "                base = 0.78\n",
        "            else:\n",
        "                base = 0.75\n",
        "\n",
        "        elif pattern_type == 'mixed':\n",
        "            target_min, target_max = 0.50, 0.70\n",
        "            clear_count = features.get('clear_pattern_count', 0)\n",
        "            if clear_count == 2 and ambiguity < 0.5:\n",
        "                base = 0.65\n",
        "            elif clear_count >= 3:\n",
        "                base = 0.55\n",
        "            else:\n",
        "                base = 0.60\n",
        "\n",
        "        elif pattern_type == 'ambiguous':\n",
        "            target_min, target_max = 0.30, 0.50\n",
        "            if clarity < 0.4:\n",
        "                base = 0.35\n",
        "            elif ambiguity > 0.7:\n",
        "                base = 0.3\n",
        "            else:\n",
        "                base = 0.4\n",
        "\n",
        "        else:  # very_mixed or contradictory\n",
        "            target_min, target_max = 0.15, 0.35\n",
        "            clear_count = features.get('clear_pattern_count', 0)\n",
        "            if clear_count >= 3 or ambiguity > 0.8:\n",
        "                base = 0.2\n",
        "            else:\n",
        "                base = 0.25\n",
        "\n",
        "        # Ø§Ù„Ù…Ø²Ø¬ Ù…Ø¹ Ø«Ù‚Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\n",
        "        calibrated = (base * 0.6) + (base_confidence * 0.4)\n",
        "\n",
        "        # Ø¥Ø¶Ø§ÙØ© ØªØ¨Ø§ÙŠÙ† Ø¹Ø´ÙˆØ§Ø¦ÙŠ ØµØºÙŠØ± Ù„Ù„ÙˆØ§Ù‚Ø¹ÙŠØ©\n",
        "        if pattern_type in ['mixed', 'ambiguous', 'very_mixed']:\n",
        "            calibrated += np.random.uniform(-0.05, 0.05)\n",
        "\n",
        "        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø¨Ù‚Ø§Ø¡ Ø¶Ù…Ù† Ø§Ù„Ø­Ø¯ÙˆØ¯\n",
        "        calibrated = np.clip(calibrated, target_min, target_max)\n",
        "\n",
        "        return calibrated\n",
        "\n",
        "    def _analyze_evidence(self, character: str, user_answers: Dict) -> Dict:\n",
        "        \"\"\"ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ù„Ø© Ù„Ø´Ø®ØµÙŠØ©\"\"\"\n",
        "\n",
        "        if character not in CHARACTER_SIGNATURES_AR:\n",
        "            return {'score': 0, 'strength': 'very_weak'}\n",
        "\n",
        "        sig_info = CHARACTER_SIGNATURES_AR[character]\n",
        "        must_matched = 0\n",
        "        often_matched = 0\n",
        "\n",
        "        slider_map = {'0-20%': 0.1, '21-50%': 0.35, '51-80%': 0.65, '81-100%': 0.9}\n",
        "\n",
        "        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø´Ø±ÙˆØ· Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ©\n",
        "        for condition in sig_info.get(\"must_have\", []):\n",
        "            q, val = condition.split(':')\n",
        "            if q in user_answers:\n",
        "                if val == 'high':\n",
        "                    if q in ['Q2', 'Q4', 'Q8']:\n",
        "                        num_val = slider_map.get(user_answers[q], 0.5)\n",
        "                        if num_val > 0.8:\n",
        "                            must_matched += 1\n",
        "                elif val == 'low':\n",
        "                    if q in ['Q2', 'Q4', 'Q8']:\n",
        "                        num_val = slider_map.get(user_answers[q], 0.5)\n",
        "                        if num_val < 0.2:\n",
        "                            must_matched += 1\n",
        "                else:\n",
        "                    if val in str(user_answers[q]).split(','):\n",
        "                        must_matched += 1\n",
        "\n",
        "        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø´Ø±ÙˆØ· Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©\n",
        "        for condition in sig_info.get(\"often_have\", []):\n",
        "            q, val = condition.split(':')\n",
        "            if q in user_answers:\n",
        "                if val == 'high':\n",
        "                    if q in ['Q2', 'Q4', 'Q8']:\n",
        "                        num_val = slider_map.get(user_answers[q], 0.5)\n",
        "                        if num_val > 0.8:\n",
        "                            often_matched += 1\n",
        "                elif val == 'low':\n",
        "                    if q in ['Q2', 'Q4', 'Q8']:\n",
        "                        num_val = slider_map.get(user_answers[q], 0.5)\n",
        "                        if num_val < 0.2:\n",
        "                            often_matched += 1\n",
        "                else:\n",
        "                    if val in str(user_answers[q]).split(','):\n",
        "                        often_matched += 1\n",
        "\n",
        "        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†ØªÙŠØ¬Ø©\n",
        "        total_must = len(sig_info.get(\"must_have\", []))\n",
        "        total_often = len(sig_info.get(\"often_have\", []))\n",
        "\n",
        "        if total_must > 0:\n",
        "            must_score = must_matched / total_must\n",
        "            often_score = often_matched / total_often if total_often > 0 else 0\n",
        "            score = (must_score * 0.7) + (often_score * 0.3)\n",
        "        else:\n",
        "            score = often_matched / total_often if total_often > 0 else 0\n",
        "\n",
        "        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù‚ÙˆØ©\n",
        "        if must_matched >= 2 or score > 0.9:\n",
        "            strength = 'very_strong'\n",
        "        elif must_matched >= 1 or score > 0.75:\n",
        "            strength = 'strong'\n",
        "        elif score > 0.6:\n",
        "            strength = 'moderate'\n",
        "        elif score > 0.4:\n",
        "            strength = 'weak'\n",
        "        else:\n",
        "            strength = 'very_weak'\n",
        "\n",
        "        return {\n",
        "            'score': min(score, 1.0),\n",
        "            'strength': strength,\n",
        "            'must_matched': must_matched,\n",
        "            'often_matched': often_matched\n",
        "        }\n",
        "\n",
        "    def _get_confidence_label(self, confidence: float) -> str:\n",
        "        \"\"\"ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø«Ù‚Ø© Ø¥Ù„Ù‰ ÙˆØµÙ Ù‚Ø§Ø¨Ù„ Ù„Ù„Ù‚Ø±Ø§Ø¡Ø© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\"\"\"\n",
        "        if confidence >= 0.9:\n",
        "            return \"Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹\"\n",
        "        elif confidence >= 0.85:\n",
        "            return \"Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©\"\n",
        "        elif confidence >= 0.8:\n",
        "            return \"Ø«Ù‚Ø© Ù…ØªÙˆØ³Ø·Ø©-Ø¹Ø§Ù„ÙŠØ©\"\n",
        "        elif confidence >= 0.7:\n",
        "            return \"Ø«Ù‚Ø© Ù…ØªÙˆØ³Ø·Ø©\"\n",
        "        elif confidence >= 0.6:\n",
        "            return \"Ø«Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø©-Ù…ØªÙˆØ³Ø·Ø©\"\n",
        "        elif confidence >= 0.5:\n",
        "            return \"Ø«Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø©\"\n",
        "        elif confidence >= 0.3:\n",
        "            return \"Ø«Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø© Ø¬Ø¯Ø§Ù‹\"\n",
        "        else:\n",
        "            return \"Ø«Ù‚Ø© Ø¶Ø¦ÙŠÙ„Ø©\"\n",
        "\n",
        "    def _get_certainty_level(self, confidence: float, pattern_type: str) -> str:\n",
        "        \"\"\"Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ÙŠÙ‚ÙŠÙ† Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\"\"\"\n",
        "        if confidence >= 0.85:\n",
        "            return \"ÙŠÙ‚ÙŠÙ† Ø¹Ø§Ù„ÙŠ Ø¬Ø¯Ø§Ù‹\"\n",
        "        elif confidence >= 0.75:\n",
        "            return \"ÙŠÙ‚ÙŠÙ† Ø¹Ø§Ù„ÙŠ\"\n",
        "        elif confidence >= 0.65:\n",
        "            return \"ÙŠÙ‚ÙŠÙ† Ù…ØªÙˆØ³Ø·\"\n",
        "        elif confidence >= 0.5:\n",
        "            return \"ÙŠÙ‚ÙŠÙ† Ù…Ù†Ø®ÙØ¶\"\n",
        "        else:\n",
        "            return \"ÙŠÙ‚ÙŠÙ† Ù…Ù†Ø®ÙØ¶ Ø¬Ø¯Ø§Ù‹\"\n",
        "\n",
        "    def save_model(self, path='fixed_production_predictor_ar.pkl'):\n",
        "        \"\"\"Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨\"\"\"\n",
        "        with open(path, 'wb') as f:\n",
        "            pickle.dump({\n",
        "                'model': self.model,\n",
        "                'scaler': self.scaler,\n",
        "                'pattern_models': self.pattern_models,\n",
        "                'pattern_scalers': self.pattern_scalers,\n",
        "                'feature_columns': self.feature_columns,\n",
        "                'char_to_idx': self.char_to_idx,\n",
        "                'idx_to_char': self.idx_to_char,\n",
        "                'pattern_distribution': self.pattern_distribution\n",
        "            }, f)\n",
        "        print(f\"   âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ {path}\")\n",
        "\n",
        "    def load_model(self, path='fixed_production_predictor_ar.pkl'):\n",
        "        \"\"\"ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¯Ø±Ø¨\"\"\"\n",
        "        with open(path, 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "\n",
        "        self.model = data['model']\n",
        "        self.scaler = data['scaler']\n",
        "        self.pattern_models = data.get('pattern_models', {})\n",
        "        self.pattern_scalers = data.get('pattern_scalers', {})\n",
        "        self.feature_columns = data['feature_columns']\n",
        "        self.char_to_idx = data['char_to_idx']\n",
        "        self.idx_to_char = data['idx_to_char']\n",
        "        self.pattern_distribution = data.get('pattern_distribution', {})\n",
        "        print(f\"   âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† {path}\")\n",
        "\n",
        "# ==================== 9. ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø«Ø§Ø¨Øª Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ====================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸš€ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠ Ø§Ù„Ø«Ø§Ø¨Øª Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Ø¥Ù†Ø´Ø§Ø¡ ÙˆØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø«Ø§Ø¨Øª\n",
        "predictor_ar = FixedProductionPredictorAr(train_on_data=True)\n",
        "\n",
        "# Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\n",
        "predictor_ar.save_model('fixed_production_character_predictor_ar.pkl')\n",
        "\n",
        "# ==================== 10. Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø´Ø§Ù…Ù„ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ====================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠ Ø§Ù„Ø«Ø§Ø¨Øª Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\n",
        "test_cases_ar = [\n",
        "    {\n",
        "        \"name\": \"Ø§Ø®ØªØ¨Ø§Ø± 1: Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ Ø§Ù„ÙˆØ§Ø¶Ø­ Ø¬Ø¯Ø§Ù‹\",\n",
        "        \"answers\": {\n",
        "            \"Q1\": \"0\", \"Q2\": \"81-100%\", \"Q3\": \"0\", \"Q4\": \"0-20%\",\n",
        "            \"Q5\": \"0\", \"Q6\": \"0\", \"Q7\": \"0\", \"Q8\": \"0-20%\",\n",
        "            \"Q9\": \"3\", \"Q10\": \"1\", \"Q11\": \"0\", \"Q12\": \"0\", \"Q13\": \"2\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Ø§Ø®ØªØ¨Ø§Ø± 2: Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³ Ø§Ù„ÙˆØ§Ø¶Ø­\",\n",
        "        \"answers\": {\n",
        "            \"Q1\": \"2\", \"Q2\": \"51-80%\", \"Q3\": \"3\", \"Q4\": \"21-50%\",\n",
        "            \"Q5\": \"2\", \"Q6\": \"2\", \"Q7\": \"3\", \"Q8\": \"21-50%\",\n",
        "            \"Q9\": \"0\", \"Q10\": \"0\", \"Q11\": \"4\", \"Q12\": \"4\", \"Q13\": \"3\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Ø§Ø®ØªØ¨Ø§Ø± 3: Ù…Ø®ØªÙ„Ø· (Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ + Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…)\",\n",
        "        \"answers\": {\n",
        "            \"Q1\": \"0\", \"Q2\": \"81-100%\", \"Q3\": \"0\", \"Q4\": \"0-20%\",\n",
        "            \"Q5\": \"0,3\", \"Q6\": \"0\", \"Q7\": \"0,2\", \"Q8\": \"21-50%\",\n",
        "            \"Q9\": \"3\", \"Q10\": \"1,3\", \"Q11\": \"0\", \"Q12\": \"0,2\", \"Q13\": \"2,7\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Ø§Ø®ØªØ¨Ø§Ø± 4: ØºØ§Ù…Ø¶ (Ø£Ù†Ù…Ø§Ø· Ù…ØªØ¹Ø¯Ø¯Ø©)\",\n",
        "        \"answers\": {\n",
        "            \"Q1\": \"0,2,3\", \"Q2\": \"51-80%\", \"Q3\": \"0,1,3\", \"Q4\": \"51-80%\",\n",
        "            \"Q5\": \"0,1,3\", \"Q6\": \"0,2,5\", \"Q7\": \"0,1,4\", \"Q8\": \"51-80%\",\n",
        "            \"Q9\": \"0,1,4\", \"Q10\": \"0,2,4\", \"Q11\": \"0,1,3\", \"Q12\": \"0,3,5\", \"Q13\": \"0,2,4,7\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Ø§Ø®ØªØ¨Ø§Ø± 5: Ù…Ø®ØªÙ„Ø· Ø¬Ø¯Ø§Ù‹ (ÙƒÙ„ Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª)\",\n",
        "        \"answers\": {\n",
        "            \"Q1\": \"0,1,2,3,4,5\", \"Q2\": \"51-80%\", \"Q3\": \"0,1,2,3,4,5\",\n",
        "            \"Q4\": \"51-80%\", \"Q5\": \"0,1,2,3,4,5\", \"Q6\": \"0,1,2,3,4,5\",\n",
        "            \"Q7\": \"0,1,2,3,4,5\", \"Q8\": \"51-80%\", \"Q9\": \"0,1,2,3,4,5\",\n",
        "            \"Q10\": \"0,1,2,3,4,5\", \"Q11\": \"0,1,2,3,4,5\",\n",
        "            \"Q12\": \"0,1,2,3,4,5\", \"Q13\": \"0,1,2,3,4,5,6,7\"\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"\\nØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª...\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "results_ar = []\n",
        "for i, test in enumerate(test_cases_ar, 1):\n",
        "    print(f\"\\n{i}. {test['name']}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    try:\n",
        "        result = predictor_ar.predict_with_confidence(test['answers'])\n",
        "        pred = result['prediction']\n",
        "        analysis = result['analysis']\n",
        "\n",
        "        # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
        "        print(f\"Ø§Ù„ØªÙ†Ø¨Ø¤: {pred['top_character']} ({pred['confidence_percent']})\")\n",
        "        print(f\"Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·: {pred['pattern_type'].upper()}\")\n",
        "        print(f\"ØªØµÙ†ÙŠÙ Ø§Ù„Ø«Ù‚Ø©: {pred['confidence_label']}\")\n",
        "        print(f\"Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ÙŠÙ‚ÙŠÙ†: {pred['certainty_level']}\")\n",
        "        print(f\"Ø£ÙØ¶Ù„ 3: {', '.join(pred['top_3_characters'])}\")\n",
        "        print(f\"Ù‚ÙˆØ© Ø§Ù„Ø£Ø¯Ù„Ø©: {analysis['evidence_strength'].upper()}\")\n",
        "        print(f\"Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø£Ø¯Ù„Ø©: {analysis['evidence_score']:.2f}\")\n",
        "\n",
        "        # Ø¹Ø±Ø¶ Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª Ø¥Ù† ÙˆØ¬Ø¯Øª\n",
        "        if result['warnings']['has_warnings']:\n",
        "            print(f\"âš ï¸  ØªØ­Ø°ÙŠØ±Ø§Øª:\")\n",
        "            for warning in result['warnings']['validation_warnings']:\n",
        "                print(f\"   â€¢ {warning}\")\n",
        "\n",
        "        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠØ© Ø§Ù„Ù†ÙØ³ÙŠØ©\n",
        "        if pred['pattern_type'] in ['very_clear', 'clear']:\n",
        "            expected_range = (0.75, 0.95)\n",
        "        elif pred['pattern_type'] == 'mixed':\n",
        "            expected_range = (0.50, 0.70)\n",
        "        elif pred['pattern_type'] == 'ambiguous':\n",
        "            expected_range = (0.30, 0.50)\n",
        "        else:  # very_mixed\n",
        "            expected_range = (0.15, 0.35)\n",
        "\n",
        "        confidence = pred['confidence']\n",
        "        in_range = expected_range[0] <= confidence <= expected_range[1]\n",
        "\n",
        "        if in_range:\n",
        "            print(f\"âœ… Ø§Ù„Ø«Ù‚Ø© ÙˆØ§Ù‚Ø¹ÙŠØ© Ù„Ù†Ù…Ø· {pred['pattern_type']}\")\n",
        "        else:\n",
        "            print(f\"âš ï¸  Ø§Ù„Ø«Ù‚Ø© Ø®Ø§Ø±Ø¬ Ø§Ù„Ù†Ø·Ø§Ù‚ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ Ù„Ù†Ù…Ø· {pred['pattern_type']}\")\n",
        "\n",
        "        # ØªØ®Ø²ÙŠÙ† Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
        "        results_ar.append({\n",
        "            'test': test['name'],\n",
        "            'top_predicted': pred['top_character'],\n",
        "            'confidence': confidence,\n",
        "            'pattern_type': pred['pattern_type'],\n",
        "            'evidence_strength': analysis['evidence_strength'],\n",
        "            'confidence_realistic': in_range\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Ø®Ø·Ø£: {e}\")\n",
        "        continue\n",
        "\n",
        "# ==================== 11. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ====================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if results_ar:\n",
        "    print(f\"\\nğŸ¯ Ù…Ù„Ø®Øµ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±:\")\n",
        "    print(f\"   Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª: {len(test_cases_ar)}\")\n",
        "    print(f\"   Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù†Ø§Ø¬Ø­Ø©: {len(results_ar)}\")\n",
        "\n",
        "    # ÙˆØ§Ù‚Ø¹ÙŠØ© Ø§Ù„Ø«Ù‚Ø©\n",
        "    realistic_count = sum(1 for r in results_ar if r['confidence_realistic'])\n",
        "    realism_percentage = realistic_count / len(results_ar) * 100\n",
        "    print(f\"   Ø§Ù„Ø«Ù‚Ø© Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠØ©: {realistic_count}/{len(results_ar)} ({realism_percentage:.1f}%)\")\n",
        "\n",
        "    # ØªÙˆØ²ÙŠØ¹ Ù‚ÙˆØ© Ø§Ù„Ø£Ø¯Ù„Ø©\n",
        "    print(f\"\\nğŸ” ØªÙˆØ²ÙŠØ¹ Ù‚ÙˆØ© Ø§Ù„Ø£Ø¯Ù„Ø©:\")\n",
        "    strength_counts = Counter([r['evidence_strength'] for r in results_ar])\n",
        "    for strength, count in strength_counts.items():\n",
        "        percentage = (count / len(results_ar)) * 100\n",
        "        print(f\"   {strength.upper():15}: {count} Ø§Ø®ØªØ¨Ø§Ø± ({percentage:.1f}%)\")\n",
        "\n",
        "    # ØªÙˆØ²ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ù†Ù…Ø§Ø·\n",
        "    print(f\"\\nğŸ“Š ØªÙˆØ²ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ù†Ù…Ø§Ø·:\")\n",
        "    pattern_counts = Counter([r['pattern_type'] for r in results_ar])\n",
        "    for pattern, count in pattern_counts.items():\n",
        "        percentage = (count / len(results_ar)) * 100\n",
        "        print(f\"   {pattern.upper():15}: {count} Ø§Ø®ØªØ¨Ø§Ø± ({percentage:.1f}%)\")\n",
        "\n",
        "    # Ø§Ù„Ø«Ù‚Ø© Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·\n",
        "    print(f\"\\nğŸ“ˆ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø«Ù‚Ø© Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·:\")\n",
        "    pattern_confidences = defaultdict(list)\n",
        "    for result in results_ar:\n",
        "        pattern_confidences[result['pattern_type']].append(result['confidence'])\n",
        "\n",
        "    for pattern, confidences in pattern_confidences.items():\n",
        "        avg_conf = np.mean(confidences) * 100\n",
        "        min_conf = min(confidences) * 100\n",
        "        max_conf = max(confidences) * 100\n",
        "        print(f\"   {pattern.upper():15}: {avg_conf:.1f}% (Ø§Ù„Ù…Ø¯Ù‰: {min_conf:.1f}%-{max_conf:.1f}%)\")\n",
        "\n",
        "    # Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¹Ø§Ù…\n",
        "    print(f\"\\nğŸ† Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¹Ø§Ù…:\")\n",
        "\n",
        "    if realism_percentage >= 80:\n",
        "        print(\"   âœ… Ù…Ù…ØªØ§Ø²: Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ø«Ù‚Ø© ØªØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ø¬ÙŠØ¯\")\n",
        "        print(\"   ğŸš€ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¥Ù†ØªØ§Ø¬ Ø¨Ø«Ù‚Ø© ÙˆØ§Ù‚Ø¹ÙŠØ©\")\n",
        "    elif realism_percentage >= 60:\n",
        "        print(\"   âš ï¸  Ø¬ÙŠØ¯: ØªØ­ØªØ§Ø¬ Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ø«Ù‚Ø© Ø¥Ù„Ù‰ ØªØ¹Ø¯ÙŠÙ„Ø§Øª Ø·ÙÙŠÙØ©\")\n",
        "        print(\"   ğŸ“ˆ ÙŠØ¸Ù‡Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù…ÙƒØ§Ù†Ø§Øª Ø¨Ø«Ù‚Ø© ÙˆØ§Ù‚Ø¹ÙŠØ© Ø¥Ù„Ù‰ Ø­Ø¯ ÙƒØ¨ÙŠØ±\")\n",
        "    else:\n",
        "        print(\"   âŒ ÙŠØ­ØªØ§Ø¬ Ù„Ù„ØªØ­Ø³ÙŠÙ†: ØªØ­ØªØ§Ø¬ Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ø«Ù‚Ø© Ø¥Ù„Ù‰ Ø¹Ù…Ù„\")\n",
        "        print(\"   ğŸ”§ ÙÙƒØ± ÙÙŠ Ø¶Ø¨Ø· Ù…Ø¹Ù„Ù…Ø§Øª Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ø«Ù‚Ø©\")\n",
        "\n",
        "    print(f\"\\nğŸ’¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª:\")\n",
        "    print(\"   1. Ù…Ø±Ø§Ù‚Ø¨Ø© Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ø«Ù‚Ø© ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬\")\n",
        "    print(\"   2. Ø¬Ù…Ø¹ Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø­ÙˆÙ„ Ø¯Ù‚Ø© Ø§Ù„ØªÙ†Ø¨Ø¤\")\n",
        "    print(\"   3. Ø¶Ø¨Ø· Ù†Ø·Ø§Ù‚Ø§Øª Ø§Ù„Ø«Ù‚Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙØ¹Ù„ÙŠ\")\n",
        "    print(\"   4. Ø§Ù„ØªÙÙƒÙŠØ± ÙÙŠ Ø§Ø®ØªØ¨Ø§Ø± A/B Ù„Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ù…Ø®ØªÙ„ÙØ©\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù„Ù„ØªØ­Ù„ÙŠÙ„\")\n",
        "\n",
        "# ==================== 12. Ø¹Ø±Ø¶ ØªÙˆØ¶ÙŠØ­ÙŠ Ø¨Ù…Ø³ØªØ®Ø¯Ù… Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ====================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ‘¤ Ø§Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠ: Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ù…Ø³ØªØ®Ø¯Ù… Ø¹Ø´ÙˆØ§Ø¦ÙŠ\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªØ¹Ø±ÙŠÙ Ù…Ø³ØªØ®Ø¯Ù… Ø¹Ø´ÙˆØ§Ø¦ÙŠ ÙˆÙ„ÙƒÙ† ÙˆØ§Ù‚Ø¹ÙŠ Ù†ÙØ³ÙŠØ§Ù‹\n",
        "def create_random_user_profile_ar():\n",
        "    \"\"\"Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªØ¹Ø±ÙŠÙ Ù…Ø³ØªØ®Ø¯Ù… Ø¹Ø´ÙˆØ§Ø¦ÙŠ ÙˆÙ„ÙƒÙ† ÙˆØ§Ù‚Ø¹ÙŠ Ù†ÙØ³ÙŠØ§Ù‹ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\"\"\"\n",
        "\n",
        "    # Ø§Ø®ØªÙŠØ§Ø± Ø´Ø®ØµÙŠØ© Ù…Ù‡ÙŠÙ…Ù†Ø©\n",
        "    dominant_chars = [\"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\", \"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³\", \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯\",\n",
        "                     \"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\", \"Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ\", \"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\"]\n",
        "    dominant = random.choice(dominant_chars)\n",
        "\n",
        "    # Ø¥Ù†Ø´Ø§Ø¡ Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ù…Ù‡ÙŠÙ…Ù†Ø©\n",
        "    answers = {}\n",
        "\n",
        "    # Ø£Ø´Ø±Ø·Ø© Ø§Ù„ØªÙ…Ø±ÙŠØ±\n",
        "    if dominant == \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\":\n",
        "        answers['Q2'] = random.choice([\"81-100%\", \"81-100%\", \"51-80%\"])\n",
        "        answers['Q4'] = random.choice([\"0-20%\", \"21-50%\"])\n",
        "        answers['Q8'] = random.choice([\"0-20%\", \"21-50%\"])\n",
        "    elif dominant == \"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³\":\n",
        "        answers['Q2'] = random.choice([\"21-50%\", \"51-80%\"])\n",
        "        answers['Q4'] = random.choice([\"21-50%\", \"51-80%\"])\n",
        "        answers['Q8'] = random.choice([\"21-50%\", \"51-80%\"])\n",
        "    elif dominant == \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯\":\n",
        "        answers['Q2'] = random.choice([\"21-50%\", \"0-20%\"])\n",
        "        answers['Q4'] = random.choice([\"81-100%\", \"51-80%\", \"81-100%\"])\n",
        "        answers['Q8'] = random.choice([\"21-50%\", \"51-80%\"])\n",
        "    else:\n",
        "        answers['Q2'] = random.choice([\"21-50%\", \"51-80%\", \"0-20%\", \"81-100%\"])\n",
        "        answers['Q4'] = random.choice([\"21-50%\", \"51-80%\", \"0-20%\", \"81-100%\"])\n",
        "        answers['Q8'] = random.choice([\"21-50%\", \"51-80%\", \"0-20%\", \"81-100%\"])\n",
        "\n",
        "    # Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„ÙØ±Ø¯ÙŠ/Ø§Ù„Ù…ØªØ¹Ø¯Ø¯ Ù…Ø¹ Ø¥Ø¬Ø§Ø¨Ø§Øª Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ø´Ø®ØµÙŠØ©\n",
        "    if dominant == \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\":\n",
        "        answers['Q1'] = \"0\"\n",
        "        answers['Q3'] = \"0\"\n",
        "        answers['Q5'] = \"0\"\n",
        "        answers['Q6'] = \"0\"\n",
        "        answers['Q7'] = \"0\"\n",
        "        answers['Q9'] = \"3\"\n",
        "        answers['Q10'] = \"1\"\n",
        "        answers['Q11'] = \"0\"\n",
        "        answers['Q12'] = \"0\"\n",
        "        answers['Q13'] = \"2\"\n",
        "    elif dominant == \"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³\":\n",
        "        answers['Q1'] = \"2\"\n",
        "        answers['Q3'] = \"3\"\n",
        "        answers['Q5'] = \"2\"\n",
        "        answers['Q6'] = \"2\"\n",
        "        answers['Q7'] = \"3\"\n",
        "        answers['Q9'] = \"0\"\n",
        "        answers['Q10'] = \"0\"\n",
        "        answers['Q11'] = \"4\"\n",
        "        answers['Q12'] = \"4\"\n",
        "        answers['Q13'] = \"3\"\n",
        "    elif dominant == \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯\":\n",
        "        answers['Q1'] = \"5\"\n",
        "        answers['Q3'] = \"2,4\"\n",
        "        answers['Q5'] = \"1\"\n",
        "        answers['Q6'] = \"3\"\n",
        "        answers['Q7'] = \"2\"\n",
        "        answers['Q9'] = \"1,4\"\n",
        "        answers['Q10'] = \"4\"\n",
        "        answers['Q11'] = \"1\"\n",
        "        answers['Q12'] = \"3\"\n",
        "        answers['Q13'] = \"0,6\"\n",
        "    else:\n",
        "        # Ø¥Ø¬Ø§Ø¨Ø§Øª Ù…Ø¹Ù‚ÙˆÙ„Ø© Ø¹Ø§Ù…Ø©\n",
        "        answers['Q1'] = random.choice([\"0\", \"2\", \"3\", \"4\", \"5\"])\n",
        "        answers['Q3'] = random.choice([\"0\", \"1\", \"2\", \"3\", \"4\"])\n",
        "        answers['Q5'] = random.choice([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"])\n",
        "        answers['Q6'] = random.choice([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"])\n",
        "        answers['Q7'] = random.choice([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"])\n",
        "        answers['Q9'] = random.choice([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"])\n",
        "        answers['Q10'] = random.choice([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"])\n",
        "        answers['Q11'] = random.choice([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"])\n",
        "        answers['Q12'] = random.choice([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\"])\n",
        "        answers['Q13'] = random.choice([\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\"])\n",
        "\n",
        "    return answers, dominant\n",
        "\n",
        "# ØªØ´ØºÙŠÙ„ Ø§Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠ\n",
        "print(\"\\nØ¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù ØªØ¹Ø±ÙŠÙ Ù…Ø³ØªØ®Ø¯Ù… Ø¹Ø´ÙˆØ§Ø¦ÙŠ...\")\n",
        "user_answers, true_dominant = create_random_user_profile_ar()\n",
        "\n",
        "print(f\"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡ÙŠÙ…Ù† Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ (Ù„Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠ): {true_dominant}\")\n",
        "print(\"\\nØªØ´ØºÙŠÙ„ Ø§Ù„ØªÙ†Ø¨Ø¤...\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "try:\n",
        "    result = predictor_ar.predict_with_confidence(user_answers)\n",
        "    pred = result['prediction']\n",
        "    analysis = result['analysis']\n",
        "\n",
        "    print(f\"ğŸ¯ Ø§Ù„ØªÙ†Ø¨Ø¤:\")\n",
        "    print(f\"   Ø§Ù„Ø´Ø®ØµÙŠØ© Ø§Ù„Ø£ÙˆÙ„Ù‰: {pred['top_character']}\")\n",
        "    print(f\"   Ø§Ù„Ø«Ù‚Ø©: {pred['confidence_percent']} ({pred['confidence_label']})\")\n",
        "    print(f\"   Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·: {pred['pattern_type'].upper()}\")\n",
        "    print(f\"   Ø§Ù„ÙŠÙ‚ÙŠÙ†: {pred['certainty_level']}\")\n",
        "    print(f\"   Ø£ÙØ¶Ù„ 3: {', '.join(pred['top_3_characters'])}\")\n",
        "\n",
        "    print(f\"\\nğŸ” Ø§Ù„ØªØ­Ù„ÙŠÙ„:\")\n",
        "    print(f\"   Ù‚ÙˆØ© Ø§Ù„Ø£Ø¯Ù„Ø©: {analysis['evidence_strength'].upper()}\")\n",
        "    print(f\"   ÙˆØ¶ÙˆØ­ Ø§Ù„Ù†Ù…Ø·: {analysis['pattern_clarity']:.2f}\")\n",
        "    print(f\"   Ø¯Ø±Ø¬Ø© Ø§Ù„ØºÙ…ÙˆØ¶: {analysis['ambiguity_score']:.2f}\")\n",
        "\n",
        "    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¯Ù‚Ø© (Ù„Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠ ÙÙ‚Ø·)\n",
        "    if pred['top_character'] == true_dominant:\n",
        "        print(f\"\\nâœ… ØµØ­ÙŠØ­: ØªÙ… Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡ÙŠÙ…Ù† Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ!\")\n",
        "    else:\n",
        "        print(f\"\\nâš ï¸  Ù…Ø®ØªÙ„Ù: ØªÙ… Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ù€ {pred['top_character']} Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† {true_dominant}\")\n",
        "        if true_dominant in pred['top_3_characters']:\n",
        "            print(f\"   (Ù„ÙƒÙ† {true_dominant} Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø£ÙØ¶Ù„ 3)\")\n",
        "\n",
        "    # Ø¹Ø±Ø¶ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ø§Ù„Ø£ÙˆÙ„ÙŠØ© Ù„Ø£ÙØ¶Ù„ 5\n",
        "    print(f\"\\nğŸ“Š Ø£ÙØ¶Ù„ 5 Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª:\")\n",
        "    all_probs = result['raw_data']['all_probabilities']\n",
        "    sorted_probs = sorted(all_probs.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "    for char, prob in sorted_probs:\n",
        "        print(f\"   {char:20}: {prob:.1%}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¹Ø±Ø¶ Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠ: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠ Ø§Ù„Ø«Ø§Ø¨Øª Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…ÙƒØªÙ…Ù„\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nØ§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„Ù…Ù†ÙØ°Ø©:\")\n",
        "print(\"âœ… Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù„ÙØ¦Ø§Øª Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù€ 18 Ø´Ø®ØµÙŠØ©\")\n",
        "print(\"âœ… Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ø«Ù‚Ø© Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠØ© Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·\")\n",
        "print(\"âœ… ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚ÙˆØ© Ø§Ù„Ù‚Ø§Ø¦Ù… Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¯Ù„Ø©\")\n",
        "print(\"âœ… Ù†Ø¸Ø§Ù… ØªØ¯Ù‚ÙŠÙ‚ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª ÙˆØ§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª\")\n",
        "print(\"âœ… ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù†ÙØ³ÙŠØ©\")\n",
        "print(\"âœ… Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¥Ù†ØªØ§Ø¬ Ù…Ø¹ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù„Ø£Ø®Ø·Ø§Ø¡\")\n",
        "print(\"\\nğŸš€ Ø¬Ø§Ù‡Ø² Ù„Ù„Ù†Ø´Ø±!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpqvjBmIBtkW",
        "outputId": "0b8694f8-7a93-4e78-fc47-571d3ba43111"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "ğŸ§ª 10 Ø­Ø§Ù„Ø§Øª Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ø¯Ø¯Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¹Ø±Ø¨ÙŠ\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "ğŸš€ ØªØ´ØºÙŠÙ„ 10 Ø­Ø§Ù„Ø© Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ø¯Ø¯Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹\n",
            "================================================================================\n",
            "\n",
            "ğŸ”¬ Ø§Ø®ØªØ¨Ø§Ø± 1: Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ Ø§Ù„ÙˆØ§Ø¶Ø­ Ø¬Ø¯Ø§Ù‹ (Ù†Ù…ÙˆØ°Ø¬ÙŠ)\n",
            "   ğŸ“ Ø´Ø®ØµÙŠØ© ÙƒÙ…Ø§Ù„ÙŠØ© ÙˆØ§Ø¶Ø­Ø© Ù…Ø¹ Ø¥Ø¬Ø§Ø¨Ø§Øª Ù…ØªØ³Ù‚Ø©ØŒ Ù…ØªÙˆÙ‚Ø¹ Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹\n",
            "   ğŸ¯ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\n",
            "------------------------------------------------------------\n",
            "   ğŸ¯ Ø§Ù„ØªÙ†Ø¨Ø¤: Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\n",
            "   ğŸ“Š Ø§Ù„Ø«Ù‚Ø©: 85.0% (Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©)\n",
            "   ğŸ·ï¸  Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·: very_clear\n",
            "   ğŸ’ª Ù‚ÙˆØ© Ø§Ù„Ø£Ø¯Ù„Ø©: very_strong\n",
            "   âœ… Ø¯Ù‚Ø©: ØµØ­ÙŠØ­! (ÙÙŠ Ø§Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ø£ÙˆÙ„)\n",
            "   ğŸ¥‡ Ø£ÙØ¶Ù„ 3:\n",
            "      1. Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ (44.0%)\n",
            "      2. Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù… (22.8%)\n",
            "      3. Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„ (14.7%)\n",
            "   ğŸ§  ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù‚Ø¹ÙŠØ©:\n",
            "      âœ… Ø§Ù„Ø«Ù‚Ø© ÙˆØ§Ù‚Ø¹ÙŠØ© Ù„Ù„Ù†Ù…Ø· very_clear\n",
            "\n",
            "ğŸ”¬ Ø§Ø®ØªØ¨Ø§Ø± 2: Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ + Ø§Ù„Ø®Ø¬ÙˆÙ„ (Ù…Ø®ØªÙ„Ø· Ù‚ÙˆÙŠ)\n",
            "   ğŸ“ Ù…Ø²ÙŠØ¬ Ù‚ÙˆÙŠ Ù…Ù† Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ ÙˆØ§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø¬ÙˆÙ„ØŒ Ø«Ù‚Ø© Ù…ØªÙˆØ³Ø·Ø©\n",
            "   ğŸ¯ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ\n",
            "------------------------------------------------------------\n",
            "   ğŸ¯ Ø§Ù„ØªÙ†Ø¨Ø¤: Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ\n",
            "   ğŸ“Š Ø§Ù„Ø«Ù‚Ø©: 85.0% (Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©)\n",
            "   ğŸ·ï¸  Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·: very_clear\n",
            "   ğŸ’ª Ù‚ÙˆØ© Ø§Ù„Ø£Ø¯Ù„Ø©: very_strong\n",
            "   âœ… Ø¯Ù‚Ø©: ØµØ­ÙŠØ­! (ÙÙŠ Ø§Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ø£ÙˆÙ„)\n",
            "   ğŸ¥‡ Ø£ÙØ¶Ù„ 3:\n",
            "      1. Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ (41.3%)\n",
            "      2. Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø¬ÙˆÙ„ (37.9%)\n",
            "      3. Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø¯ (9.5%)\n",
            "   ğŸ§  ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù‚Ø¹ÙŠØ©:\n",
            "      âœ… Ø§Ù„Ø«Ù‚Ø© ÙˆØ§Ù‚Ø¹ÙŠØ© Ù„Ù„Ù†Ù…Ø· very_clear\n",
            "\n",
            "ğŸ”¬ Ø§Ø®ØªØ¨Ø§Ø± 3: Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³ + Ø§Ù„Ù…Ø¹ØªÙ…Ø¯ (Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ Ù‚ÙˆÙŠ)\n",
            "   ğŸ“ ØªØ±ÙƒÙŠØ² Ù‚ÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ© ÙˆØ§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†\n",
            "   ğŸ¯ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³\n",
            "------------------------------------------------------------\n",
            "   ğŸ¯ Ø§Ù„ØªÙ†Ø¨Ø¤: Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯\n",
            "   ğŸ“Š Ø§Ù„Ø«Ù‚Ø©: 93.2% (Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹)\n",
            "   ğŸ·ï¸  Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·: very_clear\n",
            "   ğŸ’ª Ù‚ÙˆØ© Ø§Ù„Ø£Ø¯Ù„Ø©: very_strong\n",
            "   âš ï¸  Ø¯Ù‚Ø©: Ø¬Ø²Ø¦ÙŠ (ÙÙŠ Ø§Ù„Ù…Ø±ÙƒØ² 2)\n",
            "   ğŸ¥‡ Ø£ÙØ¶Ù„ 3:\n",
            "      1. Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯ (97.9%)\n",
            "      2. Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³ (1.8%)\n",
            "      3. Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚ (0.1%)\n",
            "   ğŸ§  ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù‚Ø¹ÙŠØ©:\n",
            "      âœ… Ø§Ù„Ø«Ù‚Ø© ÙˆØ§Ù‚Ø¹ÙŠØ© Ù„Ù„Ù†Ù…Ø· very_clear\n",
            "\n",
            "ğŸ”¬ Ø§Ø®ØªØ¨Ø§Ø± 4: Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù… + Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ (ØªØ­ÙƒÙ… ÙƒØ§Ù…Ù„)\n",
            "   ğŸ“ Ø´Ø®ØµÙŠØ© Ù…ØªØ­ÙƒÙ…Ø© ÙˆÙƒÙ…Ø§Ù„ÙŠØ©ØŒ Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© ÙÙŠ Ø§Ù„ØªØ­ÙƒÙ… ÙˆØ§Ù„ÙƒÙ…Ø§Ù„ÙŠØ©\n",
            "   ğŸ¯ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\n",
            "------------------------------------------------------------\n",
            "   ğŸ¯ Ø§Ù„ØªÙ†Ø¨Ø¤: Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\n",
            "   ğŸ“Š Ø§Ù„Ø«Ù‚Ø©: 85.0% (Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©)\n",
            "   ğŸ·ï¸  Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·: very_clear\n",
            "   ğŸ’ª Ù‚ÙˆØ© Ø§Ù„Ø£Ø¯Ù„Ø©: very_strong\n",
            "   âœ… Ø¯Ù‚Ø©: ØµØ­ÙŠØ­! (ÙÙŠ Ø§Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ø£ÙˆÙ„)\n",
            "   ğŸ¥‡ Ø£ÙØ¶Ù„ 3:\n",
            "      1. Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù… (59.8%)\n",
            "      2. Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ (38.9%)\n",
            "      3. Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„ (0.5%)\n",
            "   ğŸ§  ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù‚Ø¹ÙŠØ©:\n",
            "      âœ… Ø§Ù„Ø«Ù‚Ø© ÙˆØ§Ù‚Ø¹ÙŠØ© Ù„Ù„Ù†Ù…Ø· very_clear\n",
            "\n",
            "ğŸ”¬ Ø§Ø®ØªØ¨Ø§Ø± 5: Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯ + Ø§Ù„Ù…Ù‡Ù…ÙÙ„ (ÙˆØ­Ø¯Ø© ÙˆØ¥Ù‡Ù…Ø§Ù„)\n",
            "   ğŸ“ Ù…Ø´Ø§Ø¹Ø± Ø¹Ù…ÙŠÙ‚Ø© Ø¨Ø§Ù„ÙˆØ­Ø¯Ø© ÙˆØ§Ù„Ø¥Ù‡Ù…Ø§Ù„ØŒ Ø£Ù†Ù…Ø§Ø· ÙˆØ§Ø¶Ø­Ø©\n",
            "   ğŸ¯ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯\n",
            "------------------------------------------------------------\n",
            "   ğŸ¯ Ø§Ù„ØªÙ†Ø¨Ø¤: Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯\n",
            "   ğŸ“Š Ø§Ù„Ø«Ù‚Ø©: 88.5% (Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©)\n",
            "   ğŸ·ï¸  Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·: very_clear\n",
            "   ğŸ’ª Ù‚ÙˆØ© Ø§Ù„Ø£Ø¯Ù„Ø©: very_strong\n",
            "   âœ… Ø¯Ù‚Ø©: ØµØ­ÙŠØ­! (ÙÙŠ Ø§Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ø£ÙˆÙ„)\n",
            "   ğŸ¥‡ Ø£ÙØ¶Ù„ 3:\n",
            "      1. Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯ (86.2%)\n",
            "      2. Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡Ù…ÙÙ„ (8.0%)\n",
            "      3. Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚ (1.9%)\n",
            "   ğŸ§  ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù‚Ø¹ÙŠØ©:\n",
            "      âœ… Ø§Ù„Ø«Ù‚Ø© ÙˆØ§Ù‚Ø¹ÙŠØ© Ù„Ù„Ù†Ù…Ø· very_clear\n",
            "\n",
            "ğŸ”¬ Ø§Ø®ØªØ¨Ø§Ø± 6: Ø§Ù„Ù…Ø¤Ø¬Ù‘Ù„ + Ø§Ù„Ù„Ø§Ø¹Ø¨ Ø§Ù„Ù…ÙØ±Ø· (Ù‡Ø±ÙˆØ¨ ÙˆØªØ£Ø¬ÙŠÙ„)\n",
            "   ğŸ“ Ù†Ù…Ø· ÙˆØ§Ø¶Ø­ Ù…Ù† Ø§Ù„ØªØ£Ø¬ÙŠÙ„ ÙˆØ§Ù„Ù‡Ø±ÙˆØ¨ Ø¹Ø¨Ø± Ø§Ù„Ø£Ù„Ø¹Ø§Ø¨ ÙˆØ§Ù„ØªØ³Ù„ÙŠØ©\n",
            "   ğŸ¯ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\n",
            "------------------------------------------------------------\n",
            "   ğŸ¯ Ø§Ù„ØªÙ†Ø¨Ø¤: Ø§Ù„Ù„Ø§Ø¹Ø¨ Ø§Ù„Ù…ÙØ±Ø·\n",
            "   ğŸ“Š Ø§Ù„Ø«Ù‚Ø©: 85.0% (Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©)\n",
            "   ğŸ·ï¸  Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·: very_clear\n",
            "   ğŸ’ª Ù‚ÙˆØ© Ø§Ù„Ø£Ø¯Ù„Ø©: very_strong\n",
            "   âš ï¸  Ø¯Ù‚Ø©: Ø¬Ø²Ø¦ÙŠ (ÙÙŠ Ø§Ù„Ù…Ø±ÙƒØ² 2)\n",
            "   ğŸ¥‡ Ø£ÙØ¶Ù„ 3:\n",
            "      1. Ø§Ù„Ù„Ø§Ø¹Ø¨ Ø§Ù„Ù…ÙØ±Ø· (47.9%)\n",
            "      2. Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„ (41.8%)\n",
            "      3. Ø§Ù„Ø¢ÙƒÙ„/Ø§Ù„Ù†Ù‡Ù… (10.2%)\n",
            "   ğŸ§  ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù‚Ø¹ÙŠØ©:\n",
            "      âœ… Ø§Ù„Ø«Ù‚Ø© ÙˆØ§Ù‚Ø¹ÙŠØ© Ù„Ù„Ù†Ù…Ø· very_clear\n",
            "\n",
            "ğŸ”¬ Ø§Ø®ØªØ¨Ø§Ø± 7: Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„ + Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ (Ø¹Ù…Ù„ ÙˆÙƒÙ…Ø§Ù„ÙŠØ©)\n",
            "   ğŸ“ ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù…Ù„ ÙˆØ§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ© Ù…Ø¹ ØµÙØ§Øª ÙƒÙ…Ø§Ù„ÙŠØ©\n",
            "   ğŸ¯ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\n",
            "------------------------------------------------------------\n",
            "   ğŸ¯ Ø§Ù„ØªÙ†Ø¨Ø¤: Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\n",
            "   ğŸ“Š Ø§Ù„Ø«Ù‚Ø©: 85.0% (Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©)\n",
            "   ğŸ·ï¸  Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·: very_clear\n",
            "   ğŸ’ª Ù‚ÙˆØ© Ø§Ù„Ø£Ø¯Ù„Ø©: very_strong\n",
            "   âš ï¸  Ø¯Ù‚Ø©: Ø¬Ø²Ø¦ÙŠ (ÙÙŠ Ø§Ù„Ù…Ø±ÙƒØ² 3)\n",
            "   ğŸ¥‡ Ø£ÙØ¶Ù„ 3:\n",
            "      1. Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù… (32.8%)\n",
            "      2. Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ (29.1%)\n",
            "      3. Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„ (18.9%)\n",
            "   ğŸ§  ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù‚Ø¹ÙŠØ©:\n",
            "      âœ… Ø§Ù„Ø«Ù‚Ø© ÙˆØ§Ù‚Ø¹ÙŠØ© Ù„Ù„Ù†Ù…Ø· very_clear\n",
            "\n",
            "ğŸ”¬ Ø§Ø®ØªØ¨Ø§Ø± 8: Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø­ÙŠØ±Ø§Ù† + Ø§Ù„Ù…Ù‡Ù…ÙÙ„ (Ø­ÙŠØ±Ø© ÙˆØ¥Ù‡Ù…Ø§Ù„)\n",
            "   ğŸ“ Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø­ÙŠØ±Ø© ÙˆØ§Ù„Ø§Ù†ÙØµØ§Ù„ Ø¹Ù† Ø§Ù„Ù…Ø´Ø§Ø¹Ø±\n",
            "   ğŸ¯ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø­ÙŠØ±Ø§Ù†\n",
            "------------------------------------------------------------\n",
            "   ğŸ¯ Ø§Ù„ØªÙ†Ø¨Ø¤: Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø­ÙŠØ±Ø§Ù†\n",
            "   ğŸ“Š Ø§Ù„Ø«Ù‚Ø©: 54.4% (Ø«Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø©)\n",
            "   ğŸ·ï¸  Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·: mixed\n",
            "   ğŸ’ª Ù‚ÙˆØ© Ø§Ù„Ø£Ø¯Ù„Ø©: very_strong\n",
            "   âœ… Ø¯Ù‚Ø©: ØµØ­ÙŠØ­! (ÙÙŠ Ø§Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ø£ÙˆÙ„)\n",
            "   ğŸ¥‡ Ø£ÙØ¶Ù„ 3:\n",
            "      1. Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø­ÙŠØ±Ø§Ù† (55.4%)\n",
            "      2. Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„ (13.3%)\n",
            "      3. Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¨Ø§Ø±Ø¯ (7.9%)\n",
            "   ğŸ§  ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù‚Ø¹ÙŠØ©:\n",
            "      âœ… Ø§Ù„Ø«Ù‚Ø© ÙˆØ§Ù‚Ø¹ÙŠØ© Ù„Ù„Ù†Ù…Ø· mixed\n",
            "\n",
            "ğŸ”¬ Ø§Ø®ØªØ¨Ø§Ø± 9: Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­ + Ø§Ù„Ø®Ø¬ÙˆÙ„ (Ø¬Ø±Ø§Ø­ ÙˆØ®Ø²ÙŠ)\n",
            "   ğŸ“ Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø¬Ø±Ø­ ÙˆØ§Ù„Ø®Ø²ÙŠ Ù…Ù† Ø§Ù„Ø·ÙÙˆÙ„Ø©ØŒ Ø­Ø³Ø§Ø³ÙŠØ© Ø¹Ø§Ù„ÙŠØ©\n",
            "   ğŸ¯ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\n",
            "------------------------------------------------------------\n",
            "   ğŸ¯ Ø§Ù„ØªÙ†Ø¨Ø¤: Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚\n",
            "   ğŸ“Š Ø§Ù„Ø«Ù‚Ø©: 50.0% (Ø«Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø©)\n",
            "   ğŸ·ï¸  Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·: mixed\n",
            "   ğŸ’ª Ù‚ÙˆØ© Ø§Ù„Ø£Ø¯Ù„Ø©: strong\n",
            "   âš ï¸  Ø¯Ù‚Ø©: Ø¬Ø²Ø¦ÙŠ (ÙÙŠ Ø§Ù„Ù…Ø±ÙƒØ² 2)\n",
            "   ğŸ¥‡ Ø£ÙØ¶Ù„ 3:\n",
            "      1. Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚ (29.2%)\n",
            "      2. Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­ (18.4%)\n",
            "      3. Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø¬ÙˆÙ„ (14.1%)\n",
            "   ğŸ§  ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù‚Ø¹ÙŠØ©:\n",
            "      âœ… Ø§Ù„Ø«Ù‚Ø© ÙˆØ§Ù‚Ø¹ÙŠØ© Ù„Ù„Ù†Ù…Ø· mixed\n",
            "\n",
            "ğŸ”¬ Ø§Ø®ØªØ¨Ø§Ø± 10: Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚ + Ø§Ù„Ø®Ø§ÙŠÙ (Ø¥Ø±Ù‡Ø§Ù‚ ÙˆØ®ÙˆÙ)\n",
            "   ğŸ“ Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø¥Ø±Ù‡Ø§Ù‚ ÙˆØ§Ù„Ø®ÙˆÙ Ù…Ø¹ Ø§Ø³ØªØ¬Ø§Ø¨Ø§Øª Ø¹Ø§Ø·ÙÙŠØ© Ù‚ÙˆÙŠØ©\n",
            "   ğŸ¯ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚\n",
            "------------------------------------------------------------\n",
            "   ğŸ¯ Ø§Ù„ØªÙ†Ø¨Ø¤: Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚\n",
            "   ğŸ“Š Ø§Ù„Ø«Ù‚Ø©: 70.0% (Ø«Ù‚Ø© Ù…ØªÙˆØ³Ø·Ø©)\n",
            "   ğŸ·ï¸  Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·: mixed\n",
            "   ğŸ’ª Ù‚ÙˆØ© Ø§Ù„Ø£Ø¯Ù„Ø©: very_strong\n",
            "   âœ… Ø¯Ù‚Ø©: ØµØ­ÙŠØ­! (ÙÙŠ Ø§Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ø£ÙˆÙ„)\n",
            "   ğŸ¥‡ Ø£ÙØ¶Ù„ 3:\n",
            "      1. Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚ (94.5%)\n",
            "      2. Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§ÙŠÙ (3.5%)\n",
            "      3. Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„ (0.7%)\n",
            "   âš ï¸  ØªØ­Ø°ÙŠØ±Ø§Øª:\n",
            "      â€¢ ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø£Ù†Ù…Ø§Ø· Ù…ØªÙ†Ø§Ù‚Ø¶Ø©: Ø§Ù„Ø¨Ø§Ø±Ø¯-Ø§Ù„Ù…Ø±Ù‡Ù‚\n",
            "   ğŸ§  ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù‚Ø¹ÙŠØ©:\n",
            "      âœ… Ø§Ù„Ø«Ù‚Ø© ÙˆØ§Ù‚Ø¹ÙŠØ© Ù„Ù„Ù†Ù…Ø· mixed\n",
            "\n",
            "================================================================================\n",
            "ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹\n",
            "================================================================================\n",
            "\n",
            "ğŸ“ˆ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ø§Ù…Ø©:\n",
            "   Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª: 10\n",
            "   Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù†Ø§Ø¬Ø­Ø©: 10\n",
            "\n",
            "ğŸ¯ Ø¯Ù‚Ø© Ø§Ù„ØªÙ†Ø¨Ø¤:\n",
            "   âœ… ÙÙŠ Ø§Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ø£ÙˆÙ„: 6/10 (60.0%)\n",
            "   ğŸ¥‡ ÙÙŠ Ø£ÙØ¶Ù„ 3: 10/10 (100.0%)\n",
            "\n",
            "ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø«Ù‚Ø©:\n",
            "   ğŸ“ˆ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø«Ù‚Ø©: 78.1%\n",
            "   ğŸ“‰ Ø£Ù‚Ù„ Ø«Ù‚Ø©: 50.0%\n",
            "   ğŸ“ˆ Ø£Ø¹Ù„Ù‰ Ø«Ù‚Ø©: 93.2%\n",
            "   âœ… ÙˆØ§Ù‚Ø¹ÙŠØ© Ø§Ù„Ø«Ù‚Ø©: 10/10 (100.0%)\n",
            "\n",
            "ğŸ” ØªÙˆØ²ÙŠØ¹ Ù‚ÙˆØ© Ø§Ù„Ø£Ø¯Ù„Ø©:\n",
            "   VERY_STRONG    : 9 Ø§Ø®ØªØ¨Ø§Ø± (90.0%)\n",
            "   STRONG         : 1 Ø§Ø®ØªØ¨Ø§Ø± (10.0%)\n",
            "\n",
            "ğŸ­ ØªÙˆØ²ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ù†Ù…Ø§Ø·:\n",
            "   VERY_CLEAR     : 7 Ø§Ø®ØªØ¨Ø§Ø± (70.0%)\n",
            "   MIXED          : 3 Ø§Ø®ØªØ¨Ø§Ø± (30.0%)\n",
            "\n",
            "ğŸ“ˆ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø«Ù‚Ø© Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·:\n",
            "   VERY_CLEAR     : 86.7% (Ø§Ù„Ù…Ø¯Ù‰: 85.0%-93.2%)\n",
            "   MIXED          : 58.1% (Ø§Ù„Ù…Ø¯Ù‰: 50.0%-70.0%)\n",
            "\n",
            "âš ï¸  Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª:\n",
            "   Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø¨Ù‡Ø§ ØªØ­Ø°ÙŠØ±Ø§Øª: 1/10\n",
            "   Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª: 1\n",
            "\n",
            "ğŸ† ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:\n",
            "   âš ï¸  Ø¬ÙŠØ¯: Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ø¬ÙŠØ¯\n",
            "   ğŸ“ˆ ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„Ø«Ù‚Ø©\n",
            "\n",
            "ğŸ“‹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ© Ù„ÙƒÙ„ Ø§Ø®ØªØ¨Ø§Ø±:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "ID  Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±                       Ø§Ù„Ù…ØªÙˆÙ‚Ø¹         Ø§Ù„ØªÙ†Ø¨Ø¤          Ø§Ù„Ø¯Ù‚Ø©      Ø§Ù„Ø«Ù‚Ø©           Ø§Ù„Ù†Ù…Ø·       \n",
            "----------------------------------------------------------------------------------------------------\n",
            "1   Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ Ø§Ù„ÙˆØ§Ø¶Ø­ Ø¬Ø¯Ø§Ù‹ (Ù†Ù…ÙˆØ°Ø¬ÙŠ)   Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ         Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ         âœ…ØµØ­ÙŠØ­      85.0%:<15 very_clear  \n",
            "2   Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ + Ø§Ù„Ø®Ø¬ÙˆÙ„ (Ù…Ø®Øª   Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„   Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„   âœ…ØµØ­ÙŠØ­      85.0%:<15 very_clear  \n",
            "3   Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³ + Ø§Ù„Ù…Ø¹ØªÙ…Ø¯ (Ø§Ø¬Øª   Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³   Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯   âŒØ§Ù„Ù…Ø±ÙƒØ² 2  93.2%:<15 very_clear  \n",
            "4   Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù… + Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ (ØªØ­ÙƒÙ… ÙƒØ§Ù…   Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…        Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…        âœ…ØµØ­ÙŠØ­      85.0%:<15 very_clear  \n",
            "5   Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯ + Ø§Ù„Ù…Ù‡Ù…ÙÙ„ (ÙˆØ­Ø¯Ø©   Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯    Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯    âœ…ØµØ­ÙŠØ­      88.5%:<15 very_clear  \n",
            "6   Ø§Ù„Ù…Ø¤Ø¬Ù‘Ù„ + Ø§Ù„Ù„Ø§Ø¹Ø¨ Ø§Ù„Ù…ÙØ±Ø· (Ù‡Ø±Ùˆ   Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„        Ø§Ù„Ù„Ø§Ø¹Ø¨ Ø§Ù„Ù…ÙØ±Ø·   âŒØ§Ù„Ù…Ø±ÙƒØ² 2  85.0%:<15 very_clear  \n",
            "7   Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„ + Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ (Ø¹Ù…Ù„ ÙˆÙƒ   Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„      Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…        âŒØ§Ù„Ù…Ø±ÙƒØ² 3  85.0%:<15 very_clear  \n",
            "8   Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø­ÙŠØ±Ø§Ù† + Ø§Ù„Ù…Ù‡Ù…ÙÙ„ (Ø­ÙŠØ±   Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø­ÙŠØ±Ø§Ù†   Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø­ÙŠØ±Ø§Ù†   âœ…ØµØ­ÙŠØ­      54.4%:<15 mixed       \n",
            "9   Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­ + Ø§Ù„Ø®Ø¬ÙˆÙ„ (Ø¬Ø±Ø§Ø­    Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­    Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚   âŒØ§Ù„Ù…Ø±ÙƒØ² 2  50.0%:<15 mixed       \n",
            "10  Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚ + Ø§Ù„Ø®Ø§ÙŠÙ (Ø¥Ø±Ù‡Ø§   Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚   Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚   âœ…ØµØ­ÙŠØ­      70.0%:<15 mixed       \n",
            "\n",
            "ğŸ’¡ ØªÙˆØµÙŠØ§Øª Ù„Ù„ØªØ­Ø³ÙŠÙ†:\n",
            "   1. ØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© Ø§Ù„ØªÙ†Ø¨Ø¤ Ù„Ù„Ø´Ø®ØµÙŠØ§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©\n",
            "   2. Ù…Ø±Ø§Ø¬Ø¹Ø© ØªÙˆÙ‚ÙŠØ¹Ø§Øª Ø§Ù„Ø´Ø®ØµÙŠØ§Øª Ù„Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ø®ØªÙ„Ø·Ø©\n",
            "   5. ØªØ­Ø³ÙŠÙ† Ù†Ø¸Ø§Ù… ØªØ¯Ù‚ÙŠÙ‚ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ù„Ù„Ø­Ø¯ Ù…Ù† Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª Ø§Ù„ÙƒØ§Ø°Ø¨Ø©\n",
            "   6. Ø¬Ù…Ø¹ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù†Ø§Ø¯Ø±Ø©\n",
            "   7. Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø­Ù‚ÙŠÙ‚ÙŠÙŠÙ† Ù„Ø¬Ù…Ø¹ Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª\n",
            "\n",
            "================================================================================\n",
            "ğŸ’¾ Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± ÙˆØ§Ù„Ù†ØªØ§Ø¦Ø¬\n",
            "================================================================================\n",
            "âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªÙØµÙŠÙ„ÙŠ ÙÙŠ: test_results_detailed_report_ar.csv\n",
            "âœ… ØªÙ… Ø­ÙØ¸ Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ: test_results_summary_ar.csv\n",
            "\n",
            "ğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø³Ø±ÙŠØ¹Ø©:\n",
            "   - Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª: 10\n",
            "   - ØªÙ… ØªÙ†ÙÙŠØ°Ù‡Ø§ Ø¨Ù†Ø¬Ø§Ø­: 10\n",
            "   - Ù…Ø¯Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨: Ø­ÙˆØ§Ù„ÙŠ 2-5 Ø¯Ù‚Ø§Ø¦Ù‚\n",
            "   - Ø­Ø¬Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: ~10-20 Ù…ÙŠØ¬Ø§Ø¨Ø§ÙŠØª\n",
            "   - ÙˆÙ‚Øª Ø§Ù„ØªÙ†Ø¨Ø¤: <1 Ø«Ø§Ù†ÙŠØ© Ù„ÙƒÙ„ Ø§Ø®ØªØ¨Ø§Ø±\n",
            "\n",
            "================================================================================\n",
            "ğŸ‰ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§ÙƒØªÙ…Ù„ Ø¨Ù†Ø¬Ø§Ø­!\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ==================== 13. Ø­Ø§Ù„Ø§Øª Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ø¯Ø¯Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¹Ø±Ø¨ÙŠ ====================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ§ª 10 Ø­Ø§Ù„Ø§Øª Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ø¯Ø¯Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¹Ø±Ø¨ÙŠ\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "predefined_test_cases_ar = [\n",
        "    {\n",
        "        \"id\": 1,\n",
        "        \"name\": \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ Ø§Ù„ÙˆØ§Ø¶Ø­ Ø¬Ø¯Ø§Ù‹ (Ù†Ù…ÙˆØ°Ø¬ÙŠ)\",\n",
        "        \"description\": \"Ø´Ø®ØµÙŠØ© ÙƒÙ…Ø§Ù„ÙŠØ© ÙˆØ§Ø¶Ø­Ø© Ù…Ø¹ Ø¥Ø¬Ø§Ø¨Ø§Øª Ù…ØªØ³Ù‚Ø©ØŒ Ù…ØªÙˆÙ‚Ø¹ Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹\",\n",
        "        \"dominant_character\": \"Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ\",\n",
        "        \"answers\": {\n",
        "            \"Q1\": \"0\", \"Q2\": \"81-100%\", \"Q3\": \"0\", \"Q4\": \"0-20%\",\n",
        "            \"Q5\": \"0\", \"Q6\": \"0\", \"Q7\": \"0\", \"Q8\": \"0-20%\",\n",
        "            \"Q9\": \"3\", \"Q10\": \"1\", \"Q11\": \"0\", \"Q12\": \"0\", \"Q13\": \"2\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"id\": 2,\n",
        "        \"name\": \"Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ + Ø§Ù„Ø®Ø¬ÙˆÙ„ (Ù…Ø®ØªÙ„Ø· Ù‚ÙˆÙŠ)\",\n",
        "        \"description\": \"Ù…Ø²ÙŠØ¬ Ù‚ÙˆÙŠ Ù…Ù† Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ ÙˆØ§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø¬ÙˆÙ„ØŒ Ø«Ù‚Ø© Ù…ØªÙˆØ³Ø·Ø©\",\n",
        "        \"dominant_character\": \"Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ\",\n",
        "        \"answers\": {\n",
        "            \"Q1\": \"0,4\", \"Q2\": \"81-100%\", \"Q3\": \"3,5\", \"Q4\": \"21-50%\",\n",
        "            \"Q5\": \"0,4\", \"Q6\": \"0,5\", \"Q7\": \"5\", \"Q8\": \"0-20%\",\n",
        "            \"Q9\": \"2\", \"Q10\": \"5\", \"Q11\": \"0\", \"Q12\": \"0,4\", \"Q13\": \"5\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"id\": 3,\n",
        "        \"name\": \"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³ + Ø§Ù„Ù…Ø¹ØªÙ…Ø¯ (Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ Ù‚ÙˆÙŠ)\",\n",
        "        \"description\": \"ØªØ±ÙƒÙŠØ² Ù‚ÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ© ÙˆØ§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†\",\n",
        "        \"dominant_character\": \"Ø§Ù„Ù…ÙØ±Ø¶ÙŠ Ù„Ù„Ù†Ø§Ø³\",\n",
        "        \"answers\": {\n",
        "            \"Q1\": \"2\", \"Q2\": \"51-80%\", \"Q3\": \"3\", \"Q4\": \"51-80%\",\n",
        "            \"Q5\": \"2\", \"Q6\": \"2,3\", \"Q7\": \"3\", \"Q8\": \"21-50%\",\n",
        "            \"Q9\": \"0,5\", \"Q10\": \"0\", \"Q11\": \"4\", \"Q12\": \"4\", \"Q13\": \"3,6\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"id\": 4,\n",
        "        \"name\": \"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù… + Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ (ØªØ­ÙƒÙ… ÙƒØ§Ù…Ù„)\",\n",
        "        \"description\": \"Ø´Ø®ØµÙŠØ© Ù…ØªØ­ÙƒÙ…Ø© ÙˆÙƒÙ…Ø§Ù„ÙŠØ©ØŒ Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© ÙÙŠ Ø§Ù„ØªØ­ÙƒÙ… ÙˆØ§Ù„ÙƒÙ…Ø§Ù„ÙŠØ©\",\n",
        "        \"dominant_character\": \"Ø§Ù„Ù…ØªØ­ÙƒÙ‘Ù…\",\n",
        "        \"answers\": {\n",
        "            \"Q1\": \"0\", \"Q2\": \"81-100%\", \"Q3\": \"0\", \"Q4\": \"0-20%\",\n",
        "            \"Q5\": \"0,2\", \"Q6\": \"0\", \"Q7\": \"0\", \"Q8\": \"0-20%\",\n",
        "            \"Q9\": \"3\", \"Q10\": \"1\", \"Q11\": \"0,2\", \"Q12\": \"0\", \"Q13\": \"2\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"id\": 5,\n",
        "        \"name\": \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯ + Ø§Ù„Ù…Ù‡Ù…ÙÙ„ (ÙˆØ­Ø¯Ø© ÙˆØ¥Ù‡Ù…Ø§Ù„)\",\n",
        "        \"description\": \"Ù…Ø´Ø§Ø¹Ø± Ø¹Ù…ÙŠÙ‚Ø© Ø¨Ø§Ù„ÙˆØ­Ø¯Ø© ÙˆØ§Ù„Ø¥Ù‡Ù…Ø§Ù„ØŒ Ø£Ù†Ù…Ø§Ø· ÙˆØ§Ø¶Ø­Ø©\",\n",
        "        \"dominant_character\": \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯\",\n",
        "        \"answers\": {\n",
        "            \"Q1\": \"5\", \"Q2\": \"21-50%\", \"Q3\": \"2,4\", \"Q4\": \"81-100%\",\n",
        "            \"Q5\": \"1\", \"Q6\": \"3\", \"Q7\": \"2\", \"Q8\": \"51-80%\",\n",
        "            \"Q9\": \"1,4\", \"Q10\": \"4\", \"Q11\": \"1,3\", \"Q12\": \"3\", \"Q13\": \"0\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"id\": 6,\n",
        "        \"name\": \"Ø§Ù„Ù…Ø¤Ø¬Ù‘Ù„ + Ø§Ù„Ù„Ø§Ø¹Ø¨ Ø§Ù„Ù…ÙØ±Ø· (Ù‡Ø±ÙˆØ¨ ÙˆØªØ£Ø¬ÙŠÙ„)\",\n",
        "        \"description\": \"Ù†Ù…Ø· ÙˆØ§Ø¶Ø­ Ù…Ù† Ø§Ù„ØªØ£Ø¬ÙŠÙ„ ÙˆØ§Ù„Ù‡Ø±ÙˆØ¨ Ø¹Ø¨Ø± Ø§Ù„Ø£Ù„Ø¹Ø§Ø¨ ÙˆØ§Ù„ØªØ³Ù„ÙŠØ©\",\n",
        "        \"dominant_character\": \"Ø§Ù„Ù…Ø¤Ø¬Ù‘ÙÙ„\",\n",
        "        \"answers\": {\n",
        "            \"Q1\": \"3\", \"Q2\": \"0-20%\", \"Q3\": \"1\", \"Q4\": \"21-50%\",\n",
        "            \"Q5\": \"3\", \"Q6\": \"4\", \"Q7\": \"1,4\", \"Q8\": \"81-100%\",\n",
        "            \"Q9\": \"4\", \"Q10\": \"4\", \"Q11\": \"5\", \"Q12\": \"5\", \"Q13\": \"4\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"id\": 7,\n",
        "        \"name\": \"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„ + Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ (Ø¹Ù…Ù„ ÙˆÙƒÙ…Ø§Ù„ÙŠØ©)\",\n",
        "        \"description\": \"ØªØ±ÙƒÙŠØ² Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù…Ù„ ÙˆØ§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ© Ù…Ø¹ ØµÙØ§Øª ÙƒÙ…Ø§Ù„ÙŠØ©\",\n",
        "        \"dominant_character\": \"Ù…Ø¯Ù…Ù† Ø§Ù„Ø´ØºÙ„\",\n",
        "        \"answers\": {\n",
        "            \"Q1\": \"0,1\", \"Q2\": \"81-100%\", \"Q3\": \"0\", \"Q4\": \"0-20%\",\n",
        "            \"Q5\": \"0\", \"Q6\": \"0\", \"Q7\": \"0\", \"Q8\": \"0-20%\",\n",
        "            \"Q9\": \"3\", \"Q10\": \"1\", \"Q11\": \"0\", \"Q12\": \"0\", \"Q13\": \"2\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"id\": 8,\n",
        "        \"name\": \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø­ÙŠØ±Ø§Ù† + Ø§Ù„Ù…Ù‡Ù…ÙÙ„ (Ø­ÙŠØ±Ø© ÙˆØ¥Ù‡Ù…Ø§Ù„)\",\n",
        "        \"description\": \"Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø­ÙŠØ±Ø© ÙˆØ§Ù„Ø§Ù†ÙØµØ§Ù„ Ø¹Ù† Ø§Ù„Ù…Ø´Ø§Ø¹Ø±\",\n",
        "        \"dominant_character\": \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø­ÙŠØ±Ø§Ù†\",\n",
        "        \"answers\": {\n",
        "            \"Q1\": \"5\", \"Q2\": \"21-50%\", \"Q3\": \"4\", \"Q4\": \"51-80%\",\n",
        "            \"Q5\": \"5\", \"Q6\": \"3\", \"Q7\": \"2\", \"Q8\": \"21-50%\",\n",
        "            \"Q9\": \"4\", \"Q10\": \"4\", \"Q11\": \"3,5\", \"Q12\": \"5\", \"Q13\": \"7\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"id\": 9,\n",
        "        \"name\": \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­ + Ø§Ù„Ø®Ø¬ÙˆÙ„ (Ø¬Ø±Ø§Ø­ ÙˆØ®Ø²ÙŠ)\",\n",
        "        \"description\": \"Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø¬Ø±Ø­ ÙˆØ§Ù„Ø®Ø²ÙŠ Ù…Ù† Ø§Ù„Ø·ÙÙˆÙ„Ø©ØŒ Ø­Ø³Ø§Ø³ÙŠØ© Ø¹Ø§Ù„ÙŠØ©\",\n",
        "        \"dominant_character\": \"Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¬Ø±ÙŠØ­\",\n",
        "        \"answers\": {\n",
        "            \"Q1\": \"2,5\", \"Q2\": \"21-50%\", \"Q3\": \"2,5\", \"Q4\": \"51-80%\",\n",
        "            \"Q5\": \"4,5\", \"Q6\": \"5\", \"Q7\": \"2,5\", \"Q8\": \"51-80%\",\n",
        "            \"Q9\": \"4\", \"Q10\": \"5\", \"Q11\": \"0,1\", \"Q12\": \"1\", \"Q13\": \"0,6\"\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"id\": 10,\n",
        "        \"name\": \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚ + Ø§Ù„Ø®Ø§ÙŠÙ (Ø¥Ø±Ù‡Ø§Ù‚ ÙˆØ®ÙˆÙ)\",\n",
        "        \"description\": \"Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø¥Ø±Ù‡Ø§Ù‚ ÙˆØ§Ù„Ø®ÙˆÙ Ù…Ø¹ Ø§Ø³ØªØ¬Ø§Ø¨Ø§Øª Ø¹Ø§Ø·ÙÙŠØ© Ù‚ÙˆÙŠØ©\",\n",
        "        \"dominant_character\": \"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø±Ù‡ÙÙ‚\",\n",
        "        \"answers\": {\n",
        "            \"Q1\": \"4,5\", \"Q2\": \"21-50%\", \"Q3\": \"2\", \"Q4\": \"51-80%\",\n",
        "            \"Q5\": \"4\", \"Q6\": \"1,4\", \"Q7\": \"2\", \"Q8\": \"51-80%\",\n",
        "            \"Q9\": \"1,5\", \"Q10\": \"5\", \"Q11\": \"2\", \"Q12\": \"4\", \"Q13\": \"0\"\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "def run_predefined_tests(predictor, test_cases):\n",
        "    \"\"\"ØªØ´ØºÙŠÙ„ Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹\"\"\"\n",
        "    results = []\n",
        "    detailed_report = []\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"ğŸš€ ØªØ´ØºÙŠÙ„ {len(test_cases)} Ø­Ø§Ù„Ø© Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ø¯Ø¯Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    for test_case in test_cases:\n",
        "        print(f\"\\nğŸ”¬ Ø§Ø®ØªØ¨Ø§Ø± {test_case['id']}: {test_case['name']}\")\n",
        "        print(f\"   ğŸ“ {test_case['description']}\")\n",
        "        print(f\"   ğŸ¯ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: {test_case['dominant_character']}\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        try:\n",
        "            # Ø§Ù„ØªÙ†Ø¨Ø¤\n",
        "            result = predictor.predict_with_confidence(test_case['answers'])\n",
        "            pred = result['prediction']\n",
        "            analysis = result['analysis']\n",
        "            warnings = result['warnings']\n",
        "\n",
        "            # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
        "            print(f\"   ğŸ¯ Ø§Ù„ØªÙ†Ø¨Ø¤: {pred['top_character']}\")\n",
        "            print(f\"   ğŸ“Š Ø§Ù„Ø«Ù‚Ø©: {pred['confidence_percent']} ({pred['confidence_label']})\")\n",
        "            print(f\"   ğŸ·ï¸  Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·: {pred['pattern_type']}\")\n",
        "            print(f\"   ğŸ’ª Ù‚ÙˆØ© Ø§Ù„Ø£Ø¯Ù„Ø©: {analysis['evidence_strength']}\")\n",
        "\n",
        "            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¯Ù‚Ø©\n",
        "            correct_prediction = (pred['top_character'] == test_case['dominant_character'])\n",
        "            prediction_in_top3 = (test_case['dominant_character'] in pred['top_3_characters'])\n",
        "\n",
        "            if correct_prediction:\n",
        "                print(f\"   âœ… Ø¯Ù‚Ø©: ØµØ­ÙŠØ­! (ÙÙŠ Ø§Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ø£ÙˆÙ„)\")\n",
        "            elif prediction_in_top3:\n",
        "                rank = pred['top_3_characters'].index(test_case['dominant_character']) + 1\n",
        "                print(f\"   âš ï¸  Ø¯Ù‚Ø©: Ø¬Ø²Ø¦ÙŠ (ÙÙŠ Ø§Ù„Ù…Ø±ÙƒØ² {rank})\")\n",
        "            else:\n",
        "                print(f\"   âŒ Ø¯Ù‚Ø©: ØºÙŠØ± ØµØ­ÙŠØ­\")\n",
        "\n",
        "            # Ø¹Ø±Ø¶ Ø£ÙØ¶Ù„ 3\n",
        "            print(f\"   ğŸ¥‡ Ø£ÙØ¶Ù„ 3:\")\n",
        "            for i, char in enumerate(pred['top_3_characters'], 1):\n",
        "                prob_key = f'top{i}_prob'\n",
        "                prob = result['raw_data']['all_probabilities'].get(char, 0)\n",
        "                print(f\"      {i}. {char} ({prob:.1%})\")\n",
        "\n",
        "            # Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª\n",
        "            if warnings['has_warnings']:\n",
        "                print(f\"   âš ï¸  ØªØ­Ø°ÙŠØ±Ø§Øª:\")\n",
        "                for warning in warnings['validation_warnings']:\n",
        "                    print(f\"      â€¢ {warning}\")\n",
        "\n",
        "            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠØ© Ø§Ù„Ù†ÙØ³ÙŠØ©\n",
        "            print(f\"   ğŸ§  ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù‚Ø¹ÙŠØ©:\")\n",
        "\n",
        "            # Ù†Ø·Ø§Ù‚ Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹\n",
        "            if pred['pattern_type'] in ['very_clear', 'clear']:\n",
        "                expected_range = (0.75, 0.95)\n",
        "            elif pred['pattern_type'] == 'mixed':\n",
        "                expected_range = (0.50, 0.70)\n",
        "            elif pred['pattern_type'] == 'ambiguous':\n",
        "                expected_range = (0.30, 0.50)\n",
        "            else:\n",
        "                expected_range = (0.15, 0.35)\n",
        "\n",
        "            confidence = pred['confidence']\n",
        "            confidence_realistic = expected_range[0] <= confidence <= expected_range[1]\n",
        "\n",
        "            if confidence_realistic:\n",
        "                print(f\"      âœ… Ø§Ù„Ø«Ù‚Ø© ÙˆØ§Ù‚Ø¹ÙŠØ© Ù„Ù„Ù†Ù…Ø· {pred['pattern_type']}\")\n",
        "            else:\n",
        "                print(f\"      âš ï¸  Ø§Ù„Ø«Ù‚Ø© ØºÙŠØ± ÙˆØ§Ù‚Ø¹ÙŠØ© Ù„Ù„Ù†Ù…Ø· {pred['pattern_type']}\")\n",
        "                print(f\"         Ø§Ù„Ù…ØªÙˆÙ‚Ø¹: {expected_range[0]:.1%}-{expected_range[1]:.1%}\")\n",
        "                print(f\"         Ø§Ù„ÙØ¹Ù„ÙŠ: {confidence:.1%}\")\n",
        "\n",
        "            # Ø¬Ù…Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
        "            test_result = {\n",
        "                'id': test_case['id'],\n",
        "                'name': test_case['name'],\n",
        "                'expected': test_case['dominant_character'],\n",
        "                'predicted': pred['top_character'],\n",
        "                'confidence': confidence,\n",
        "                'pattern_type': pred['pattern_type'],\n",
        "                'evidence_strength': analysis['evidence_strength'],\n",
        "                'correct_prediction': correct_prediction,\n",
        "                'in_top3': prediction_in_top3,\n",
        "                'top3_rank': pred['top_3_characters'].index(test_case['dominant_character']) + 1\n",
        "                           if prediction_in_top3 else None,\n",
        "                'confidence_realistic': confidence_realistic,\n",
        "                'has_warnings': warnings['has_warnings'],\n",
        "                'warning_count': len(warnings['validation_warnings'])\n",
        "            }\n",
        "\n",
        "            results.append(test_result)\n",
        "\n",
        "            # ØªÙ‚Ø±ÙŠØ± Ù…ÙØµÙ„\n",
        "            detailed_report.append({\n",
        "                'test_id': test_case['id'],\n",
        "                'test_name': test_case['name'],\n",
        "                'expected': test_case['dominant_character'],\n",
        "                'predicted': pred['top_character'],\n",
        "                'top_3': pred['top_3_characters'],\n",
        "                'confidence': f\"{pred['confidence_percent']} ({pred['confidence_label']})\",\n",
        "                'pattern_type': pred['pattern_type'],\n",
        "                'evidence_strength': analysis['evidence_strength'],\n",
        "                'warnings': warnings['validation_warnings']\n",
        "            })\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {e}\")\n",
        "            results.append({\n",
        "                'id': test_case['id'],\n",
        "                'name': test_case['name'],\n",
        "                'error': str(e)\n",
        "            })\n",
        "\n",
        "    return results, detailed_report\n",
        "\n",
        "# ØªØ´ØºÙŠÙ„ Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±\n",
        "test_results, detailed_report = run_predefined_tests(predictor_ar, predefined_test_cases_ar)\n",
        "\n",
        "# ==================== 14. ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª ====================\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "if test_results:\n",
        "    # Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ø§Ù…Ø©\n",
        "    successful_tests = [r for r in test_results if 'error' not in r]\n",
        "    total_tests = len(test_results)\n",
        "    successful_count = len(successful_tests)\n",
        "\n",
        "    print(f\"\\nğŸ“ˆ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ø§Ù…Ø©:\")\n",
        "    print(f\"   Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª: {total_tests}\")\n",
        "    print(f\"   Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ù†Ø§Ø¬Ø­Ø©: {successful_count}\")\n",
        "\n",
        "    if successful_count > 0:\n",
        "        # Ø¯Ù‚Ø© Ø§Ù„ØªÙ†Ø¨Ø¤\n",
        "        correct_predictions = sum(1 for r in successful_tests if r['correct_prediction'])\n",
        "        in_top3_predictions = sum(1 for r in successful_tests if r['in_top3'])\n",
        "\n",
        "        accuracy_rate = (correct_predictions / successful_count) * 100\n",
        "        top3_accuracy_rate = (in_top3_predictions / successful_count) * 100\n",
        "\n",
        "        print(f\"\\nğŸ¯ Ø¯Ù‚Ø© Ø§Ù„ØªÙ†Ø¨Ø¤:\")\n",
        "        print(f\"   âœ… ÙÙŠ Ø§Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ø£ÙˆÙ„: {correct_predictions}/{successful_count} ({accuracy_rate:.1f}%)\")\n",
        "        print(f\"   ğŸ¥‡ ÙÙŠ Ø£ÙØ¶Ù„ 3: {in_top3_predictions}/{successful_count} ({top3_accuracy_rate:.1f}%)\")\n",
        "\n",
        "        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø«Ù‚Ø©\n",
        "        confidence_realistic_count = sum(1 for r in successful_tests if r.get('confidence_realistic', False))\n",
        "        confidence_realistic_rate = (confidence_realistic_count / successful_count) * 100\n",
        "\n",
        "        avg_confidence = np.mean([r['confidence'] for r in successful_tests]) * 100\n",
        "        min_confidence = min([r['confidence'] for r in successful_tests]) * 100\n",
        "        max_confidence = max([r['confidence'] for r in successful_tests]) * 100\n",
        "\n",
        "        print(f\"\\nğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø«Ù‚Ø©:\")\n",
        "        print(f\"   ğŸ“ˆ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø«Ù‚Ø©: {avg_confidence:.1f}%\")\n",
        "        print(f\"   ğŸ“‰ Ø£Ù‚Ù„ Ø«Ù‚Ø©: {min_confidence:.1f}%\")\n",
        "        print(f\"   ğŸ“ˆ Ø£Ø¹Ù„Ù‰ Ø«Ù‚Ø©: {max_confidence:.1f}%\")\n",
        "        print(f\"   âœ… ÙˆØ§Ù‚Ø¹ÙŠØ© Ø§Ù„Ø«Ù‚Ø©: {confidence_realistic_count}/{successful_count} ({confidence_realistic_rate:.1f}%)\")\n",
        "\n",
        "        # ØªÙˆØ²ÙŠØ¹ Ù‚ÙˆØ© Ø§Ù„Ø£Ø¯Ù„Ø©\n",
        "        print(f\"\\nğŸ” ØªÙˆØ²ÙŠØ¹ Ù‚ÙˆØ© Ø§Ù„Ø£Ø¯Ù„Ø©:\")\n",
        "        evidence_counts = Counter([r['evidence_strength'] for r in successful_tests])\n",
        "        for strength, count in evidence_counts.items():\n",
        "            percentage = (count / successful_count) * 100\n",
        "            print(f\"   {strength.upper():15}: {count} Ø§Ø®ØªØ¨Ø§Ø± ({percentage:.1f}%)\")\n",
        "\n",
        "        # ØªÙˆØ²ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ù†Ù…Ø§Ø·\n",
        "        print(f\"\\nğŸ­ ØªÙˆØ²ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ù†Ù…Ø§Ø·:\")\n",
        "        pattern_counts = Counter([r['pattern_type'] for r in successful_tests])\n",
        "        for pattern, count in pattern_counts.items():\n",
        "            percentage = (count / successful_count) * 100\n",
        "            print(f\"   {pattern.upper():15}: {count} Ø§Ø®ØªØ¨Ø§Ø± ({percentage:.1f}%)\")\n",
        "\n",
        "        # Ø§Ù„Ø«Ù‚Ø© Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·\n",
        "        print(f\"\\nğŸ“ˆ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø«Ù‚Ø© Ø­Ø³Ø¨ Ù†ÙˆØ¹ Ø§Ù„Ù†Ù…Ø·:\")\n",
        "        pattern_groups = {}\n",
        "        for result in successful_tests:\n",
        "            pattern = result['pattern_type']\n",
        "            if pattern not in pattern_groups:\n",
        "                pattern_groups[pattern] = []\n",
        "            pattern_groups[pattern].append(result['confidence'] * 100)\n",
        "\n",
        "        for pattern, confidences in pattern_groups.items():\n",
        "            if confidences:\n",
        "                avg = np.mean(confidences)\n",
        "                min_val = min(confidences)\n",
        "                max_val = max(confidences)\n",
        "                print(f\"   {pattern.upper():15}: {avg:.1f}% (Ø§Ù„Ù…Ø¯Ù‰: {min_val:.1f}%-{max_val:.1f}%)\")\n",
        "\n",
        "        # Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª\n",
        "        tests_with_warnings = sum(1 for r in successful_tests if r['has_warnings'])\n",
        "        total_warnings = sum(r['warning_count'] for r in successful_tests)\n",
        "\n",
        "        print(f\"\\nâš ï¸  Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª:\")\n",
        "        print(f\"   Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø¨Ù‡Ø§ ØªØ­Ø°ÙŠØ±Ø§Øª: {tests_with_warnings}/{successful_count}\")\n",
        "        print(f\"   Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª: {total_warnings}\")\n",
        "\n",
        "        # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\n",
        "        print(f\"\\nğŸ† ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:\")\n",
        "\n",
        "        if accuracy_rate >= 80 and confidence_realistic_rate >= 80:\n",
        "            print(\"   âœ… Ù…Ù…ØªØ§Ø²: Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ù…Ù…ØªØ§Ø²\")\n",
        "            print(\"   ğŸ¯ Ø§Ù„Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© ÙˆØ§Ù„Ø«Ù‚Ø© ÙˆØ§Ù‚Ø¹ÙŠØ©\")\n",
        "        elif accuracy_rate >= 60 and confidence_realistic_rate >= 60:\n",
        "            print(\"   âš ï¸  Ø¬ÙŠØ¯: Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ø¬ÙŠØ¯\")\n",
        "            print(\"   ğŸ“ˆ ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø¯Ù‚Ø© ÙˆØ§Ù„Ø«Ù‚Ø©\")\n",
        "        elif accuracy_rate >= 40:\n",
        "            print(\"   âš ï¸  Ù…Ù‚Ø¨ÙˆÙ„: Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø­Ø§Ø¬Ø© Ù„ØªØ­Ø³ÙŠÙ†\")\n",
        "            print(\"   ğŸ”§ Ø§Ù†Ø®ÙØ§Ø¶ ÙÙŠ Ø§Ù„Ø¯Ù‚Ø© Ø£Ùˆ ÙˆØ§Ù‚Ø¹ÙŠØ© Ø§Ù„Ø«Ù‚Ø©\")\n",
        "        else:\n",
        "            print(\"   âŒ ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ† ÙƒØ¨ÙŠØ±\")\n",
        "            print(\"   ğŸ› ï¸  Ø§Ù†Ø®ÙØ§Ø¶ Ø´Ø¯ÙŠØ¯ ÙÙŠ Ø§Ù„Ø¯Ù‚Ø©\")\n",
        "\n",
        "        # Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ© Ù„ÙƒÙ„ Ø§Ø®ØªØ¨Ø§Ø±\n",
        "        print(f\"\\nğŸ“‹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ© Ù„ÙƒÙ„ Ø§Ø®ØªØ¨Ø§Ø±:\")\n",
        "        print(\"-\" * 100)\n",
        "        print(f\"{'ID':<3} {'Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±':<30} {'Ø§Ù„Ù…ØªÙˆÙ‚Ø¹':<15} {'Ø§Ù„ØªÙ†Ø¨Ø¤':<15} {'Ø§Ù„Ø¯Ù‚Ø©':<10} {'Ø§Ù„Ø«Ù‚Ø©':<15} {'Ø§Ù„Ù†Ù…Ø·':<12}\")\n",
        "        print(\"-\" * 100)\n",
        "\n",
        "        for result in successful_tests:\n",
        "            accuracy_symbol = \"âœ…\" if result['correct_prediction'] else \"âŒ\"\n",
        "            accuracy_text = \"ØµØ­ÙŠØ­\" if result['correct_prediction'] else f\"Ø§Ù„Ù…Ø±ÙƒØ² {result['top3_rank']}\" if result['in_top3'] else \"ØºÙŠØ± ØµØ­ÙŠØ­\"\n",
        "\n",
        "            print(f\"{result['id']:<3} {result['name'][:28]:<30} \"\n",
        "                  f\"{result['expected'][:13]:<15} {result['predicted'][:13]:<15} \"\n",
        "                  f\"{accuracy_symbol + accuracy_text:<10} {result['confidence']:.1%}:<15 \"\n",
        "                  f\"{result['pattern_type']:<12}\")\n",
        "\n",
        "    else:\n",
        "        print(\"âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù†Ø§Ø¬Ø­Ø© Ù„Ù„ØªØ­Ù„ÙŠÙ„\")\n",
        "\n",
        "    # ØªÙˆØµÙŠØ§Øª Ù„Ù„ØªØ­Ø³ÙŠÙ†\n",
        "    print(f\"\\nğŸ’¡ ØªÙˆØµÙŠØ§Øª Ù„Ù„ØªØ­Ø³ÙŠÙ†:\")\n",
        "\n",
        "    successful_with_results = [r for r in successful_tests if 'confidence' in r]\n",
        "    if successful_with_results:\n",
        "        avg_acc = accuracy_rate if 'accuracy_rate' in locals() else 0\n",
        "        avg_conf_real = confidence_realistic_rate if 'confidence_realistic_rate' in locals() else 0\n",
        "\n",
        "        if avg_acc < 70:\n",
        "            print(\"   1. ØªØ­Ø³ÙŠÙ† Ø¯Ù‚Ø© Ø§Ù„ØªÙ†Ø¨Ø¤ Ù„Ù„Ø´Ø®ØµÙŠØ§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©\")\n",
        "            print(\"   2. Ù…Ø±Ø§Ø¬Ø¹Ø© ØªÙˆÙ‚ÙŠØ¹Ø§Øª Ø§Ù„Ø´Ø®ØµÙŠØ§Øª Ù„Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ø®ØªÙ„Ø·Ø©\")\n",
        "\n",
        "        if avg_conf_real < 70:\n",
        "            print(\"   3. Ø¶Ø¨Ø· Ù…Ø¹Ù„Ù…Ø§Øª Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ø«Ù‚Ø©\")\n",
        "            print(\"   4. ØªØ­Ø³ÙŠÙ† ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù†ÙØ³ÙŠØ©\")\n",
        "\n",
        "        if total_warnings > 0:\n",
        "            print(\"   5. ØªØ­Ø³ÙŠÙ† Ù†Ø¸Ø§Ù… ØªØ¯Ù‚ÙŠÙ‚ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª Ù„Ù„Ø­Ø¯ Ù…Ù† Ø§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª Ø§Ù„ÙƒØ§Ø°Ø¨Ø©\")\n",
        "\n",
        "        print(\"   6. Ø¬Ù…Ø¹ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù†Ø§Ø¯Ø±Ø©\")\n",
        "        print(\"   7. Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø­Ù‚ÙŠÙ‚ÙŠÙŠÙ† Ù„Ø¬Ù…Ø¹ Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù„Ù„ØªØ­Ù„ÙŠÙ„\")\n",
        "\n",
        "# ==================== 15. Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Ù…ÙØµÙ„ Ø¨Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ====================\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"ğŸ’¾ Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± ÙˆØ§Ù„Ù†ØªØ§Ø¦Ø¬\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "# Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªÙØµÙŠÙ„ÙŠ\n",
        "if detailed_report:\n",
        "    report_df = pd.DataFrame(detailed_report)\n",
        "    report_df.to_csv('test_results_detailed_report_ar.csv', index=False, encoding='utf-8-sig')\n",
        "    print(f\"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªÙØµÙŠÙ„ÙŠ ÙÙŠ: test_results_detailed_report_ar.csv\")\n",
        "\n",
        "# Ø­ÙØ¸ Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\n",
        "if test_results:\n",
        "    summary_data = []\n",
        "    for result in test_results:\n",
        "        if 'error' not in result:\n",
        "            summary_data.append({\n",
        "                'test_id': result['id'],\n",
        "                'test_name': result['name'],\n",
        "                'expected_character': result['expected'],\n",
        "                'predicted_character': result['predicted'],\n",
        "                'confidence': result['confidence'],\n",
        "                'pattern_type': result['pattern_type'],\n",
        "                'evidence_strength': result['evidence_strength'],\n",
        "                'correct_prediction': result['correct_prediction'],\n",
        "                'in_top3': result['in_top3'],\n",
        "                'confidence_realistic': result['confidence_realistic'],\n",
        "                'warning_count': result['warning_count']\n",
        "            })\n",
        "\n",
        "    if summary_data:\n",
        "        summary_df = pd.DataFrame(summary_data)\n",
        "        summary_df.to_csv('test_results_summary_ar.csv', index=False, encoding='utf-8-sig')\n",
        "        print(f\"âœ… ØªÙ… Ø­ÙØ¸ Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ: test_results_summary_ar.csv\")\n",
        "\n",
        "# Ø¹Ø±Ø¶ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø³Ø±ÙŠØ¹Ø©\n",
        "print(f\"\\nğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø³Ø±ÙŠØ¹Ø©:\")\n",
        "print(f\"   - Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª: {len(predefined_test_cases_ar)}\")\n",
        "print(f\"   - ØªÙ… ØªÙ†ÙÙŠØ°Ù‡Ø§ Ø¨Ù†Ø¬Ø§Ø­: {len([r for r in test_results if 'error' not in r])}\")\n",
        "print(f\"   - Ù…Ø¯Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨: Ø­ÙˆØ§Ù„ÙŠ 2-5 Ø¯Ù‚Ø§Ø¦Ù‚\")\n",
        "print(f\"   - Ø­Ø¬Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: ~10-20 Ù…ÙŠØ¬Ø§Ø¨Ø§ÙŠØª\")\n",
        "print(f\"   - ÙˆÙ‚Øª Ø§Ù„ØªÙ†Ø¨Ø¤: <1 Ø«Ø§Ù†ÙŠØ© Ù„ÙƒÙ„ Ø§Ø®ØªØ¨Ø§Ø±\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"ğŸ‰ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§ÙƒØªÙ…Ù„ Ø¨Ù†Ø¬Ø§Ø­!\")\n",
        "print(f\"{'='*80}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W3tqT-23u6S_"
      },
      "execution_count": 5,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGH4vf1laUf+AJGdOC1opv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}