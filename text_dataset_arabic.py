#!/usr/bin/env python3
"""
HIGH-QUALITY EGYPTIAN ARABIC IFS Character Dataset Generator - FIXED ENCODING
Generates perfect Egyptian Arabic dataset with correct encoding and no garbled text
"""

import os
import sys
import csv
import json
import argparse
import random
import re
import time
import hashlib
import glob
from typing import List, Dict, Set, Tuple, Optional
from collections import defaultdict
from pathlib import Path
import unicodedata
import codecs

# ---------------- Configuration ----------------
SEED = 12345
random.seed(SEED)

# Define as a variable that can be modified
RAW_DATA_DIR = "data/raw"
OUT_DIR_DEFAULT = "data/processed"
TOTAL_EXAMPLES = 360000  # 20,000 per character Ã— 18 characters
EXAMPLES_PER_CHARACTER = 20000

# ---------------- All Characters in Arabic ----------------
ALL_CHARACTERS = [
    "Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ", "Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ", "Ù…Ø±Ø¶ÙŠ Ø§Ù„Ù†Ø§Ø³", "Ø§Ù„Ù…ØªØ­ÙƒÙ…", "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…ØªØ²Ù†",
    "Ù…Ø¯Ù…Ù† Ø§Ù„Ø¹Ù…Ù„", "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø´ÙˆØ´", "Ø§Ù„Ù…Ù…Ø§Ø·Ù„", "Ø§Ù„Ø¥ÙØ±Ø§Ø· ÙÙŠ Ø§Ù„Ø£ÙƒÙ„", "Ù…Ø¯Ù…Ù† Ø§Ù„Ø£Ù„Ø¹Ø§Ø¨",
    "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯", "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§Ø¦Ù", "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡Ù…Ù„", "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø®Ø²ÙŠ", "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø«Ù‚ÙÙ‘ÙÙ„",
    "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯", "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ØºÙŠÙˆØ±", "Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ù…Ø¬Ø±ÙˆØ­"
]

# Character mapping for consistency
CHARACTER_MAPPING = {
    "Inner Critic": "Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ",
    "Perfectionist": "Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ", 
    "People-Pleaser": "Ù…Ø±Ø¶ÙŠ Ø§Ù„Ù†Ø§Ø³",
    "Controller": "Ø§Ù„Ù…ØªØ­ÙƒÙ…",
    "Stoic Part": "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…ØªØ²Ù†",
    "Workaholic": "Ù…Ø¯Ù…Ù† Ø§Ù„Ø¹Ù…Ù„",
    "Confused Part": "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø´ÙˆØ´",
    "Procrastinator": "Ø§Ù„Ù…Ù…Ø§Ø·Ù„",
    "Overeater/Binger": "Ø§Ù„Ø¥ÙØ±Ø§Ø· ÙÙŠ Ø§Ù„Ø£ÙƒÙ„",
    "Excessive Gamer": "Ù…Ø¯Ù…Ù† Ø§Ù„Ø£Ù„Ø¹Ø§Ø¨",
    "Lonely Part": "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯",
    "Fearful Part": "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§Ø¦Ù",
    "Neglected Part": "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡Ù…Ù„",
    "Ashamed Part": "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø®Ø²ÙŠ",
    "Overwhelmed Part": "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø«Ù‚ÙÙ‘ÙÙ„",
    "Dependent Part": "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯",
    "Jealous Part": "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ØºÙŠÙˆØ±",
    "Wounded Child": "Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ù…Ø¬Ø±ÙˆØ­"
}

# Emotion mapping in Arabic
EMOTION_MAPPING = {
    "sadness": "Ø­Ø²Ù†",
    "fear": "Ø®ÙˆÙ",
    "anger": "ØºØ¶Ø¨",
    "neutral": "Ù…Ø­Ø§ÙŠØ¯"
}

# Category mapping in Arabic
CATEGORY_MAPPING = {
    "critical": "Ø§Ù†ØªÙ‚Ø§Ø¯ÙŠ",
    "anxious": "Ù‚Ù„Ù‚",
    "protective": "ÙˆÙ‚Ø§Ø¦ÙŠ", 
    "compulsive": "Ù‚ÙÙ‡Ù’Ø±ÙŠ",
    "avoidant": "ØªØ¬Ù†Ø¨ÙŠ",
    "disoriented": "Ù…Ø´ÙˆØ´",
    "sad": "Ø­Ø²ÙŠÙ†",
    "shame": "Ø®Ø²ÙŠ",
    "vulnerable": "Ù‡Ø´"
}

# ---------------- Character Core Patterns in Proper Egyptian Arabic ----------------
CHARACTER_PATTERNS = {
    "Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ": {
        "emotion": "Ø­Ø²Ù†",
        "category": "Ø§Ù†ØªÙ‚Ø§Ø¯ÙŠ",
        "core_phrases": [
            "Ù…Ø´ ÙƒÙØ§ÙŠØ© ÙƒÙˆÙŠØ³", "ÙØ§Ø´Ù„", "Ù…Ø´ Ù‚ÙŠÙ…", "Ø®ÙŠØ¨Ø© Ø£Ù…Ù„", "ØºÙŠØ± ÙƒØ§ÙÙŠ",
            "Ù…Ø´ ÙƒÙˆÙŠØ³ Ø£Ø¨Ø¯Ù‹Ø§", "Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ø¨ÙØ´Ù„", "Ù…Ø¹Ø±ÙØ´ Ø£Ø¹Ù…Ù„ Ø­Ø§Ø¬Ø© ØµØ­", "Ø¹Ø¨Ø¡",
            "Ù…Ø´ Ù…ÙÙŠØ¯", "Ù…Ø´ ÙÙŠÙ‡ Ø£Ù…Ù„", "ØºÙŠØ± ÙƒÙØ¡", "Ø¨Ù‡ Ø¹ÙŠÙˆØ¨", "Ø¨Ø®Ø±Ø¨ Ø¹Ù„Ù‰ Ù†ÙØ³ÙŠ",
            "Ø£Ø³ÙˆØ£ Ø¹Ø¯Ùˆ Ù„ÙŠØ§", "Ù…Ø³ØªØ­Ù‚Ø´", "Ù…Ø´ Ù‚ÙŠÙ…", "ÙÙŠÙ‡ Ø¥ÙŠÙ‡ ÙÙŠÙ‘ÙØŸ",
            "Ø§Ù„ØºÙŠØ± Ø£Ø­Ø³Ù† Ù…Ù†ÙŠ", "ÙÙŠÙ‡ Ø¹ÙŠØ¨ Ø¬ÙˆÙ‡Ø±ÙŠ", "Ø¨Ø¹Ù…Ù„ ÙÙˆØ¶Ù‰ ÙÙŠ ÙƒÙ„ Ø­Ø§Ø¬Ø©"
        ],
        "templates": [
            "Ø¨Ø­Ø³ Ø¥Ù†ÙŠ {}",
            "Ø£Ù†Ø§ Ø¯Ø§ÙŠÙ…Ø§Ù‹ {}",
            "Ù…Ù‡Ù…Ø§ Ø£Ø¹Ù…Ù„ØŒ Ø¨Ø­Ø³ Ø¥Ù†ÙŠ {}",
            "Ø§Ù„ØµÙˆØª Ø¬ÙˆØ§ÙŠØ§ Ø¨ÙŠÙ‚ÙˆÙ„ÙŠ Ø¥Ù†ÙŠ {}",
            "Ù…Ø´ Ø¹Ø§Ø±Ù Ø£Ø¨Ø¹Ø¯ Ø¹Ù† ÙÙƒØ±Ø© Ø¥Ù†Ù‘ÙÙŠ {}",
            "Ù…Ù† Ø¬ÙˆØ§ØŒ Ø¨Ø¢Ù…Ù† Ø¥Ù†ÙŠ {}",
            "Ø¯Ø§ÙŠÙ…Ø§Ù‹ Ù‚Ù„ÙÙ‚ Ø¥Ù†ÙŠ {}",
            "ÙÙƒØ±Ø© Ø¥Ù†Ù‘ÙÙŠ {} Ù…Ø´ Ø¨ØªØ¨Ø¹Ø¯ Ø¹Ù†ÙŠ",
            "Ø¨Ø¹Ø§Ù†ÙŠ Ù…Ù† Ø¥Ù†Ù‘ÙÙŠ {}",
            "Ø§ØªÙ‚Ø§Ù„ÙŠ Ø¥Ù†ÙŠ {} Ù…Ù† Ø²Ù…Ø§Ù† ÙØ¨Ù‚ÙŠØª Ø£ØµØ¯Ù‚"
        ]
    },
    
    "Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ": {
        "emotion": "Ø®ÙˆÙ",
        "category": "Ù‚Ù„Ù‚",
        "core_phrases": [
            "Ù„Ø§Ø²Ù… ÙŠÙƒÙˆÙ† Ù…Ø«Ø§Ù„ÙŠ", "Ù…Ø´ ÙÙŠÙ‡ Ø¹ÙŠÙˆØ¨", "Ù…ÙÙŠØ´ Ø£Ø®Ø·Ø§Ø¡", "Ø¸Ø¨Ø· ØªÙ…Ø§Ù…", "ÙƒÙ„ ØªÙØµÙŠÙ„Ø©",
            "Ø£Ø¹ÙŠØ¯ ÙƒÙ„ Ø­Ø§Ø¬Ø©", "Ø§Ù„ÙƒÙˆÙŠØ³ Ù…Ø´ ÙƒÙØ§ÙŠØ©", "Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø¥Ù…Ø§ Ù…Ø«Ø§Ù„ÙŠ Ø£Ùˆ ÙˆÙ„Ø§ Ø­Ø§Ø¬Ø©", "Ù…Ø¹Ø§ÙŠÙŠØ± Ø¹Ø§Ù„ÙŠØ©",
            "Ù…Ø´ Ø¹Ø§Ø±Ù Ø£Ù‚Ø¨Ù„ Ø£ÙŠ Ø¹ÙŠØ¨", "Ù…Ù‡ÙˆÙˆØ³ Ø¨Ø§Ù„ØªÙØ§ØµÙŠÙ„", "Ù„Ø§Ø²Ù… ÙŠÙƒÙˆÙ† Ù…Ø´ ÙÙŠÙ‡ Ø£ÙŠ Ø¹ÙŠØ¨", "Ø®Ø§ÙŠÙ Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡",
            "Ù…ÙÙŠØ´ Ø­Ø§Ø¬Ø© ÙƒÙˆÙŠØ³Ø© ÙƒÙØ§ÙŠØ©", "Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ø¨Ø±Ø§Ø¬Ø¹", "Ø§Ù„Ø¯Ù‚Ø© Ù…Ù‡Ù…Ø©", "ØµÙØ± ØªØ³Ø§Ù…Ø­ ÙÙŠ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡"
        ],
        "templates": [
            "ÙƒÙ„ Ø­Ø§Ø¬Ø© {}",
            "Ø£Ù†Ø§ Ù…Ø­ØªØ§Ø¬ ÙƒÙ„ Ø­Ø§Ø¬Ø© ØªÙƒÙˆÙ† {}",
            "Ù…Ø´ Ù‡Ù‚Ø¯Ù‘ÙÙ… Ø´ØºÙ„ Ø¥Ù„Ø§ Ù„Ùˆ Ù‡Ùˆ {}",
            "ÙÙƒØ±Ø© {} Ø¨ØªØ®ÙˆÙÙ†ÙŠ",
            "Ø¨Ù‚Ø¹Ø¯ Ø³Ø§Ø¹Ø§Øª Ø¹Ø´Ø§Ù† Ø£ØªØ£ÙƒØ¯ Ø¥Ù† Ø§Ù„Ø­Ø§Ø¬Ø§Øª {}",
            "Ø´ØºÙ„ÙŠ Ù„Ø§Ø²Ù… ÙŠÙƒÙˆÙ† {} ÙˆØ¥Ù„Ø§ Ù…Ø´ Ù‡Ø´Ø§Ø±ÙƒÙ‡",
            "Ø¯Ø§ÙŠÙ…Ø§Ù‹ Ù‚Ù„ÙÙ‚ Ø¹Ù„Ù‰ {}",
            "{} Ù…Ø´ Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ù…Ø³Ø§ÙˆÙ…Ø© Ø¨Ø§Ù„Ù†Ø³Ø¨Ø§Ù„ÙŠ",
            "Ù…Ø´ Ù‡Ø¹Ø±Ù Ø£Ø±ØªØ§Ø­ Ø¥Ù„Ø§ Ù„Ù…Ø§ Ø§Ù„Ø­Ø§Ø¬Ø§Øª {}",
            "{} Ø¨ØªØ­ÙƒÙ‘ÙÙ… ÙÙŠ ÙƒÙ„ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø´ØºÙ„"
        ]
    },
    
    "Ù…Ø±Ø¶ÙŠ Ø§Ù„Ù†Ø§Ø³": {
        "emotion": "Ø®ÙˆÙ",
        "category": "Ù‚Ù„Ù‚",
        "core_phrases": [
            "Ø¨Ù‚ÙˆÙ„ Ø¢Ù‡", "Ø¨ØªØ¬Ù†Ø¨ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„", "Ù…Ø­ØªØ§Ø¬ Ù…ÙˆØ§ÙÙ‚Ø©", "Ø¨ÙØ±Ø¶ÙÙŠ Ø§Ù„Ù†Ø§Ø³", "Ø§Ø­ØªÙŠØ§Ø¬Ø§Øª Ø§Ù„Ù†Ø§Ø³",
            "Ø¹Ø§ÙŠØ² Ø§Ù„ÙƒÙ„ ÙŠØ­Ø¨Ù†ÙŠ", "Ù…Ø´ Ø¹Ø§Ø±Ù Ø£Ù‚ÙˆÙ„ Ù„Ø£", "Ø¨Ø®Ù„ÙŠ Ø§Ù„Ù†Ø§Ø³ ØªÙÙ‚Ø¯Ù‘ÙÙ… Ø¹Ù„ÙŠÙ‘Ù", "Ù…Ø³Ø¤ÙˆÙ„ Ø¹Ù† Ù…Ø´Ø§Ø¹Ø±Ù‡Ù…",
            "Ø®Ø§ÙŠÙ Ù…Ù† Ø§Ù„Ø±ÙØ¶", "Ø¨Ø¶Ø­Ù‘ÙÙŠ Ø¨Ø§Ø­ØªÙŠØ§Ø¬Ø§ØªÙŠ", "Ø¨Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ù„Ø§Ù…", "Ø¨ØºÙŠØ± Ù†ÙØ³ÙŠ Ø¹Ø´Ø§Ù† Ø£Ù†Ø§Ø³Ø¨",
            "Ø¨ØªØ¬Ù†Ø¨ Ø§Ù„Ø®Ù„Ø§ÙØ§Øª", "Ù…Ø­ØªØ§Ø¬ ØªØ£ÙƒÙŠØ¯", "Ø¨Ù‚Ø¯Ù‘ÙÙ… Ø§Ù„Ù†Ø§Ø³ Ø¹Ù„Ù‰ Ù†ÙØ³ÙŠ",
            "Ø®Ø§ÙŠÙ Ù…Ù† Ø§Ù„Ø§Ø³ØªÙ†ÙƒØ§Ø±", "Ø¨Ø®Ø¯Ù… Ø§Ù„ÙƒÙ„", "Ø¨Ù†ÙØ³Ù‰ Ù†ÙØ³ÙŠ ÙÙŠ Ø§Ù„Ù†Ø§Ø³"
        ],
        "templates": [
            "Ø£Ù†Ø§ Ø¯Ø§ÙŠÙ…Ø§Ù‹ {}",
            "Ù…Ø´ Ø¹Ø§Ø±Ù Ø£ÙˆÙ‚Ù Ù†ÙØ³ÙŠ Ø¹Ù† {}",
            "Ø§Ø­ØªÙŠØ§Ø¬ÙŠ Ø¥Ù†Ù‘ÙÙŠ {} Ø¨ÙŠØ³ÙŠØ·Ø± Ø¹Ù„Ù‰ Ø­ÙŠØ§ØªÙŠ",
            "Ø£Ù†Ø§ Ø®Ø§ÙŠÙ Ù…Ù† {}",
            "{} Ù‡Ùˆ ÙˆØ¶Ø¹ÙŠ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ",
            "Ø¨Ø­Ø³ Ø¥Ù†ÙŠ Ù…Ø¬Ø¨ÙˆØ± Ø¥Ù†ÙŠ {}",
            "Ø£ÙƒØ¨Ø± Ø®ÙˆÙ Ø¹Ù†Ø¯ÙŠ Ù‡Ùˆ {}",
            "Ø¨ÙÙ†ÙŠÙ‘ÙØª Ø­ÙŠØ§ØªÙŠ Ø¹Ù„Ù‰ {}",
            "{} Ø¨Ø­Ø³Ù‡ Ø²ÙŠ Ø§Ù„Ø¨Ù‚Ø§Ø¡ Ø¨Ø§Ù„Ù†Ø³Ø¨Ø§Ù„ÙŠ",
            "Ù…Ø¹Ø±ÙØ´ Ø£ÙˆÙ‚Ù {}"
        ]
    },
    
    "Ø§Ù„Ù…ØªØ­ÙƒÙ…": {
        "emotion": "ØºØ¶Ø¨",
        "category": "ÙˆÙ‚Ø§Ø¦ÙŠ",
        "core_phrases": [
            "Ù…Ø³ÙŠØ·Ø±", "Ø·Ø±ÙŠÙ‚ØªÙŠ", "Ø¨ØªØ­ÙƒÙ‘ÙÙ… ÙÙŠ ÙƒÙ„ ØµØºÙŠØ±Ø© ÙˆÙƒØ¨ÙŠØ±Ø©", "Ù…Ø­ØªØ§Ø¬ Ù†Ø¸Ø§Ù…", "Ø¨Ø´Ø±Ù Ø¹Ù„Ù‰ ÙƒÙ„ Ø­Ø§Ø¬Ø©",
            "ÙÙ‚Ø¯Ø§Ù† Ø§Ù„Ø³ÙŠØ·Ø±Ø© Ø¨ÙŠØ®ÙˆÙÙ†ÙŠ", "Ø§Ù„ÙÙˆØ¶Ù‰ Ø¨ØªÙƒÙˆÙ† Ù†ØªÙŠØ¬Ø©", "Ù…Ø´ Ø¹Ø§Ø±Ù Ø£ÙˆÙƒÙ‘ÙÙ„", "Ù…Ø­ØªØ§Ø¬ Ø£Ø¯ÙŠØ±",
            "ÙƒÙ„ Ø­Ø§Ø¬Ø© Ù„Ø§Ø²Ù… ØªÙØªÙ’Ø¨ÙØ¹ Ø®Ø·ØªÙŠ", "Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù…ØªØ¹Ø¨Ø§Ù†ÙŠ", "Ø¹Ø¯Ù… Ø§Ù„Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆÙ‚Ø¹ Ø®Ø·Ø±",
            "Ù„Ø§Ø²Ù… Ø£Ø´Ø±Ù", "Ø§Ù„Ø­Ø§Ø¬Ø§Øª Ø¨ØªÙ†Ù‡Ø§Ø± Ù…Ù† ØºÙŠØ±ÙŠ", "Ù…Ø´ÙƒÙ„Ø§Øª Ø«Ù‚Ø©", "Ù…Ø­ØªØ§Ø¬ ØªØ±ØªÙŠØ¨",
            "Ø§Ù„Ù†Ø¸Ø§Ù… Ø¯Ù‡ Ø£Ø­Ø³Ù† Ø­Ø§Ø¬Ø©", "Ù‚Ù„ÙÙ‚ Ù„Ù…Ø§ Ù…Ø´ Ø£Ù†Ø§ Ø§Ù„Ù…Ø³ÙŠØ·Ø±", "Ø¨ÙØ¶Ù„ Ø§Ù„Ø­Ø§Ø¬Ø§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©"
        ],
        "templates": [
            "Ø£Ù†Ø§ Ù…Ø­ØªØ§Ø¬ Ø£ÙƒÙˆÙ† {}",
            "{} Ø£Ø³Ø§Ø³ÙŠ Ø¨Ø§Ù„Ù†Ø³Ø¨Ø§Ù„ÙŠ",
            "Ù„Ù…Ø§ Ù…Ø´ {}, Ø¨Ø¨Ù‚Ù‰ Ù…Ø°Ø¹ÙˆØ±",
            "Ø§Ø­ØªÙŠØ§Ø¬ÙŠ Ø¥Ù†ÙŠ {} Ø¨ÙŠØ£Ø«Ù‘ÙØ± ÙÙŠ Ø¹Ù„Ø§Ù‚Ø§ØªÙŠ",
            "{} Ø¨Ø­Ø³Ù‡ Ø²ÙŠ Ø§Ù„Ø£Ù…Ø§Ù† Ø¨Ø§Ù„Ù†Ø³Ø¨Ø§Ù„ÙŠ",
            "Ù…Ø´ Ù‡Ø¹Ø±Ù Ø£Ø´ØªØºÙ„ Ù…Ù† ØºÙŠØ± {}",
            "{} Ø¯Ù‡ Ø·Ø±ÙŠÙ‚ØªÙŠ ÙÙŠ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø¹Ø¯Ù… Ø§Ù„ØªØ£ÙƒØ¯",
            "Ø¥ØµØ±Ø§Ø±ÙŠ Ø¹Ù„Ù‰ {} Ø¨ÙŠØ³Ø¨Ø¨ Ù…Ø´Ø§ÙƒÙ„",
            "Ø¨Ø­Ø³ Ø¨Ø§Ù„Ø£Ù…Ø§Ù† Ù„Ù…Ø§ {}",
            "{} Ø¨Ù‚Ù‰ Ø¢Ù„ÙŠØ© ØªØ¹Ø§Ù…Ù„ÙŠ"
        ]
    },
    
    "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…ØªØ²Ù†": {
        "emotion": "Ù…Ø­Ø§ÙŠØ¯",
        "category": "ÙˆÙ‚Ø§Ø¦ÙŠ",
        "core_phrases": [
            "Ù…Ø´ Ø¨Ø¸Ù‡Ø± Ù…Ø´Ø§Ø¹Ø±ÙŠ", "ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±", "Ù…Ù†ÙØµÙ„", "Ù‡Ø§Ø¯Ø¦", "Ù…Ø´ Ø¹Ø§Ø·ÙÙŠ",
            "Ø¨Ø®Ù„ÙŠ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø¬ÙˆØ§ÙŠØ§", "Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø¶Ø¹Ù", "Ø¨ÙØ¶Ù„ Ø§Ù„Ù‡Ø¯ÙˆØ¡", "Ø¨ÙƒØ¨Øª Ø§Ù„Ù…Ø´Ø§Ø¹Ø±",
            "Ø¹Ù‚Ù„Ø§Ù†ÙŠ Ù…Ø´ Ø¹Ø§Ø·ÙÙŠ", "Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø¨ØªØ¹Ø·Ù‘ÙÙ„", "Ø¨Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø±Ø¨Ø§Ø·Ø© Ø¬Ø£Ø´ÙŠ", "Ù…Ø³Ø§ÙØ© Ø¹Ø§Ø·ÙÙŠØ©",
            "Ù…Ø´ Ø¨Ø¹Ø¨Ù‘ÙØ± Ø¹Ù† Ù…Ø´Ø§Ø¹Ø±ÙŠ", "Ø¨ÙØ¶Ù„ Ø§Ù„ÙˆØ¬Ù‡ Ø§Ù„Ù‡Ø§Ø¯Ø¦", "Ø­ÙˆØ§Ø¦Ø· Ø¹Ø§Ø·ÙÙŠØ©", "Ø¨ØªØ¬Ù†Ø¨ Ø§Ù„Ù‡Ø´Ø§Ø´Ø©"
        ],
        "templates": [
            "Ø£Ù†Ø§ Ù†Ø§Ø¯Ø±Ø§Ù‹ Ù…Ø§ {}",
            "{} Ù‡Ùˆ ÙˆØ¶Ø¹ÙŠ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ",
            "Ø§Ù„Ù†Ø§Ø³ Ø¨ØªÙ‚ÙˆÙ„ Ø¹Ù„ÙŠÙ‘ Ø¥Ù†Ù‘ÙÙŠ {} Ø²ÙŠØ§Ø¯Ø©",
            "Ø§ØªØ¹Ù„Ù…Øª Ø¥Ù†ÙŠ {}",
            "{} Ø¨ÙŠØ­Ù…ÙŠÙ†ÙŠ Ù…Ù† Ø§Ù„Ø£Ø°Ù‰",
            "Ø§ØªØ¬Ø§Ù‡ÙŠ Ø¥Ù†Ù‘ÙÙŠ {} Ø¨ÙŠØ¹Ø²Ù„Ù†ÙŠ",
            "Ø¨Ø­Ø³ Ø¨Ø§Ù„Ø£Ù…Ø§Ù† Ù„Ù…Ø§ Ø£Ù†Ø§ {}",
            "{} Ø¨Ù‚Ù‰ Ø¢Ù„ÙŠØ© Ø¯ÙØ§Ø¹ÙŠ",
            "Ù…Ø¹Ø±ÙØ´ Ø£ÙˆÙ‚Ù {}",
            "{} Ø¨Ø­Ø³Ù‡ Ø²ÙŠ Ø§Ù„Ø®ÙŠØ§Ø± Ø§Ù„ÙˆØ­ÙŠØ¯"
        ]
    },
    
    "Ù…Ø¯Ù…Ù† Ø§Ù„Ø¹Ù…Ù„": {
        "emotion": "Ù…Ø­Ø§ÙŠØ¯",
        "category": "Ù‚ÙÙ‡Ù’Ø±ÙŠ",
        "core_phrases": [
            "Ø¯Ø§ÙŠÙ…Ø§Ù‹ Ø¨Ø´ØªØºÙ„", "Ù…Ø´ØºÙˆÙ„ Ø£ÙˆÙŠ", "Ù…Ù†ØªØ¬", "Ù…Ø´ Ø¹Ø§Ø±Ù Ø£ÙˆÙ‚Ù Ø§Ù„Ø´ØºÙ„", "Ù‡ÙˆÙŠØªÙŠ ÙÙŠ Ø§Ù„Ø´ØºÙ„",
            "Ù‚ÙŠÙ…ØªÙŠ Ù…ØªÙ‘ÙØµÙ„Ø© Ø¨Ø§Ù„Ø´ØºÙ„", "Ø¨Ø´ØªØºÙ„ Ø²ÙŠØ§Ø¯Ø©", "Ù…ÙØªÙ‘ÙØ¹Ø±Ù Ø¨Ø§Ù„Ø´ØºÙ„", "Ø§Ù„Ø±Ø§Ø­Ø© Ø¨ØªØ®Ù„ÙŠÙ†ÙŠ Ø£Ø­Ø³ Ø¨Ø§Ù„Ø°Ù†Ø¨",
            "Ù…Ø±ÙƒÙ‘ÙØ² Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ù†Ø¬Ø§Ø²", "Ø¥Ù†ØªØ§Ø¬ÙŠØ© Ø¯Ø§Ø¦Ù…Ø©", "Ø§Ù„Ø´ØºÙ„ Ø¨ÙŠÙƒÙˆÙ† Ø£ÙˆÙ„ÙˆÙŠØ©", "Ø¨ØªÙ†Ø¬Ø­ ÙÙŠ Ø§Ù„Ø´ØºÙ„",
            "Ø®Ø§ÙŠÙ Ù…Ù† Ù‚Ù„Ø© Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ©", "Ø¨Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø´ØºÙ„ ÙÙŠ Ø§Ù„ØªÙ‡Ø±Ø¨", "Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø¥Ø¯Ù…Ø§Ù† Ø§Ù„Ø´ØºÙ„", "Ø¨Ù‡Ù…Ù„ Ù†ÙØ³ÙŠ"
        ],
        "templates": [
            "Ø£Ù†Ø§ Ø¯Ø§ÙŠÙ…Ø§Ù‹ {}",
            "{} Ø¨ÙŠÙØ¹Ø±Ù‘ÙÙ Ù…ÙŠÙ† Ø£Ù†Ø§",
            "Ø§Ø­ØªÙŠØ§Ø¬ÙŠ Ø¥Ù†ÙŠ {} Ø¨ÙŠØ³ØªÙ‡Ù„ÙƒÙ†ÙŠ",
            "Ù…Ø´ Ù…ØªØ®ÙŠÙ„ Ø­ÙŠØ§ØªÙŠ Ù…Ù† ØºÙŠØ± {}",
            "{} Ù‡Ùˆ Ø·Ø±ÙŠÙ‚ØªÙŠ ÙÙŠ Ø¥Ø«Ø¨Ø§Øª Ù‚ÙŠÙ…ØªÙŠ",
            "Ø¨Ø³ØªØ®Ø¯Ù… {} Ø¹Ø´Ø§Ù† Ø£ØªÙ‡Ø±Ø¨ Ù…Ù† Ù…Ø´Ø§ÙƒÙ„ ØªØ§Ù†ÙŠØ©",
            "Ù‡ÙˆÙŠØªÙŠ Ù…ØªÙ‘ÙØµÙ„Ø© Ø¨{}",
            "{} Ø¨Ø­Ø³Ù‡ Ø²ÙŠ Ù‡Ø¯ÙÙŠ",
            "Ø¨Ø­Ø³ Ù†ÙØ³ÙŠ Ø¶Ø§ÙŠØ¹ Ù„Ù…Ø§ Ù…Ø´ {}",
            "{} Ø³ÙŠØ·Ø± Ø¹Ù„Ù‰ Ø­ÙŠØ§ØªÙŠ"
        ]
    },
    
    "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø´ÙˆØ´": {
        "emotion": "Ø®ÙˆÙ",
        "category": "Ù…Ø´ÙˆØ´",
        "core_phrases": [
            "Ù…Ø´ ÙØ§Ù‡Ù…", "Ù…Ø´ Ø¹Ø§Ø±Ù", "Ù…Ø®ØªÙ„Ø· Ø¹Ù„ÙŠÙ‘Ù", "Ù…Ø´ ÙˆØ§Ø¶Ø­", "Ù…ØªØ±Ø¯Ø¯",
            "Ø¯Ù…Ø§ØºÙŠ Ø¶Ø¨Ø§Ø¨ÙŠØ©", "Ù…Ø´ Ø¹Ø§Ø±Ù Ø£ÙÙƒØ± ÙƒÙˆÙŠØ³", "ÙƒÙ„ Ø­Ø§Ø¬Ø© Ù…Ø´ ÙˆØ§Ø¶Ø­Ø©", "Ø¶Ø§Ø¦Ø¹",
            "Ù…Ø´ Ù…ØªØ£ÙƒØ¯", "Ù…Ø´ Ø¹Ø§Ø±Ù Ø§ØªØ¹Ø§Ù…Ù„", "Ø¶Ø¨Ø§Ø¨ ÙÙŠ Ø§Ù„Ù…Ø®", "Ù…Ø´ ÙÙŠÙ‡ ÙˆØ¶ÙˆØ­", "Ù…ØªØ­ÙŠÙ‘ÙØ±",
            "Ù…Ø­ÙŠØ±", "Ù…Ù…Ø²Ù‚ Ø¨ÙŠÙ† Ø§Ø®ØªÙŠØ§Ø±Ø§Øª", "Ù…Ø´ Ø¹Ø§Ø±Ù Ø£ÙÙ‡Ù…", "Ù…Ø´ Ø¹Ø§Ø±Ù Ø£Ù‚Ø±Ø± Ù…Ø¹ ÙƒØªØ± Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª"
        ],
        "templates": [
            "Ø¨Ø­Ø³ Ø¯Ø§ÙŠÙ…Ø§Ù‹ Ø¥Ù†ÙŠ {}",
            "{} Ù‡Ùˆ ÙˆØ¶Ø¹ÙŠ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ",
            "Ø¯Ù…Ø§ØºÙŠ Ø¯Ø§ÙŠÙ…Ø§Ù‹ {}",
            "Ø¨Ø¹Ø§Ù†ÙŠ Ù…Ù† Ø¥Ù†Ù‘ÙÙŠ {}",
            "{} Ø¨ÙŠØ¹Ù‚Ù‘ÙØ¯ Ù‚Ø±Ø§Ø±Ø§ØªÙŠ Ø§Ù„ÙŠÙˆÙ…ÙŠØ©",
            "Ù…Ø´ Ø¹Ø§Ø±Ù Ø£Ù‡Ø±Ø¨ Ù…Ù† Ø­Ø³ Ø¥Ù†Ù‘ÙÙŠ {}",
            "{} Ø¨ÙŠØ£Ø«Ù‘ÙØ± Ø¹Ù„Ù‰ ÙƒÙ„ Ø­Ø§Ø¬Ø© Ø¨Ø¹Ù…Ù„Ù‡Ø§",
            "Ø¥Ù†Ù‘ÙÙŠ {} Ø¯Ø§ÙŠÙ…Ø§Ù‹ Ø¨ÙŠÙˆÙ‚ÙÙ†ÙŠ",
            "ÙŠØ§ Ø±ÙŠØª Ø£Ù‚Ø¯Ø± Ø£ÙˆÙ‚Ù Ø­Ø³ Ø¥Ù†Ù‘ÙÙŠ {}",
            "{} Ø¨ÙŠØ®Ù„ÙŠ Ø±Ø¤ÙŠØªÙŠ Ù…Ø´ ÙˆØ§Ø¶Ø­Ø©"
        ]
    },
    
    "Ø§Ù„Ù…Ù…Ø§Ø·Ù„": {
        "emotion": "Ù…Ø­Ø§ÙŠØ¯",
        "category": "ØªØ¬Ù†Ø¨ÙŠ",
        "core_phrases": [
            "Ù‡Ø¹Ù…Ù„Ù‡ Ø¨Ø¹Ø¯ÙŠÙ†", "Ø¨ÙƒØ±Ø§", "Ø¨Ø£Ø¬Ù‘ÙÙ„", "Ø¨ØªØ£Ø®Ø±", "Ø¢Ø®Ø± Ù„Ø­Ø¸Ø©",
            "Ø¨ØªØ¬Ù†Ø¨ Ù…Ø§ Ø£Ø¨Ø¯Ø¡", "Ù…Ø´ ÙÙŠ Ø§Ù„Ù…ÙˆØ¯", "Ù…Ø³ØªÙ†Ù‰ Ø§Ù„Ø­Ù…Ø§Ø³", "Ø¨Ø´ØªØºÙ„ ØªØ­Øª Ø§Ù„Ø¶ØºØ·",
            "ÙÙŠÙ‡ ÙˆÙ‚Øª ÙƒØªÙŠØ±", "ÙƒÙ…Ø§Ù† ÙˆØ§Ø­Ø¯ Ø¨Ø³", "Ù…Ø³ØªØ¹Ø¯ Ù„Ù‡", "Ø¨ØªØ¬Ù†Ø¨ Ø§Ù„Ù…Ù‡Ø§Ù…", "Ø¨ØªØ£Ø®Ø± ÙÙŠ Ø§Ù„Ø¹Ù…Ù„",
            "Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ù…Ø§Ø·Ù„Ø©", "Ù…Ø³ØªÙ†Ù‰ Ø§Ù„Ù„Ø­Ø¸Ø© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©", "Ø£Ù†Ø§ ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ Ù‡ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹"
        ],
        "templates": [
            "Ø£Ù†Ø§ Ø¯Ø§ÙŠÙ…Ø§Ù‹ {}",
            "Ø¹ÙØ¯Ù‘ÙØªÙŠ Ø¥Ù†ÙŠ {} Ø¨ØªÙŠØ¬ÙŠØ¨ Ù…Ø´Ø§ÙƒÙ„",
            "Ø¨Ù‚ÙˆÙ„ Ù„Ù†ÙØ³ÙŠ Ø¥Ù†Ù‘ÙÙŠ Ù‡{}",
            "{} Ù‡Ùˆ Ø¢Ù„ÙŠØ© ØªØ¹Ø§Ù…Ù„ÙŠ",
            "Ù…Ø´ Ø¹Ø§Ø±Ù Ø£ÙˆÙ‚Ù {}",
            "Ø§ØªØ¬Ø§Ù‡ÙŠ Ø¥Ù†ÙŠ {} Ø¨ÙŠØ£Ø«Ù‘ÙØ± Ø¹Ù„Ù‰ Ø­ÙŠØ§ØªÙŠ",
            "{} Ø¨Ø­Ø³Ù‡ Ø£Ø³Ù‡Ù„ ÙÙŠ Ø§Ù„Ù„Ø­Ø¸Ø©",
            "Ø¹Ø§Ø±Ù Ø¥Ù†Ù‘ÙÙŠ Ù„Ø§Ø²Ù… Ø£ÙˆÙ‚Ù {}",
            "{} Ø¨Ù‚Ù‰ Ù†Ù…Ø·",
            "Ø¨Ø³ØªØ®Ø¯Ù… {} Ø¹Ø´Ø§Ù† Ø£ØªÙ‡Ø±Ø¨ Ù…Ù† Ø¹Ø¯Ù… Ø§Ù„Ø±Ø§Ø­Ø©"
        ]
    },
    
    "Ø§Ù„Ø¥ÙØ±Ø§Ø· ÙÙŠ Ø§Ù„Ø£ÙƒÙ„": {
        "emotion": "Ø­Ø²Ù†",
        "category": "Ù‚ÙÙ‡Ù’Ø±ÙŠ",
        "core_phrases": [
            "Ù…Ø´ Ø¹Ø§Ø±Ù Ø£ÙˆÙ‚Ù Ø§Ù„Ø£ÙƒÙ„", "Ø§Ù„Ø£ÙƒÙ„ Ø§Ù„Ø¹Ø§Ø·ÙÙŠ", "Ø¨Ù†Ù‡Ù…", "Ø¥Ø¯Ù…Ø§Ù† Ø§Ù„Ø£ÙƒÙ„", "Ø¨Ø¢ÙƒÙ„ Ù„Ù…Ø§ Ø£ÙƒÙˆÙ† Ù…ØªÙˆØªØ±",
            "Ø¨Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£ÙƒÙ„ ÙÙŠ Ø§Ù„ØªØ¹Ø§Ù…Ù„", "Ø§Ù„Ø£ÙƒÙ„ Ø¨ÙŠØ±ÙŠØ­Ù†ÙŠ", "Ø¨Ø¢ÙƒÙ„ Ø¹Ø´Ø§Ù† Ø£Ø®Ø¯Ù‘ÙØ± Ù†ÙØ³ÙŠ", "Ø¨Ø¢ÙƒÙ„ ÙÙŠ Ø§Ù„Ø³Ø±",
            "Ø¨Ø­Ø³ Ø¨Ø§Ù„Ø°Ù†Ø¨ Ø¨Ø¹Ø¯ Ø§Ù„Ø£ÙƒÙ„", "Ø®Ø²ÙŠ Ù…Ù† Ø§Ù„Ø£ÙƒÙ„", "Ù…Ø´ Ù…Ø³ÙŠØ·Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø£ÙƒÙ„", "Ø¨Ø³Ø¯ Ù…Ø´Ø§Ø¹Ø±ÙŠ Ø¨Ø§Ù„Ø£ÙƒÙ„",
            "Ø¨ØªØ®Ø¯ÙŠØ± Ù†ÙØ³ÙŠ Ø¨Ø§Ù„Ø£ÙƒÙ„", "Ø§Ù„Ø£ÙƒÙ„ Ù‡Ùˆ Ù‡Ø±ÙˆØ¨ÙŠ", "Ø§Ù„Ø£ÙƒÙ„ Ø§Ù„Ù‚ÙÙ‡Ù’Ø±ÙŠ", "Ø¨Ø¢ÙƒÙ„ Ù„Ù…Ø§ ÙŠÙƒÙˆÙ† Ø¹Ù†Ø¯ÙŠ Ù…Ø´Ø§Ø¹Ø±",
            "Ù…Ø´Ø§ÙƒÙ„ ÙÙŠ Ø¹Ù„Ø§Ù‚ØªÙŠ Ù…Ø¹ Ø§Ù„Ø£ÙƒÙ„"
        ],
        "templates": [
            "Ù„Ù…Ø§ Ø£ÙƒÙˆÙ† Ù…ØªÙˆØªØ±ØŒ Ø£Ù†Ø§ {}",
            "Ø¨Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£ÙƒÙ„ Ø¹Ø´Ø§Ù† {}",
            "Ù†Ù…Ø·ÙŠ ÙÙŠ {} Ø¨ÙŠØ®Ù„ÙŠÙ†ÙŠ Ø£Ø­Ø³ Ø¨Ø§Ù„Ø®Ø²ÙŠ",
            "{} Ù‡Ùˆ Ø·Ø±ÙŠÙ‚ØªÙŠ ÙÙŠ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±",
            "Ø¨Ø­Ø³ Ø¥Ù†ÙŠ Ù…Ø´ Ù…Ø³ÙŠØ·Ø± Ù„Ù…Ø§ {}",
            "{} Ø¨ÙŠÙ‚Ø¯Ù‘ÙÙ… Ø±Ø§Ø­Ø© Ù…Ø¤Ù‚ØªØ©",
            "Ø¹Ù„Ø§Ù‚ØªÙŠ Ù…Ø¹ Ø§Ù„Ø£ÙƒÙ„ ÙÙŠÙ‡Ø§ {}",
            "Ø¨Ø¹Ù…Ù„ Ø¹Ù„Ù‰ Ø¥Ù†ÙŠ Ø£ÙˆÙ‚Ù {}",
            "{} Ø¨Ø­Ø³Ù‡ Ø²ÙŠ Ø§Ù„Ø±Ø§Ø­Ø© Ø§Ù„ÙˆØ­ÙŠØ¯Ø©",
            "Ø¨Ø¹Ø§Ù†ÙŠ Ù…Ù† {} Ø¯Ø§ÙŠÙ…Ø§Ù‹"
        ]
    },
    
    "Ù…Ø¯Ù…Ù† Ø§Ù„Ø£Ù„Ø¹Ø§Ø¨": {
        "emotion": "Ø­Ø²Ù†",
        "category": "Ù‚ÙÙ‡Ù’Ø±ÙŠ",
        "core_phrases": [
            "Ø¨Ù„Ø¹Ø¨ Ø·ÙˆÙ„ Ø§Ù„Ù„ÙŠÙ„", "Ø¨ØªÙ‡Ø±Ø¨ Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ø£Ù„Ø¹Ø§Ø¨", "Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ", "Ø¥Ø¯Ù…Ø§Ù† Ø§Ù„Ø£Ù„Ø¹Ø§Ø¨", "Ù…Ù‡ÙˆÙˆØ³",
            "Ø¨ØªØ¬Ù†Ø¨ Ø§Ù„Ø¹Ø§Ù„Ù… Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ", "Ø§Ù„Ù‡Ø±ÙˆØ¨ Ø§Ù„Ø±Ù‚Ù…ÙŠ", "Ø¨ÙØ¶Ù„ Ø§Ù„Ø­ÙŠØ§Ø© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©", "Ù…ÙƒØ§ÙØ¢Øª Ø§Ù„Ø£Ù„Ø¹Ø§Ø¨", "Ù‡ÙˆÙŠØ© Ø§Ù„Ù„Ø§Ø¹Ø¨",
            "ØªØ¬Ù†Ø¨ Ø§Ù„ÙˆØ§Ù‚Ø¹", "Ø¨Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù„Ø¹Ø§Ø¨ ÙÙŠ Ø§Ù„ØªØ¹Ø§Ù…Ù„", "Ù…Ø´ Ø¹Ø§Ø±Ù Ø£ÙˆÙ‚Ù Ø§Ù„Ù„Ø¹Ø¨", "Ø§Ù„Ø£Ù„Ø¹Ø§Ø¨ Ø£Ù‡Ù… Ù…Ù† Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ÙŠØ§Øª",
            "Ù†Ø¬Ø§Ø­ Ø§ÙØªØ±Ø§Ø¶ÙŠ", "Ù…Ø¬ØªÙ…Ø¹ Ø§Ù„Ø£Ù„Ø¹Ø§Ø¨", "Ø¨Ø±ÙØ¹ Ø§Ù„Ù…Ø³ØªÙˆÙ‰ ÙÙŠ ÙƒÙ„ Ø­Ø§Ø¬Ø©"
        ],
        "templates": [
            "Ø¨Ù‚Ø¹Ø¯ Ø³Ø§Ø¹Ø§Øª {}",
            "{} Ø¨ÙŠØ³Ø§Ø¹Ø¯Ù†ÙŠ Ø£Ù‡Ø±Ø¨ Ù…Ù† Ø§Ù„ÙˆØ§Ù‚Ø¹",
            "Ø¹ÙØ¯Ù‘ÙØªÙŠ Ø¥Ù†ÙŠ {} Ø¨ØªÙ‚Ù„Ù‚Ù†ÙŠ",
            "Ø¨Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù„Ø¹Ø§Ø¨ Ø¹Ø´Ø§Ù† {}",
            "{} Ø¨Ø­Ø³Ù‡ Ø£Ø­Ø³Ù† Ù…Ù† Ø§Ù„Ø­ÙŠØ§Ø© Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©",
            "Ø§ØªØ¬Ø§Ù‡ÙŠ Ø¥Ù†ÙŠ {} Ø¨ÙŠØ£Ø«Ù‘ÙØ± Ø¹Ù„Ù‰ Ù…Ø³Ø¤ÙˆÙ„ÙŠØ§ØªÙŠ",
            "{} Ø¨ÙŠÙ‚Ø¯Ù‘ÙÙ… Ø§Ù„Ù„ÙŠ Ø§Ù„Ø­ÙŠØ§Ø© Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù…Ø´ Ø¨ØªÙ‚Ø¯Ù‘ÙÙ…Ù‡",
            "Ù‚Ù„ÙÙ‚ Ø¹Ù„Ù‰ {}",
            "{} Ø¨Ù‚Ù‰ Ø·Ø±ÙŠÙ‚ÙŠ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ ÙÙŠ Ø§Ù„ØªØ¹Ø§Ù…Ù„",
            "Ø¨Ø­Ø³ Ø¥Ù†ÙŠ Ø¹Ø§ÙŠØ´ Ù„Ù…Ø§ {}"
        ]
    },
    
    "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯": {
        "emotion": "Ø­Ø²Ù†",
        "category": "Ø­Ø²ÙŠÙ†",
        "core_phrases": [
            "ÙˆØ­ÙŠØ¯", "Ù„ÙˆØ­Ø¯ÙŠ", "Ù…Ù†Ø¹Ø²Ù„", "Ù…Ù†Ù‚Ø·Ø¹", "Ù…ÙÙŠØ´ Ø­Ø¯ ÙØ§Ù‡Ù…",
            "Ù…Ø´ Ø¹Ø§ÙŠØ²ÙŠÙ†Ù†ÙŠ", "Ù…Ù‡Ù…Ù„", "Ù…Ø´ Ù…Ø¹Ø§Ù‡Ù…", "ÙØ§Ø¶ÙÙŠ Ø¬ÙˆØ§ÙŠØ§", "Ø¨Ø­Ù† Ù„Ù„Ø§ØªØµØ§Ù„",
            "Ø¨Ø­Ø³ Ø¥Ù†ÙŠ Ù…Ù†ÙØµÙ„", "Ø¹Ø²Ù„Ø© Ø¹Ø§Ø·ÙÙŠØ©", "Ø¨Ø­Ù† Ù„Ù„Ø±ÙÙ‚Ø©", "Ù…Ø´ Ù…Ø­Ø¨ÙˆØ¨",
            "Ø¨Ø±Ù‘ÙØ§ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©", "Ù†Ø§Ù‚ØµÙ„ÙŠ Ø§ØªØµØ§Ù„", "ÙØ±Ø§Øº Ø¬ÙˆØ§ÙŠØ§", "Ù…Ù†Ø¹Ø²Ù„ Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠÙ‹Ø§"
        ],
        "templates": [
            "Ø¨Ø­Ø³ Ø¥Ù†ÙŠ {} Ù‚ÙˆÙŠ",
            "Ø­Ø³ {} Ø¯Ù‡ Ù…Ø´ Ø¨ÙŠØ±ÙˆØ­",
            "Ø¥Ù†Ù‘ÙÙŠ {} Ø¯Ø§ÙŠÙ…Ù‹Ø§ Ø¨ÙŠØ£Ø«Ù‘ÙØ± Ø¹Ù„Ù‰ Ø³Ø¹Ø§Ø¯ØªÙŠ",
            "Ø¨Ø¹Ø§Ù†ÙŠ Ù…Ù† Ø¥Ù†Ù‘ÙÙŠ {}",
            "{} Ø¨ÙŠØµØ¨Øº ÙƒÙ„ ØªØ¬Ø§Ø±Ø¨ÙŠ",
            "Ø­Ø³ Ø¥Ù†Ù‘ÙÙŠ {} Ø¯Ù‡ ÙƒØ¨ÙŠØ± Ø£ÙˆÙŠ",
            "Ù…Ø´ Ø¹Ø§Ø±Ù Ø£Ù‡Ø±Ø¨ Ù…Ù† {}",
            "Ø­ÙŠØ§ØªÙŠ Ù…ØªÙ‘ÙØ¹Ø±ÙØ© Ø¨{}",
            "{} Ø¨ÙŠØ¹Ù‚Ù‘ÙØ¯ Ø§Ù„Ù…ÙˆØ§Ù‚Ù Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©",
            "ÙŠØ§ Ø±ÙŠØª Ø£Ù‚Ø¯Ø± Ø£ÙˆÙ‚Ù Ø¥Ù†Ù‘ÙÙŠ {} Ù‚ÙˆÙŠ"
        ]
    },
    
    "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§Ø¦Ù": {
        "emotion": "Ø®ÙˆÙ",
        "category": "Ù‚Ù„Ù‚",
        "core_phrases": [
            "Ø®Ø§ÙŠÙ", "Ù…Ø°Ø¹ÙˆØ±", "Ù‚Ù„ÙÙ‚", "Ù…ØªÙˆØªØ±", "Ù‡Ù„Ø¹",
            "Ø£Ø³ÙˆØ£ Ø³ÙŠÙ†Ø§Ø±ÙŠÙˆ", "Ø®Ø·Ø±", "Ù…Ø´ Ø¢Ù…Ù†", "Ù‡Ø´", "Ø®ÙˆÙ Ù…Ù† Ø§Ù„ÙØ´Ù„",
            "Ù‚Ù„Ù‚ Ù…ØªÙˆÙ‚Ø¹", "Ù…ÙØ±Ø¹ÙˆØ¨", "Ø¥ÙŠÙ‡ Ù„Ùˆ Ø­ØµÙ„ Ø­Ø§Ø¬Ø© ÙˆØ­Ø´Ø©", "Ù…ØªÙˆØ¬Ø³",
            "Ù‚Ù„Ù‚ Ø¯Ø§Ø¦Ù…", "ØªÙÙƒÙŠØ± ÙƒØ§Ø±Ø«ÙŠ", "Ù…ØªØ¬Ù†Ø¨ Ù„Ù„Ù…Ø®Ø§Ø·Ø±Ø©", "Ø¹Ù‚Ù„ÙŠØ© Ø®Ø§ÙŠÙØ©"
        ],
        "templates": [
            "Ø£Ù†Ø§ Ø¯Ø§ÙŠÙ…Ø§Ù‹ {}",
            "Ø¯Ù…Ø§ØºÙŠ Ø¯Ø§ÙŠÙ…Ø§Ù‹ Ø¨ØªØ±ÙˆØ­ Ù„{}",
            "{} Ø¨ÙŠØ³ÙŠØ·Ø± Ø¹Ù„Ù‰ Ù‚Ø±Ø§Ø±Ø§ØªÙŠ",
            "Ø¨Ø¹ÙŠØ´ ÙÙŠ Ø­Ø§Ù„Ø© {}",
            "Ø§ØªØ¬Ø§Ù‡ÙŠ Ø¥Ù†ÙŠ {} Ø¨ÙŠØ­Ø¯ Ù…Ù†ÙŠ",
            "{} Ø¨Ø­Ø³Ù‡ Ø²ÙŠ Ø±ÙÙŠÙ‚ÙŠ Ø§Ù„Ø¯Ø§Ø¦Ù…",
            "Ù…Ø´ Ø¹Ø§Ø±Ù Ø£Ù‡Ø±Ø¨ Ù…Ù† Ø­Ø³ Ø¥Ù†Ù‘ÙÙŠ {}",
            "Ø­ÙŠØ§ØªÙŠ Ù…ØªØ­ÙƒÙ‘ÙÙ…Ø© Ø¨{}",
            "{} Ø¨ÙŠÙˆÙ‚ÙÙ†ÙŠ Ø¹Ù† Ø§Ù„Ù…Ø¬Ø§Ø²ÙØ©",
            "ÙŠØ§ Ø±ÙŠØª Ø£Ù‚Ø¯Ø± Ø£ÙˆÙ‚Ù Ø¥Ù†Ù‘ÙÙŠ {}"
        ]
    },
    
    "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡Ù…Ù„": {
        "emotion": "Ø­Ø²Ù†",
        "category": "Ø­Ø²ÙŠÙ†",
        "core_phrases": [
            "Ù…Ù‡Ù…Ù„", "Ù…ØªØ¬Ø§Ù‡ÙÙ„", "Ù…Ø´ Ù…Ù„Ø­ÙˆØ¸", "Ù…Ø´ ÙˆØ§Ø®Ø¯ Ø¨Ø§Ù„Ù‡ Ù…Ù†ÙŠ", "Ù…Ø´ Ù…Ù‡Ù…",
            "Ù…Ø´ Ù…Ø±Ø¦ÙŠ", "Ù…Ø´ Ù…ØªØ´Ø§Ù", "Ù…Ø´ Ù…ØªØ³Ù…Ø¹", "Ø¨ÙŠØ¹Ù…Ù„ Ø¹Ø´Ø§Ù† ÙŠØªÙ„ÙØª Ø§Ù†ØªØ¨Ø§Ù‡", "Ø¥Ù‡Ù…Ø§Ù„ Ø¹Ø§Ø·ÙÙŠ",
            "Ø§Ø­ØªÙŠØ§Ø¬Ø§Øª Ù…Ø´ Ù…ØªØ­Ù‚Ù‚Ø©", "Ù…Ø´ Ù…ØªÙ‚Ø¯Ù‘ÙÙ…", "Ù…Ù†Ø³ÙÙŠ", "Ù…Ø´ Ù…ØªØ§Ø¨Ø¹", "Ø¨Ø­Ù† Ù„Ù„Ø§Ù†ØªØ¨Ø§Ù‡",
            "Ø¨Ø­Ø³ Ø¥Ù†ÙŠ Ù…Ø´ Ù…Ù‡Ù…", "Ø´Ø®Øµ ÙÙŠ Ø§Ù„Ø®Ù„ÙÙŠØ©", "Ù…Ø´ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ø£Ø¨Ø¯Ù‹Ø§"
        ],
        "templates": [
            "Ø¯Ø§ÙŠÙ…Ø§Ù‹ Ø¨Ø­Ø³ Ø¥Ù†ÙŠ {}",
            "Ø­Ø³ Ø¥Ù†Ù‘ÙÙŠ {} Ø¯Ù‡ Ø¨ÙŠÙˆØ¬Ø¹",
            "ØªØ¬Ø±Ø¨ØªÙŠ Ù…Ø¹ {} Ø¨ØªØ£Ø«Ù‘ÙØ± Ø¹Ù„Ù‰ Ù‚ÙŠÙ…ØªÙŠ Ø§Ù„Ø°Ø§ØªÙŠØ©",
            "Ø¨Ø¹Ø§Ù†ÙŠ Ù…Ù† Ø¥Ù†Ù‘ÙÙŠ {}",
            "{} ÙƒØ§Ù† Ù†Ù…Ø· ÙÙŠ Ø­ÙŠØ§ØªÙŠ",
            "Ø­Ø³ Ø¥Ù†Ù‘ÙÙŠ {} Ø¯Ù‡ Ù…Ø¤Ù„Ù…",
            "Ù…Ø´ Ø¹Ø§Ø±Ù Ø£Ù‡Ø² Ø­Ø³ Ø¥Ù†Ù‘ÙÙŠ {}",
            "{} Ø¨ÙŠØµØ¹Ø¨ Ø¹Ù„ÙŠÙ‘Ù Ø£Ø·Ù„Ø¨ Ø§Ù„Ù„ÙŠ Ø£Ù†Ø§ Ù…Ø­ØªØ§Ø¬Ù‡",
            "Ø·ÙÙˆÙ„ØªÙŠ ÙƒØ§Ù†Øª ÙÙŠÙ‡Ø§ {} ÙƒØªÙŠØ±",
            "Ø¨Ø¹Ù…Ù„ Ø¹Ù„Ù‰ Ù…Ø´Ø§Ø¹Ø± {}"
        ]
    },
    
    "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø®Ø²ÙŠ": {
        "emotion": "Ø­Ø²Ù†",
        "category": "Ø®Ø²ÙŠ",
        "core_phrases": [
            "Ù…Ø®Ø²ÙŠ", "Ø®Ø²ÙŠ", "Ù…Ø­Ø±Ø¬", "Ù…ØªØ°Ù†Ù‘ÙØ¨", "Ø¨Ù‡ Ø¹ÙŠÙˆØ¨",
            "Ø´Ø®Øµ ÙˆØ­Ø´", "ÙØ´Ù„ Ø£Ø®Ù„Ø§Ù‚ÙŠ", "Ø³Ø± ÙˆØ³Ø®", "Ù…ÙÙ‡Ø§Ù†", "Ù†Ø¯Ù…Ø§Ù†",
            "Ù†Ø§Ø¯Ù…", "ÙˆØ§Ø¹ÙŠ Ù„Ù†ÙØ³ÙŠ", "Ù…Ø­ÙƒÙˆÙ… Ø¹Ù„ÙŠÙ‘Ù", "Ù…Ø¹ÙŠØ¨", "Ù…Ø³ØªØ­Ù‚Ø´",
            "Ù…Ù„ÙˆÙ‘ÙØ«", "Ø®Ø²ÙŠ Ù…Ø®ÙÙŠ", "Ø®Ø²ÙŠ Ø¬ÙˆÙ‡Ø±ÙŠ"
        ],
        "templates": [
            "Ø£Ù†Ø§ Ø¨Ø­Ù…Ù„ {} Ø¹Ù…ÙŠÙ‚",
            "Ø­Ø³ {} Ø¯Ù‡ Ù…Ø´ Ø¨ÙŠØ±ÙˆØ­",
            "Ø­Ø³ÙŠ Ø¨{} Ø¨ÙŠØ£Ø«Ù‘ÙØ± Ø¹Ù„Ù‰ ÙƒÙ„ Ø­Ø§Ø¬Ø©",
            "Ø¨Ø¹Ø§Ù†ÙŠ Ù…Ù† {}",
            "{} Ø¨ÙŠØµØ¨Øº Ø¥Ø²Ø§ÙŠ Ø¨Ø´ÙˆÙ Ù†ÙØ³ÙŠ",
            "ÙˆØ²Ù† {} Ø¯Ù‡ Ø«Ù‚ÙŠÙ„",
            "Ù…Ø´ Ø¹Ø§Ø±Ù Ø£Ù‡Ø±Ø¨ Ù…Ù† {}",
            "{} Ø¨ÙŠØµØ¹Ø¨ Ø¹Ù„ÙŠÙ‘Ù Ø§Ù„Ø§ØªØµØ§Ù„",
            "Ø­ÙŠØ§ØªÙŠ Ù…ØªÙ‘ÙØ³Ù…Ø© Ø¨{}",
            "Ø¨Ø¹Ù…Ù„ Ø¹Ù„Ù‰ Ø£ØªØ´Ø§ÙÙ‰ Ù…Ù† {}"
        ]
    },
    
    "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø«Ù‚ÙÙ‘ÙÙ„": {
        "emotion": "Ø®ÙˆÙ",
        "category": "Ù‚Ù„Ù‚",
        "core_phrases": [
            "Ù…Ø«Ù‚ÙÙ‘ÙÙ„", "ÙƒØªÙŠØ± Ø£ÙˆÙŠ", "Ù…Ø´ Ù‚Ø§Ø¯Ø± Ø£ØªØ­Ù…Ù„", "ØºØ±Ù‚Ø§Ù†", "Ø¶ØºØ·",
            "ØªÙˆØªØ±", "Ø§Ø­ØªØ±Ø§Ù‚ Ù†ÙØ³ÙŠ", "Ù…ØªØ¹ÙØ¨", "Ø­ÙÙ…Ù„ Ø­Ø³ÙŠ Ø²Ø§Ø¦Ø¯", "Ù…Ø·Ø§Ù„Ø¨ Ù…ØªØ¹Ø¯Ø¯Ø©",
            "Ù…ØºÙ’Ø±ÙÙ‚", "Ù…Ø´ ØºØ§Ø·Ø³", "Ù…Ø¯ÙÙˆÙ†", "Ù…Ø­Ø·ÙˆØ· Ø¹Ù„ÙŠÙ‡", "Ø­ÙÙ…Ù„ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø²Ø§Ø¦Ø¯",
            "Ø­ÙÙ…Ù„ Ø¹Ø§Ø·ÙÙŠ Ø²Ø§Ø¦Ø¯", "Ù…Ø´Ø¯ÙˆØ¯ ÙÙŠ Ø§ØªØ¬Ø§Ù‡Ø§Øª", "Ù…ÙÙŠØ´ Ù…Ø³Ø§Ø­Ø©", "Ù…Ø­ØªØ§Ø¬ Ø±Ø§Ø­Ø©"
        ],
        "templates": [
            "Ø¨Ø­Ø³ Ø¯Ø§ÙŠÙ…Ø§Ù‹ Ø¥Ù†ÙŠ {}",
            "Ø­Ø§Ù„Ø© {} Ø¯ÙŠ Ù…Ø±Ù‡Ù‚Ø©",
            "Ø§ØªØ¬Ø§Ù‡ÙŠ Ø¥Ù†ÙŠ {} Ø¨ÙŠØ£Ø«Ù‘ÙØ± Ø¹Ù„Ù‰ ØµØ­ØªÙŠ",
            "Ø¨Ø¹Ø§Ù†ÙŠ Ù…Ù† Ø¥Ù†Ù‘ÙÙŠ {}",
            "{} Ø¨ÙŠØµØ¹Ø¨ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„ÙŠÙˆÙ…ÙŠØ©",
            "Ø­Ø³ Ø¥Ù†Ù‘ÙÙŠ {} Ù…Ø´ Ø¨ÙŠÙØ¶Ù‰",
            "Ù…Ø´ Ø¹Ø§Ø±Ù Ø£ÙˆÙ‚Ù Ø¥Ù†Ù‘ÙÙŠ {}",
            "{} Ø¨ÙŠÙˆÙ‚ÙÙ†ÙŠ Ø¹Ù† Ø¥Ù†Ù‘ÙÙŠ Ø£Ø´ØªØºÙ„ ÙƒÙˆÙŠØ³",
            "Ø­ÙŠØ§ØªÙŠ Ù…ØªÙ‘ÙØ¹Ø±ÙØ© Ø¨{}",
            "Ø¨Ø¹Ù…Ù„ Ø¹Ù„Ù‰ Ø¥Ø¯Ø§Ø±Ø© {} Ø§Ù„Ø¯Ø§Ø¦Ù…"
        ]
    },
    
    "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯": {
        "emotion": "Ø®ÙˆÙ",
        "category": "Ù‚Ù„Ù‚",
        "core_phrases": [
            "Ù…Ø´ Ø¹Ø§Ø±Ù Ø£Ø¹Ù…Ù„ Ù„ÙˆØ­Ø¯ÙŠ", "Ù…Ø­ØªØ§Ø¬ Ù…Ø³Ø§Ø¹Ø¯Ø©", "Ù…Ø¹ØªÙ…Ø¯", "Ø¨ÙØ¹ØªÙ…ÙØ¯ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø§Ø³", "Ù…Ø´ Ø¹Ø§Ø±Ù Ø£ØªØµØ±Ù‘ÙÙ",
            "Ù…Ø¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†", "Ù…Ø­ØªØ§Ø¬ Ù†Ø§Ø³", "Ø®Ø§ÙŠÙ Ù…Ù† Ø§Ù„Ø§Ø³ØªÙ‚Ù„Ø§Ù„ÙŠØ©", "Ù…ØªØ¹Ù„Ù‘ÙÙ‚", "Ù…Ø­ØªØ§Ø¬ Ø¥Ø±Ø´Ø§Ø¯",
            "Ù…Ø´ Ù‚Ø§Ø¯Ø± Ù„ÙˆØ­Ø¯ÙŠ", "Ù…ØªØ´Ø§Ø¨Ùƒ", "Ù…Ø®ØªÙ„Ø·", "Ù…ÙÙŠØ´ Ø§Ø³ØªÙ‚Ù„Ø§Ù„ÙŠØ©", "Ù…Ø´ÙƒÙ„Ø§Øª Ø§Ø±ØªØ¨Ø§Ø·",
            "Ù…Ø­ØªØ§Ø¬ ØªØ£ÙƒÙŠØ¯", "Ø¨Ø¯ÙˆØ± Ø¹Ù„Ù‰ Ù…ÙˆØ§ÙÙ‚Ø©", "Ø¨Ø¯ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù†Ø§Ø³", "Ø¨ÙØªÙ’Ø¨ÙØ¹ Ø§Ù„Ù†Ø§Ø³"
        ],
        "templates": [
            "Ø¨Ø¹Ø§Ù†ÙŠ Ù…Ù† {}",
            "Ø§Ø­ØªÙŠØ§Ø¬ÙŠ Ø¥Ù†ÙŠ {} Ø¨ÙŠÙ‚Ù„Ù‚Ù†ÙŠ",
            "Ø¨Ø­Ø³ Ø¥Ù†ÙŠ {} ÙÙŠ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª",
            "Ù†Ù…Ø·ÙŠ ÙÙŠ {} Ø¨ÙŠÙˆÙ‚ÙÙ†ÙŠ",
            "{} Ø¨ÙŠØ£Ø«Ù‘ÙØ± Ø¹Ù„Ù‰ Ø§Ø³ØªÙ‚Ù„Ø§Ù„ÙŠØªÙŠ",
            "Ù…Ø´ Ø¹Ø§Ø±Ù Ø£ÙˆÙ‚Ù {}",
            "Ø§ØªØ¬Ø§Ù‡ÙŠ Ø¥Ù†ÙŠ {} Ø¨ÙŠÙ‚Ù„Ù‚Ù†ÙŠ",
            "{} Ø¨ÙŠØµØ¹Ø¨ Ø¹Ù„ÙŠÙ‘Ù Ø£ØµØ¯Ù‚ Ù†ÙØ³ÙŠ",
            "Ø¨Ø¹Ù…Ù„ Ø¹Ù„Ù‰ Ø¥Ù†Ù‘ÙÙŠ Ø£ØªØºÙ„Ø¨ Ø¹Ù„Ù‰ {}",
            "{} ÙƒØ§Ù† Ù†Ù…Ø· ÙÙŠ Ø­ÙŠØ§ØªÙŠ ÙƒÙ„Ù‡Ø§"
        ]
    },
    
    "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ØºÙŠÙˆØ±": {
        "emotion": "ØºØ¶Ø¨",
        "category": "ÙˆÙ‚Ø§Ø¦ÙŠ",
        "core_phrases": [
            "ØºÙŠÙˆØ±", "Ø­Ø§Ø³Ø¯", "Ù…Ø³ØªØ§Ø¡", "Ù…Ù‚Ø§Ø±Ù†Ø©", "Ø§Ù„ØºÙŠØ± Ø¹Ù†Ø¯Ù‡Ù… Ø§Ù„Ù„ÙŠ Ø£Ù†Ø§ Ø¹Ø§ÙŠØ²Ù‡",
            "Ù„ÙŠÙ‡ Ù‡Ù… Ù…Ø´ Ø£Ù†Ø§", "ØªÙ…Ù„ÙƒÙŠ", "Ø®Ø§ÙŠÙ Ø£Ø®Ø³Ø±", "ØºÙŠØ± ÙƒØ§ÙÙŠ Ø¨Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©", "Ø§Ù„Ø­ÙŠØ§Ø© Ù…Ø´ Ø¹Ø§Ø¯Ù„Ø©",
            "Ø¨Ø­Ø³Ø¯ Ø§Ù„Ù†Ø§Ø³ Ø¹Ù„Ù‰ Ø§Ù„Ù„ÙŠ Ø¹Ù†Ø¯Ù‡Ù…", "Ø¹ÙŠÙ†Ù‡ Ø®Ø¶Ø±Ø§", "Ø¥Ù‚Ù„ÙŠÙ…ÙŠ", "Ù…Ø´ Ù…ØªØ£ÙƒØ¯", "Ù…Ø´ ÙˆØ§Ø«Ù‚",
            "Ù…Ù†Ø§ÙØ³", "ØªÙ†Ø§ÙØ³", "ÙŠØ§ Ø±ÙŠØª Ø¹Ù†Ø¯ÙŠ", "Ù…Ø± Ù…Ù† Ù†Ø¬Ø§Ø­ Ø§Ù„Ù†Ø§Ø³"
        ],
        "templates": [
            "Ø¨Ø­Ø³ {} Ù„Ù…Ø§ Ø§Ù„Ù†Ø§Ø³ ØªÙ†Ø¬Ø­",
            "Ø§ØªØ¬Ø§Ù‡ÙŠ Ø¥Ù†ÙŠ {} Ø¨ÙŠØ®Ø¬Ù„Ù†ÙŠ",
            "Ø¨Ø¹Ø§Ù†ÙŠ Ù…Ù† Ù…Ø´Ø§Ø¹Ø± {}",
            "{} Ø¨ÙŠØ£Ø«Ù‘ÙØ± Ø¹Ù„Ù‰ Ø¹Ù„Ø§Ù‚Ø§ØªÙŠ",
            "Ù…Ø´ Ø¹Ø§Ø±Ù Ø£ÙˆÙ‚Ù Ø­Ø³ Ø¥Ù†Ù‘ÙÙŠ {}",
            "Ù†Ù…Ø·ÙŠ ÙÙŠ {} Ø¨ÙŠÙ‚Ù„Ù‚Ù†ÙŠ",
            "{} Ø¨ÙŠØµØ¹Ø¨ Ø¹Ù„ÙŠÙ‘Ù Ø£ÙƒÙˆÙ† ÙØ±Ø­Ø§Ù† Ù„Ù„Ù†Ø§Ø³",
            "Ø¨Ø¹Ù…Ù„ Ø¹Ù„Ù‰ {}",
            "{} Ø¨Ø­Ø³Ù‡ Ø²ÙŠ Ø±Ø¯ Ø§Ù„ÙØ¹Ù„ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ",
            "Ù…Ø´Ø§Ø¹Ø± {} Ø¨ØªÙ‚Ù„Ù‚Ù†ÙŠ"
        ]
    },
    
    "Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ù…Ø¬Ø±ÙˆØ­": {
        "emotion": "Ø­Ø²Ù†",
        "category": "Ù‡Ø´",
        "core_phrases": [
            "Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ", "Ø¬Ø±Ø­ Ø§Ù„Ø·ÙÙˆÙ„Ø©", "Ø£Ø°Ù‰ Ù…Ù† Ø§Ù„Ù…Ø§Ø¶ÙŠ", "Ø¬Ø±Ø­ Ø¹Ø§Ø·ÙÙŠ", "ØµØ¯Ù…Ø©",
            "Ø·ÙÙ„ Ù‡Ø´", "Ø·ÙÙ„ Ù…ØªØ±ÙˆÙƒ", "Ø·ÙÙ„ Ù…Ø±ÙÙˆØ¶", "Ø·ÙÙ„ Ù…ØªØ£Ø°ÙŠ", "Ø¬Ø±Ø­ Ø¬ÙˆÙ‡Ø±ÙŠ",
            "Ù†ÙØ³ÙŠ Ø§Ù„ØµØºÙŠØ±Ø©", "Ø·ÙÙ„ Ù…Ø­ØªØ§Ø¬", "Ø·ÙÙ„ Ø®Ø§ÙŠÙ", "Ø·ÙÙ„ ÙˆØ­ÙŠØ¯", "Ø·ÙÙ„ Ù…Ø®Ø²ÙŠ",
            "Ø¬Ø±Ø­ Ù‚Ø¯ÙŠÙ…", "ØµØ¯Ù…Ø© ØªØ·ÙˆØ±ÙŠØ©", "Ø¬Ø±Ø­ Ø§Ø±ØªØ¨Ø§Ø·", "Ø£Ù„Ù… Ù…Ø´ Ù…ØªØ´Ø§ÙÙŠ"
        ],
        "templates": [
            "{} Ø¨ØªØ£Ø«Ù‘ÙØ± Ø¹Ù„ÙŠÙ‘Ù Ù„Ø³Ù‡",
            "Ø£Ù†Ø§ Ø¨Ø­Ù…Ù„ {} Ù…Ø¹Ø§ÙŠØ§",
            "{} Ù…Ù† Ù…Ø§Ø¶ÙŠÙ‘ÙŠ Ø¨ØªØ£Ø«Ù‘ÙØ± Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø§Ø¶Ø±",
            "Ø¨Ø¹Ù…Ù„ Ø¹Ù„Ù‰ Ø£ØªØ´Ø§ÙÙ‰ Ù…Ù† {}",
            "{} Ù…Ø­ØªØ§Ø¬Ø© Ø§Ù†ØªØ¨Ø§Ù‡",
            "{} Ø¨ØªØ´ÙƒÙ‘ÙÙ„ Ø¥Ø²Ø§ÙŠ Ø¨ØªØ±Ø¨Ø·",
            "Ø¨ØªØ¹Ù„Ù… Ø¥Ù†ÙŠ Ø£Ø±Ø¨Ù‘ÙÙŠ {}",
            "ÙˆØ¬Ø¹ {} Ø¨ÙŠØ·Ù„Ø¹ ÙƒØ«ÙŠØ±",
            "{} Ø¨Ø­Ø³Ù‡Ø§ Ø­Ø³Ø§Ø³Ø© Ø£Ø­ÙŠØ§Ù†Ù‹Ø§",
            "Ø¨Ø¹Ù…Ù„ Ù…Ø¹ {}"
        ]
    }
}

# ---------------- Arabic Language Enhancement ----------------
ARABIC_PREFIXES = [
    "Ø¨ØµØ±Ø§Ø­Ø©ØŒ ", "Ø£Ø­ÙŠØ§Ù†Ø§Ù‹ ", "Ù…Ø¤Ø®Ø±Ø§Ù‹ØŒ ", "ÙƒØ«ÙŠØ±Ø§Ù‹ ", "ÙÙŠ Ø§Ù„ÙØªØ±Ø© Ø§Ù„Ø£Ø®ÙŠØ±Ø©ØŒ ",
    "Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø© Ø¥Ù†Ù‘ ", "Ù„Ø§Ø­Ø¸Øª Ø¥Ù†ÙŠ ", "Ø¨Ø­Ø³ Ø¥Ù†ÙŠ ", "Ø¨ÙŠØ¨Ø¯Ùˆ Ø¥Ù†ÙŠ ",
    "ÙÙŠ ØªØ¬Ø±Ø¨ØªÙŠØŒ ", "Ø¨Ø§Ù„Ù†Ø³Ø¨Ø§Ù„ÙŠØŒ ", "Ø´Ø®ØµÙŠØ§Ù‹ØŒ ", "Ø¨ØµØ±Ø§Ø­Ø© Ø´Ø¯ÙŠØ¯Ø©ØŒ ",
    "Ù„Ø§Ø²Ù… Ø£Ø¹ØªØ±Ù Ø¥Ù†ÙŠ ", "Ù…Ø³ØªÙˆØ¹Ø¨ Ø¥Ù†ÙŠ ", "Ø¨ØªØ¹Ù„Ù… Ø¥Ù†ÙŠ ",
    "Ø£ÙƒØªØ± ÙˆØ£ÙƒØªØ±ØŒ ", "ÙÙŠ Ø²ÙŠØ§Ø¯Ø©ØŒ ", "Ù‡Ø°Ù‡ Ø§Ù„Ø£ÙŠØ§Ù…ØŒ ", "Ø¯Ù„ÙˆÙ‚ØªÙŠØŒ "
]

ARABIC_SUFFIXES = [
    ".", " ÙˆØµØ¹Ø¨ Ø£ÙˆÙŠ.", " ÙˆØ¹Ø§Ù…Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹.",
    " ÙˆØ¨ÙŠØ£Ø«Ù‘ÙØ± Ø¹Ù„Ù‰ Ø­ÙŠØ§ØªÙŠ Ø§Ù„ÙŠÙˆÙ…ÙŠØ©.", " ÙˆÙ…Ø´ Ø¹Ø§Ø±Ù Ø£Ø¹Ù…Ù„ Ø¥ÙŠÙ‡ ÙÙŠÙ‡.",
    " ÙˆÙ…Ø±Ù‡Ù‚ Ø£ÙˆÙŠ.", " Ø¨Ø³ Ø¨Ø¹Ù…Ù„ Ø¹Ù„Ù‰ Ø£ØªØºÙŠØ±.",
    " ÙˆØ¨ÙŠØ£Ø«Ù‘ÙØ± Ø¹Ù„Ù‰ Ø¹Ù„Ø§Ù‚Ø§ØªÙŠ.", " Ù…Ø¹ Ø¥Ù†Ù‘ÙÙŠ Ù…Ø´ Ø¹Ø§ÙŠØ² ÙŠÙƒÙˆÙ† ÙƒØ¯Ù‡.",
    " ÙˆÙ…Ø´ Ø¹Ø§Ø±Ù Ø£ØµÙ„Ø­Ù‡.", " ÙˆØ¨ÙŠØ®Ù„ÙŠ ÙƒÙ„ Ø­Ø§Ø¬Ø© Ø£ØµØ¹Ø¨.",
    " ÙˆØ¨ÙŠØ£Ø«Ù‘ÙØ± Ø¹Ù„Ù‰ ØµØ­ØªÙŠ.", " Ø¨Ø³ Ø§Ù„ØªØºÙŠÙŠØ± Ø´Ø¨Ù‡ Ù…Ø³ØªØ­ÙŠÙ„.",
    " ÙˆØªØ¹Ø¨Ø§Ù† Ù…Ù† Ø§Ù„Ø¥Ø­Ø³Ø§Ø³ Ø¯Ù‡.", " ÙˆØ¯Ù‡ Ø§Ù„Ù„ÙŠ Ø®Ù„Ø§Ù†ÙŠ Ø£Ø¯ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø³Ø§Ø¹Ø¯Ø©.",
    " ÙˆØ¯Ù‡ Ø¨Ù‚Ù‰ Ù†Ù…Ø·.", " Ø¨Ø³ Ø§Ù„ÙˆØ¹ÙŠ Ù‡Ùˆ Ø£ÙˆÙ„ Ø®Ø·ÙˆØ©.",
    " ÙˆØ¨ØªØ¹Ù„Ù… Ø£ØªÙ‚Ø¨Ù„Ù‡.", " ÙˆØ¨ÙŠØ­ØªØ§Ø¬ Ø§Ù‡ØªÙ…Ø§Ù… Ø¯Ø§Ø¦Ù….",
    " ÙˆØ¨ØªØ·ÙˆÙ‘ÙØ± Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª Ø£ØªØ¹Ø§Ù…Ù„ Ø¨ÙŠÙ‡Ø§."
]

ARABIC_CONTEXTS = [
    "Ù„Ù…Ø§ Ø£ÙƒÙˆÙ† ÙÙŠ Ø§Ù„Ø´ØºÙ„ØŒ ", "ÙÙŠ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§ØªØŒ ", "Ù…Ø¹ Ø¹ÙŠÙ„ØªÙŠØŒ ",
    "Ø­ÙˆØ§Ù„ÙŠÙ† ØµØ­Ø§Ø¨ÙŠØŒ ", "Ù„Ù…Ø§ Ø£ÙƒÙˆÙ† Ù„ÙˆØ­Ø¯ÙŠØŒ ", "ÙÙŠ Ø§Ù„Ø£ÙˆÙ‚Ø§Øª Ø§Ù„ØµØ¹Ø¨Ø©ØŒ ",
    "Ù„Ù…Ø§ Ø£ÙƒÙˆÙ† Ù…Ù†Ø²Ø¹Ø¬ØŒ ", "ÙÙŠ Ø§Ù„Ù…ÙˆØ§Ù‚Ù Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©ØŒ ", "Ù„Ù…Ø§ Ø£Ù‚Ø§Ø¨Ù„ ØªØ­Ø¯ÙŠØ§ØªØŒ ",
    "ÙÙŠ Ø­ÙŠØ§ØªÙŠ Ø§Ù„ÙŠÙˆÙ…ÙŠØ©ØŒ ", "Ù„Ù…Ø§ Ø¨Ø£Ø®Ø¯ Ù‚Ø±Ø§Ø±Ø§ØªØŒ ", "ÙÙŠ Ù…ÙˆØ§Ù‚Ù Ø¬Ø¯ÙŠØ¯Ø©ØŒ ",
    "Ù„Ù…Ø§ Ø§Ù„Ø­Ø§Ø¬Ø§Øª Ø¨ØªÙƒÙˆÙ† ØµØ¹Ø¨Ø©ØŒ ", "ÙÙŠ Ø§Ù„Ø®Ù„Ø§ÙØ§ØªØŒ ", "Ù„Ù…Ø§ Ø£ÙƒÙˆÙ† ØªØ¹Ø¨Ø§Ù†ØŒ ",
    "Ù„Ù…Ø§ Ø§Ù„ØªÙˆÙ‚Ø¹Ø§Øª Ø¹Ø§Ù„ÙŠØ©ØŒ ", "ÙÙŠ Ø£Ù…Ø§ÙƒÙ† Ù…Ø´ Ù…Ø¹ØªØ§Ø¯ Ø¹Ù„ÙŠÙ‡Ø§ØŒ ", "Ù„Ù…Ø§ Ù„Ø§Ø²Ù… Ø£Ù†Ø¬Ø­ØŒ ",
    "Ù„Ù…Ø§ Ø§Ù„Ù†Ø§Ø³ ØªÙØªÙ’Ø¹ÙÙ…ÙØ¯ Ø¹Ù„ÙŠÙ‘ÙØŒ ", "Ù„Ù…Ø§ Ø£ÙƒÙˆÙ† ØªØ­Øª Ø¶ØºØ·ØŒ "
]

ARABIC_INTENSIFIERS = [
    "Ø¹Ù…ÙŠÙ‚Ø§Ù‹", "Ø¯Ø§ÙŠÙ…Ø§Ù‹", "Ø£Ø¨Ø¯Ø§Ù‹", "Ù…Ø´ Ø£Ø¨Ø¯Ø§Ù‹", "Ø¯Ø§ÙŠÙ…Ø§Ù‹",
    "ÙƒØ¨ÙŠØ± Ø£ÙˆÙŠ", "Ø¹Ù…ÙŠÙ‚ Ø¬Ø¯Ø§Ù‹", "Ù‚ÙˆÙŠ", "Ø¨Ø§Ø³ØªÙ…Ø±Ø§Ø±",
    "Ù…Ø²Ù…Ù†", "Ø¹Ø§Ø¯Ø©Ù‹", "ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹", "Ø§Ù†Ø¹ÙƒØ§Ø³ÙŠØ§Ù‹",
    "Ù…Ù† ØºÙŠØ± ÙˆØ¹ÙŠ", "ØºØ±ÙŠØ²ÙŠØ§Ù‹", "Ù…Ø´ Ù…Ù…ÙƒÙ† Ø§Ù„Ù‡Ø±ÙˆØ¨ Ù…Ù†Ù‡", "ÙÙŠ ÙƒÙ„ Ø­ØªØ©"
]

# ---------------- Arabic Text Processing ----------------
def normalize_arabic_text(text: str) -> str:
    """Normalize Arabic text for consistency."""
    if not text:
        return ""
    
    # Normalize Unicode to composed form
    text = unicodedata.normalize('NFC', text)
    
    # Fix common Arabic spacing issues
    text = re.sub(r'[Ù€_]', '', text)  # Remove tatweel/kashida
    text = re.sub(r'\s+', ' ', text)  # Normalize whitespace
    text = text.strip()
    
    # Ensure proper Arabic punctuation
    text = re.sub(r'([\u0600-\u06FF])\.', r'\1.', text)
    
    return text

def validate_arabic_text(text: str) -> bool:
    """Validate that text contains proper Arabic characters and structure."""
    if not text:
        return False
    
    # Check for minimum Arabic content (at least 30% of characters should be Arabic)
    arabic_chars = sum(1 for c in text if '\u0600' <= c <= '\u06FF' or c in ' .ØŒØ›ØŸ')
    if arabic_chars < len(text) * 0.3:
        return False
    
    # Check for proper sentence ending
    if not text.endswith(('.', '!', 'ØŸ', 'ØŒ')):
        return False
    
    # Check for reasonable length
    if len(text) < 10 or len(text) > 500:
        return False
    
    return True

def ensure_arabic_encoding(text: str) -> str:
    """Ensure text is properly encoded in UTF-8 and contains valid Arabic."""
    try:
        # Try to decode as UTF-8 first
        text.encode('utf-8').decode('utf-8')
    except:
        # If not UTF-8, try to fix it
        try:
            text = text.encode('latin-1').decode('utf-8')
        except:
            text = text.encode('utf-8', 'ignore').decode('utf-8', 'ignore')
    
    return text

# ---------------- Helper Functions ----------------
def clean_text(text: str) -> str:
    """Clean text while preserving meaning."""
    if not text:
        return ""
    
    text = ensure_arabic_encoding(text)
    text = normalize_arabic_text(text)
    
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text)
    
    # Ensure proper sentence structure
    if not text.endswith(('.', '!', 'ØŸ', 'ØŒ')):
        text = text + '.'
    
    return text

def load_raw_datasets(raw_data_dir: str) -> Dict[str, List[str]]:
    """
    Load all text datasets from the raw data directory.
    Returns a dictionary mapping dataset names to lists of sentences.
    """
    raw_data = {}
    
    if not os.path.exists(raw_data_dir):
        print(f"âš ï¸ ØªØ­Ø°ÙŠØ±: Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… '{raw_data_dir}' Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯.")
        print(f"   Ù‡Ù†Ø¹Ù…Ù„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§ØµØ·Ù†Ø§Ø¹ÙŠØ© Ø¨Ø¯Ù„ ÙƒØ¯Ù‡.")
        return raw_data
    
    # Find all text files
    text_files = []
    for ext in ['*.txt', '*.csv', '*.json', '*.jsonl']:
        text_files.extend(glob.glob(os.path.join(raw_data_dir, '**', ext), recursive=True))
        text_files.extend(glob.glob(os.path.join(raw_data_dir, ext)))
    
    print(f"ğŸ“‚ Ù„Ù‚ÙŠÙ†Ø§ {len(text_files)} Ù…Ù„Ù Ø¨ÙŠØ§Ù†Ø§Øª Ø®Ø§Ù…")
    
    for file_path in text_files:
        filename = os.path.basename(file_path)
        try:
            if filename.endswith('.txt'):
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                    # Split into sentences (Arabic punctuation)
                    sentences = re.split(r'[.!ØŸ]+', content)
                    sentences = [clean_text(s) for s in sentences if clean_text(s)]
                    if sentences:
                        raw_data[filename] = sentences
                        print(f"   Ø­ÙÙ…Ù‘ÙÙ„Ù†Ø§ {len(sentences)} Ø¬Ù…Ù„Ø© Ù…Ù† {filename}")
            
            elif filename.endswith('.csv'):
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    reader = csv.reader(f)
                    sentences = []
                    for row in reader:
                        if row:
                            text = row[0] if len(row) > 0 else ' '.join(row)
                            if text and len(text.strip()) > 10:
                                sentences.append(clean_text(text))
                    if sentences:
                        raw_data[filename] = sentences
                        print(f"   Ø­ÙÙ…Ù‘ÙÙ„Ù†Ø§ {len(sentences)} ØµÙ Ù…Ù† {filename}")
            
            elif filename.endswith('.json') or filename.endswith('.jsonl'):
                sentences = []
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    if filename.endswith('.jsonl'):
                        for line in f:
                            try:
                                data = json.loads(line.strip())
                                if isinstance(data, dict) and 'text' in data:
                                    sentences.append(clean_text(data['text']))
                                elif isinstance(data, str):
                                    sentences.append(clean_text(data))
                            except:
                                pass
                    else:
                        try:
                            data = json.load(f)
                            if isinstance(data, list):
                                for item in data:
                                    if isinstance(item, dict) and 'text' in item:
                                        sentences.append(clean_text(item['text']))
                                    elif isinstance(item, str):
                                        sentences.append(clean_text(item))
                            elif isinstance(data, dict):
                                for key, value in data.items():
                                    if isinstance(value, str) and len(value) > 10:
                                        sentences.append(clean_text(value))
                        except:
                            pass
                
                if sentences:
                    raw_data[filename] = sentences
                    print(f"   Ø­ÙÙ…Ù‘ÙÙ„Ù†Ø§ {len(sentences)} Ø¹Ù†ØµØ± Ù…Ù† {filename}")
        
        except Exception as e:
            print(f"   ØºÙ„Ø· ÙÙŠ ØªØ­Ù…ÙŠÙ„ {filename}: {str(e)}")
    
    # Combine all sentences into one large list
    all_sentences = []
    for sentences in raw_data.values():
        all_sentences.extend(sentences)
    
    print(f"\nğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ù…Ø­Ù…Ù„Ø©: {len(all_sentences)}")
    return {"all_sentences": all_sentences, **raw_data}

def extract_patterns_from_text(text: str, character: str) -> List[str]:
    """
    Extract relevant patterns and phrases from raw text for a specific character.
    """
    patterns = []
    
    # Character-specific Arabic keywords
    character_keywords = {
        "Ø§Ù„Ù†Ø§Ù‚Ø¯ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ": ["ÙØ´Ù„", "Ù‚ÙŠÙ…", "ÙƒÙØ§ÙŠØ©", "Ø®ÙŠØ¨Ø©", "Ø¹Ø¨Ø¡"],
        "Ø§Ù„ÙƒÙ…Ø§Ù„ÙŠ": ["Ù…Ø«Ø§Ù„ÙŠ", "Ø¹ÙŠÙˆØ¨", "Ø®Ø·Ø£", "ØªÙØµÙŠÙ„Ø©", "Ù…Ø¹ÙŠØ§Ø±", "ÙŠØ±Ø§Ø¬Ø¹"],
        "Ù…Ø±Ø¶ÙŠ Ø§Ù„Ù†Ø§Ø³": ["ÙŠØ±Ø¶ÙŠ", "Ù…ÙˆØ§ÙÙ‚Ø©", "Ù…Ø´ÙƒÙ„Ø©", "Ø¢Ù‡", "Ø±ÙØ¶", "ØºÙŠØ±"],
        "Ø§Ù„Ù…ØªØ­ÙƒÙ…": ["Ø³ÙŠØ·Ø±Ø©", "ÙŠØªØ­ÙƒÙ…", "Ù†Ø¸Ø§Ù…", "Ø®Ø·Ø©", "ØªØ±ØªÙŠØ¨", "ÙÙˆØ¶Ù‰"],
        "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…ØªØ²Ù†": ["Ù…Ø´Ø§Ø¹Ø±", "Ù…Ù†ÙØµÙ„", "Ù‡Ø§Ø¯Ø¦", "ÙŠÙƒØ¨Øª", "Ø¹Ù‚Ù„Ø§Ù†ÙŠ", "Ø¹Ø§Ø·ÙÙŠ"],
        "Ù…Ø¯Ù…Ù† Ø§Ù„Ø¹Ù…Ù„": ["Ø´ØºÙ„", "Ù…Ø´ØºÙˆÙ„", "Ù…Ù†ØªØ¬", "Ø¥Ù†Ø¬Ø§Ø²", "ÙŠØ´ØªØºÙ„", "Ø±Ø§Ø­Ø©"],
        "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø´ÙˆØ´": ["Ù…Ø´ ÙØ§Ù‡Ù…", "ØºÙŠØ± ÙˆØ§Ø¶Ø­", "Ù…ØªØ±Ø¯Ø¯", "Ø¶Ø¨Ø§Ø¨ÙŠØ©", "ØºÙŠØ± Ù…ØªØ£ÙƒØ¯", "Ø¶Ø§Ø¦Ø¹"],
        "Ø§Ù„Ù…Ù…Ø§Ø·Ù„": ["Ø¨Ø¹Ø¯ÙŠÙ†", "Ø¨ÙƒØ±Ø§", "ÙŠØ¤Ø¬Ù„", "ÙŠØ£Ø®Ø±", "Ù„Ø­Ø¸Ø©", "ÙŠØªØ¬Ù†Ø¨"],
        "Ø§Ù„Ø¥ÙØ±Ø§Ø· ÙÙŠ Ø§Ù„Ø£ÙƒÙ„": ["ÙŠØ£ÙƒÙ„", "Ø£ÙƒÙ„", "Ø¨Ù†Ù‡Ù…", "Ø¹Ø§Ø·ÙÙŠ", "ÙŠØ®Ø¯Ø±", "Ø±Ø§Ø­Ø©"],
        "Ù…Ø¯Ù…Ù† Ø§Ù„Ø£Ù„Ø¹Ø§Ø¨": ["ÙŠÙ„Ø¹Ø¨", "Ø£Ù„Ø¹Ø§Ø¨", "Ø§ÙØªØ±Ø§Ø¶ÙŠ", "ÙŠÙ‡Ø±Ø¨", "Ù…Ø¯Ù…Ù†", "Ù…Ù‡ÙˆÙˆØ³"],
        "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ÙˆØ­ÙŠØ¯": ["ÙˆØ­ÙŠØ¯", "Ù„ÙˆØ­Ø¯ÙŠ", "Ù…Ù†Ø¹Ø²Ù„", "Ù…Ù†Ù‚Ø·Ø¹", "ØºÙŠØ± Ù…Ø±ØºÙˆØ¨", "ÙØ§Ø¶ÙÙŠ"],
        "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø®Ø§Ø¦Ù": ["Ø®Ø§ÙŠÙ", "Ù…Ø°Ø¹ÙˆØ±", "Ù‚Ù„ÙÙ‚", "Ù…ØªÙˆØªØ±", "Ù‡Ù„Ø¹", "Ø®ÙˆÙ"],
        "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ù‡Ù…Ù„": ["Ù…Ù‡Ù…Ù„", "Ù…ØªØ¬Ø§Ù‡ÙÙ„", "ØºÙŠØ± Ù…Ø±Ø¦ÙŠ", "ØºÙŠØ± Ù…Ø³Ù…ÙˆØ¹", "ØºÙŠØ± Ù…Ø±Ø¦ÙŠ", "Ù…Ø´ Ù…ØªØ´Ø§Ù"],
        "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø®Ø²ÙŠ": ["Ù…Ø®Ø²ÙŠ", "Ø®Ø²ÙŠ", "Ù…Ø­Ø±Ø¬", "Ù…ØªØ°Ù†Ù‘ÙØ¨", "ØºÙŠØ± Ù‚ÙŠÙ…", "Ù…Ø¹ÙŠØ¨"],
        "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø«Ù‚ÙÙ‘ÙÙ„": ["Ù…Ø«Ù‚ÙÙ‘ÙÙ„", "ÙƒØ«ÙŠØ±", "ÙŠØªØ­Ù…Ù„", "ØºØ±Ù‚Ø§Ù†", "Ø¶ØºØ·", "ØªÙˆØªØ±"],
        "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…Ø¹ØªÙ…Ø¯": ["ÙŠØ¹ØªÙ…Ø¯", "Ù…Ø³Ø§Ø¹Ø¯Ø©", "Ù…Ø¹ØªÙ…Ø¯", "ÙŠØªØ¹Ù„Ù‚", "Ø¥Ø±Ø´Ø§Ø¯", "Ù…Ø³ØªÙ‚Ù„"],
        "Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„ØºÙŠÙˆØ±": ["ØºÙŠÙˆØ±", "Ø­Ø§Ø³Ø¯", "Ù…Ø³ØªØ§Ø¡", "Ù…Ù‚Ø§Ø±Ù†Ø©", "ØªÙ…Ù„ÙƒÙŠ", "Ù…Ù†Ø§ÙØ³"],
        "Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ù…Ø¬Ø±ÙˆØ­": ["Ø·ÙÙ„", "Ø¬Ø±Ø­", "ØµØ¯Ù…Ø©", "Ù…ØªØ£Ø°ÙŠ", "Ù…ØªØ±ÙˆÙƒ", "Ù…Ø±ÙÙˆØ¶"]
    }
    
    keywords = character_keywords.get(character, [])
    text = ensure_arabic_encoding(text)
    
    # Find sentences containing keywords
    for keyword in keywords:
        if keyword in text:
            # Extract phrase around keyword
            words = text.split()
            for i, word in enumerate(words):
                if keyword in word:
                    # Get context window around keyword
                    start = max(0, i - 3)
                    end = min(len(words), i + 4)
                    phrase = ' '.join(words[start:end])
                    patterns.append(phrase)
                    break
    
    return patterns

def enhance_character_patterns(raw_data: Dict[str, List[str]]) -> Dict:
    """
    Enhance character patterns with patterns extracted from raw data.
    """
    enhanced_patterns = CHARACTER_PATTERNS.copy()
    
    all_sentences = raw_data.get("all_sentences", [])
    if not all_sentences:
        print("âš ï¸ Ù…ÙÙŠØ´ Ø¬Ù…Ù„ Ø®Ø§Ù…ØŒ Ù‡Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø¨Ø³")
        return enhanced_patterns
    
    print("\nğŸ” Ø¨Ù†Ø³ØªØ®Ø±Ø¬ Ø£Ù†Ù…Ø§Ø· Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù…...")
    
    for character in ALL_CHARACTERS:
        character_patterns = []
        
        # Extract patterns from raw sentences
        for sentence in all_sentences:
            patterns = extract_patterns_from_text(sentence, character)
            character_patterns.extend(patterns)
        
        # Remove duplicates and clean
        character_patterns = list(set(character_patterns))
        character_patterns = [clean_text(p) for p in character_patterns if len(clean_text(p)) > 10]
        
        if character_patterns:
            # Add to core phrases (limit to 10 additional patterns)
            additional_patterns = character_patterns[:10]
            enhanced_patterns[character]["core_phrases"].extend(additional_patterns)
            
            # Ensure unique phrases
            enhanced_patterns[character]["core_phrases"] = list(set(enhanced_patterns[character]["core_phrases"]))
            
            print(f"   {character}: Ø¶ÙÙ†Ø§ {len(additional_patterns)} Ù†Ù…Ø·")
    
    return enhanced_patterns

def create_realistic_arabic_variation(base_template: str, core_phrase: str, 
                                     raw_sentences: List[str], variation_num: int) -> str:
    """
    Create a realistic Arabic variation using raw sentence patterns.
    """
    rng = random.Random(SEED + hash(base_template) + hash(core_phrase) + variation_num)
    
    # 30% chance to use raw sentence structure
    if raw_sentences and rng.random() < 0.3:
        raw_sentence = rng.choice(raw_sentences)
        if core_phrase in raw_sentence:
            text = raw_sentence
        else:
            words = raw_sentence.split()
            if len(words) > 3:
                insert_pos = rng.randint(1, len(words) - 2)
                words.insert(insert_pos, core_phrase)
                text = ' '.join(words)
            else:
                text = base_template.format(core_phrase)
    else:
        text = base_template.format(core_phrase)
    
    # Apply random Arabic modifications
    modifications = []
    
    # Add Arabic prefix (30% chance)
    if rng.random() < 0.3:
        prefix = rng.choice(ARABIC_PREFIXES)
        modifications.append(("prefix", prefix))
    
    # Add Arabic suffix (40% chance)
    if rng.random() < 0.4:
        suffix = rng.choice(ARABIC_SUFFIXES)
        modifications.append(("suffix", suffix))
    
    # Add Arabic context (25% chance)
    if rng.random() < 0.25:
        context = rng.choice(ARABIC_CONTEXTS)
        modifications.append(("context", context))
    
    # Apply modifications
    for mod_type, mod_text in modifications:
        if mod_type == "prefix":
            text = mod_text + text
        elif mod_type == "suffix":
            text = text.rstrip('.!ØŸ') + mod_text
        elif mod_type == "context":
            text = mod_text + text
    
    return clean_text(text)

def create_compound_arabic_variation(char1_data: dict, char2_data: dict, 
                                    raw_sentences: List[str], variation_num: int) -> str:
    """Create a realistic compound Arabic variation combining two character patterns."""
    rng = random.Random(SEED + variation_num * 1000)
    
    # 20% chance to use raw sentence for compound
    if raw_sentences and rng.random() < 0.2:
        raw_sentence = rng.choice(raw_sentences)
        char1_phrases = [p for p in char1_data["core_phrases"] if p in raw_sentence]
        char2_phrases = [p for p in char2_data["core_phrases"] if p in raw_sentence]
        
        if char1_phrases and char2_phrases:
            return clean_text(raw_sentence)
    
    # Get random elements from each character
    phrase1 = rng.choice(char1_data["core_phrases"])
    phrase2 = rng.choice(char2_data["core_phrases"])
    
    # Create Arabic compound patterns
    patterns = [
        f"Ø¨Ù„Ø§Ø­Ø¸ Ø¥Ù†ÙŠ Ù„Ù…Ø§ {phrase1}ØŒ ÙØ£Ù†Ø§ ÙƒÙ…Ø§Ù† ÙŠÙ…ÙŠÙ„ Ø¥Ù†ÙŠ {phrase2}",
        f"Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø§ {phrase1} Ùˆ{phrase2} Ù…Ø±ØªØ¨Ø·ÙŠÙ† Ø¹Ù†Ø¯ÙŠ",
        f"Ø¨ÙŠØ¨Ø¯Ùˆ Ø¥Ù†ÙŠ {phrase1} Ùˆ{phrase2} Ù…Ø±ØªØ¨Ø·ÙŠÙ† ÙÙŠ Ø­Ø§Ù„ØªÙŠ",
        f"Ø¨Ø¯Ø£Øª Ø£Ø´ÙˆÙ Ø¥Ø²Ø§ÙŠ {phrase1} Ù…ØªØ¹Ù„Ù‚ Ø¨Ù€{phrase2}",
        f"Ù„Ù…Ø§ Ø¨Ø­Ø³ Ø¥Ù†ÙŠ {phrase1}ØŒ Ø¯Ù‡ Ø¨ÙŠØ¬ÙŠ Ø¨Ù€{phrase2}",
        f"Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù†ÙØ³ÙŠ Ø£Ø´Ø§Ø± Ù„Ø¹Ù„Ø§Ù‚Ø© Ø¨ÙŠÙ† {phrase1} Ùˆ{phrase2}",
        f"Ù„Ø§Ø­Ø¸Øª Ù†Ù…Ø·: Ù„Ù…Ø§ Ø£ÙƒÙˆÙ† {phrase1}ØŒ ÙØ£Ù†Ø§ Ø£ÙƒØªØ± Ø¹Ø±Ø¶Ø© Ø¥Ù†Ù‘ÙÙŠ {phrase2}",
        f"Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨ÙŠÙ† {phrase1} Ùˆ{phrase2} Ø¨Ù‚Ù‰ Ø£ÙˆØ¶Ø­ Ø¨Ø§Ù„Ù†Ø³Ø¨Ø§Ù„ÙŠ",
        f"ØµØ¹Ø¨ Ø£ÙØµÙ„ {phrase1} Ø¹Ù† {phrase2}",
        f"ÙÙŠÙ‡ Ø£ÙŠØ§Ù…ØŒ {phrase1} Ùˆ{phrase2} Ø¨ÙŠÙƒÙˆÙ†ÙˆØ§ Ù…ØªØ´Ø§Ø¨ÙƒÙŠÙ†"
    ]
    
    text = rng.choice(patterns)
    
    # Apply random Arabic modifications
    if rng.random() < 0.3:
        text = rng.choice(ARABIC_PREFIXES) + text
    if rng.random() < 0.4:
        text = text.rstrip('.!ØŸ') + rng.choice(ARABIC_SUFFIXES)
    
    return clean_text(text)

def generate_character_cues(character: str) -> str:
    """Generate character cues string in Arabic."""
    if character in CHARACTER_PATTERNS:
        phrases = CHARACTER_PATTERNS[character]["core_phrases"][:10]
        return "Ø› ".join(phrases)
    return character

def create_arabic_advanced_variation(character: str, core_phrases: List[str], 
                                    variation_num: int) -> str:
    """Create advanced Arabic variations with complex structures."""
    rng = random.Random(SEED + variation_num * 2000)
    
    phrase1 = rng.choice(core_phrases)
    phrase2 = rng.choice(core_phrases)
    
    # Advanced Arabic patterns
    advanced_patterns = [
        f"Ø§Ù„Ù„ÙŠ Ø¨Ø§ÙÙ‡Ù…Ù‡ Ø¯Ù„ÙˆÙ‚ØªÙŠ Ø¥Ù†Ù‘ÙÙŠ {phrase1} ÙˆØ¯Ù‡ Ø§Ù„ÙˆØ¹ÙŠ Ø¯Ù‡ Ø¨ÙŠØºÙŠØ± Ø·Ø±ÙŠÙ‚Ø© ØªØ¹Ø§Ù…Ù„ÙŠ Ù…Ø¹ Ø§Ù„Ø­Ø§Ø¬Ø§Øª",
        f"Ø±Ø­Ù„ØªÙŠ Ù…Ø¹ {phrase1} Ø¹Ù„Ù…ØªÙ†ÙŠ Ø¯Ø±ÙˆØ³ Ù…Ù‡Ù…Ø© Ø¹Ù† Ù†ÙØ³ÙŠ",
        f"Ø§Ù„Ø¹Ù…Ù„ Ø¹Ù„Ù‰ {phrase1} ÙƒØ§Ù† ØªØ­Ø¯ÙŠ Ø¨Ø³ ØªØ­ÙˆÙ‘ÙÙ„ Ù„Ø­Ø§Ø¬Ø© Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©",
        f"Ø¨Ø¯Ø£Øª Ø£ÙÙ‡Ù… Ø¬Ø°ÙˆØ± {phrase1}",
        f"Ù†Ù…Ø· {phrase1} Ø¨ÙŠØ¸Ù‡Ø± Ø£Ù‚ÙˆÙ‰ Ù„Ù…Ø§ {phrase2}",
        f"Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù†ÙØ³ÙŠ Ø¨ÙŠÙ‚ÙˆÙ„ Ø¥Ù†Ù‘ÙÙŠ {phrase1} Ø¬Ø§ÙŠØ¨Ù‡Ø§ Ù…Ù† {phrase2}",
        f"Ø¨Ù„Ø§Ø­Ø¸ Ø¥Ù†ÙŠ {phrase1} Ø¨ÙŠØ­ØµÙ„ Ù„Ù…Ø§ Ø¨Ø­Ø³ Ø¥Ù†ÙŠ {phrase2}",
        f"ØªØ¹Ù„Ù…Øª Ø¥Ø¯Ø§Ø±Ø© {phrase1} ÙƒØ§Ù† Ø¬Ø²Ø¡ Ø£Ø³Ø§Ø³ÙŠ Ù…Ù† Ù†Ù…ÙˆÙŠ",
        f"ÙƒÙ„ Ù…Ø§ Ø¨ÙØ´ÙÙŠ Ù…Ù† {phrase1}ØŒ ÙƒÙ„ Ù…Ø§ Ø¨Ø®ØªØ¨Ø± {phrase2} Ø£Ù‚Ù„",
        f"{phrase1} ÙƒØ§Ù†Øª Ù…Ø³ÙŠØ·Ø±Ø© Ø¹Ù„Ù‰ Ø­ÙŠØ§ØªÙŠØŒ Ù„ÙƒÙ† Ø¯Ù„ÙˆÙ‚ØªÙŠ Ø¨ØªØ¹Ù„Ù… Ø¥Ù†ÙŠ {phrase2}"
    ]
    
    text = rng.choice(advanced_patterns)
    
    # Add Arabic modifications
    if rng.random() < 0.3:
        text = rng.choice(ARABIC_PREFIXES) + text
    if rng.random() < 0.4:
        text = text.rstrip('.!ØŸ') + rng.choice(ARABIC_SUFFIXES)
    
    return clean_text(text)

# ---------------- Main Generation Function ----------------
def generate_high_quality_arabic_dataset(raw_data_dir: str, out_dir: str, examples_per_char: int = EXAMPLES_PER_CHARACTER):
    """Generate high-quality Arabic dataset with perfect language and no duplicates."""
    os.makedirs(out_dir, exist_ok=True)
    output_path = os.path.join(out_dir, "ifs_arabic_dataset.csv")
    
    print("=" * 70)
    print("ğŸ¯ Ø¨ÙÙ†Ù’Ø´ÙØ¦ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª IFS Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬ÙˆØ¯Ø©")
    print("=" * 70)
    
    # Load raw data
    print("ğŸ“‚ Ø¨Ù†Ø­Ù…Ù‘ÙÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù…...")
    raw_data = load_raw_datasets(raw_data_dir)
    
    # Enhance character patterns with raw data
    print("\nğŸ”§ Ø¨Ù†Ø­Ø³Ù‘ÙÙ† Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø´Ø®ØµÙŠØ§Øª Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù…...")
    enhanced_patterns = enhance_character_patterns(raw_data)
    
    all_sentences = raw_data.get("all_sentences", [])
    
    print(f"\nğŸ“Š ØªÙƒÙˆÙŠÙ† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:")
    print(f"   Ø¹Ø¯Ø¯ Ø§Ù„Ø´Ø®ØµÙŠØ§Øª: {len(ALL_CHARACTERS)}")
    print(f"   Ø£Ù…Ø«Ù„Ø© Ù„ÙƒÙ„ Ø´Ø®ØµÙŠØ©: {examples_per_char:,}")
    print(f"   Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ù…Ø«Ù„Ø©: {examples_per_char * len(ALL_CHARACTERS):,}")
    print(f"   Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ø®Ø§Ù… Ø§Ù„Ù…ØªØ§Ø­Ø©: {len(all_sentences)}")
    print("=" * 70)
    
    start_time = time.time()
    
    # Use codecs to ensure UTF-8 encoding with BOM for Excel compatibility
    with codecs.open(output_path, "w", encoding="utf-8-sig") as f:
        writer = csv.writer(f)
        writer.writerow(["id", "input_text", "detected_emotion", "inner_char", "character_cues", "category"])
        
        row_id = 1
        total_generated = 0
        global_text_set = set()
        
        for character_idx, character in enumerate(ALL_CHARACTERS):
            char_start_time = time.time()
            print(f"\nğŸ“ Ø¨Ù†Ø´ØªØºÙ„ Ø¹Ù„Ù‰: {character}")
            
            char_data = enhanced_patterns[character]
            emotion = char_data["emotion"]
            category = char_data["category"]
            cues = generate_character_cues(character)
            
            templates = char_data["templates"]
            core_phrases = char_data["core_phrases"]
            
            char_text_set = set()
            examples_generated = 0
            
            # Phase 1: Basic template variations (60% of examples)
            print(f"  Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: ØªÙ†ÙˆÙŠØ¹Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ©...")
            basic_target = int(examples_per_char * 0.6)
            
            template_idx = 0
            phrase_idx = 0
            variation_num = 0
            
            while examples_generated < basic_target:
                template = templates[template_idx % len(templates)]
                phrase = core_phrases[phrase_idx % len(core_phrases)]
                
                text = create_realistic_arabic_variation(template, phrase, all_sentences, variation_num)
                text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
                
                # Validate Arabic text before adding
                if validate_arabic_text(text) and text_hash not in global_text_set and text_hash not in char_text_set:
                    global_text_set.add(text_hash)
                    char_text_set.add(text_hash)
                    
                    writer.writerow([row_id, text, emotion, character, cues, category])
                    row_id += 1
                    examples_generated += 1
                    total_generated += 1
                    
                    if examples_generated % 1000 == 0:
                        print(f"    ÙˆÙ„Ù‘ÙØ¯Ù†Ø§ {examples_generated:,}/{basic_target:,} Ù…Ø«Ø§Ù„ Ø£Ø³Ø§Ø³ÙŠ")
                        f.flush()
                
                variation_num += 1
                template_idx += 1
                phrase_idx += 1
                
                if variation_num > 100000:
                    print(f"    ØªØ­Ø°ÙŠØ±: ÙˆØµÙ„Ù†Ø§ Ù„Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„ØªÙ†ÙˆÙŠØ¹Ø§ØªØŒ Ø¨Ù†Ù†ØªÙ‚Ù„ Ù„Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©")
                    break
            
            # Phase 2: Compound variations (25% of examples)
            print(f"  Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: ØªÙ†ÙˆÙŠØ¹Ø§Øª Ù…Ø±ÙƒØ¨Ø©...")
            compound_target = int(examples_per_char * 0.25)
            
            other_chars = [c for c in ALL_CHARACTERS if c != character]
            compound_variation_num = 0
            
            while examples_generated < (basic_target + compound_target):
                other_char = random.choice(other_chars)
                other_data = enhanced_patterns[other_char]
                
                text = create_compound_arabic_variation(char_data, other_data, all_sentences, compound_variation_num)
                text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
                
                if validate_arabic_text(text) and text_hash not in global_text_set and text_hash not in char_text_set:
                    global_text_set.add(text_hash)
                    char_text_set.add(text_hash)
                    
                    writer.writerow([row_id, text, emotion, character, cues, category])
                    row_id += 1
                    examples_generated += 1
                    total_generated += 1
                    
                    if examples_generated % 1000 == 0:
                        print(f"    ÙˆÙ„Ù‘ÙØ¯Ù†Ø§ {examples_generated - basic_target:,}/{compound_target:,} Ù…Ø«Ø§Ù„ Ù…Ø±ÙƒØ¨")
                        f.flush()
                
                compound_variation_num += 1
                
                if compound_variation_num > 50000:
                    print(f"    ØªØ­Ø°ÙŠØ±: ÙˆØµÙ„Ù†Ø§ Ù„Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„ØªÙ†ÙˆÙŠØ¹Ø§Øª Ø§Ù„Ù…Ø±ÙƒØ¨Ø©")
                    break
            
            # Phase 3: Advanced variations (15%)
            print(f"  Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: ØªÙ†ÙˆÙŠØ¹Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©...")
            advanced_target = examples_per_char
            advanced_variation_num = 0
            
            while examples_generated < advanced_target:
                text = create_arabic_advanced_variation(character, core_phrases, advanced_variation_num)
                text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
                
                if validate_arabic_text(text) and text_hash not in global_text_set and text_hash not in char_text_set:
                    global_text_set.add(text_hash)
                    char_text_set.add(text_hash)
                    
                    writer.writerow([row_id, text, emotion, character, cues, category])
                    row_id += 1
                    examples_generated += 1
                    total_generated += 1
                    
                    if examples_generated % 1000 == 0:
                        print(f"    ÙˆÙ„Ù‘ÙØ¯Ù†Ø§ {examples_generated:,}/{examples_per_char:,} Ù…Ø«Ø§Ù„ Ø¥Ø¬Ù…Ø§Ù„ÙŠ")
                        f.flush()
                
                advanced_variation_num += 1
                
                if advanced_variation_num > 30000 and examples_generated < advanced_target:
                    print(f"    Ø¨Ù†Ø¹Ù…Ù„ ØªÙ†ÙˆÙŠØ¹Ø§Øª Ø¨Ø³ÙŠØ·Ø© Ø¹Ø´Ø§Ù† Ù†ÙˆØµÙ„ Ù„Ù„Ù‡Ø¯Ù...")
                    while examples_generated < advanced_target:
                        template = random.choice(templates)
                        phrase = random.choice(core_phrases)
                        text = create_realistic_arabic_variation(template, phrase, all_sentences, advanced_variation_num)
                        text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()
                        
                        if validate_arabic_text(text) and text_hash not in global_text_set:
                            global_text_set.add(text_hash)
                            char_text_set.add(text_hash)
                            
                            writer.writerow([row_id, text, emotion, character, cues, category])
                            row_id += 1
                            examples_generated += 1
                            total_generated += 1
                            advanced_variation_num += 1
                    
                    break
            
            char_time = time.time() - char_start_time
            print(f"  âœ… {character}: {examples_generated:,} Ù…Ø«Ø§Ù„ ÙÙŠ {char_time:.1f} Ø«Ø§Ù†ÙŠØ©")
    
    total_time = time.time() - start_time
    
    print("\n" + "=" * 70)
    print("âœ… Ø§ÙƒØªÙ…Ù„ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª!")
    print("=" * 70)
    print(f"ğŸ“Š Ø¥Ø­ØµØ§Ø¡Ø§Øª:")
    print(f"   Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ù…ÙˆÙ„Ø¯Ø©: {total_generated:,}")
    print(f"   Ø£Ù…Ø«Ù„Ø© Ù„ÙƒÙ„ Ø´Ø®ØµÙŠØ©: ~{total_generated // len(ALL_CHARACTERS):,}")
    print(f"   ÙˆÙ‚Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {total_time:.1f} Ø«Ø§Ù†ÙŠØ© ({total_time/60:.1f} Ø¯Ù‚ÙŠÙ‚Ø©)")
    print(f"   Ù…ØªÙˆØ³Ø· Ø§Ù„Ø³Ø±Ø¹Ø©: {total_generated/total_time:.0f} Ù…Ø«Ø§Ù„/Ø«Ø§Ù†ÙŠØ©")
    print(f"\nğŸ“ Ù…Ù„Ù Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬: {output_path}")
    
    # Verify counts and duplicates
    print("\nğŸ” Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø¬ÙˆØ¯Ø©:")
    with codecs.open(output_path, 'r', encoding='utf-8-sig') as f:
        lines = f.readlines()
        line_count = len(lines) - 1
        
        texts = set()
        duplicate_count = 0
        arabic_validation_errors = 0
        reader = csv.reader(lines[1:])
        for row in reader:
            if len(row) > 1:
                text = row[1]
                if text in texts:
                    duplicate_count += 1
                if not validate_arabic_text(text):
                    arabic_validation_errors += 1
                texts.add(text)
    
    print(f"   Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø·Ø± ÙÙŠ CSV: {line_count:,}")
    print(f"   Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ÙØ±ÙŠØ¯Ø©: {len(texts):,}")
    print(f"   ØªÙƒØ±Ø§Ø±Ø§Øª: {duplicate_count}")
    print(f"   Ø£Ø®Ø·Ø§Ø¡ ÙÙŠ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¹Ø±Ø¨ÙŠ: {arabic_validation_errors}")
    
    if line_count == total_generated and duplicate_count == 0 and arabic_validation_errors == 0:
        print("   âœ“ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¹Ø¯ Ù†Ø§Ø¬Ø­ - Ù„Ø§ ØªÙˆØ¬Ø¯ ØªÙƒØ±Ø§Ø±Ø§Øª Ø£Ùˆ Ø£Ø®Ø·Ø§Ø¡")
    else:
        print(f"   âš ï¸ Ù…Ø´Ø§ÙƒÙ„: Ù…ØªÙˆÙ‚Ø¹ {total_generated:,}, Ø­ØµÙ„Ù†Ø§ {line_count:,}, ØªÙƒØ±Ø§Ø±Ø§Øª: {duplicate_count}, Ø£Ø®Ø·Ø§Ø¡ Ø¹Ø±Ø¨ÙŠ: {arabic_validation_errors}")
    
    # Generate mapping files
    generate_mapping_files(out_dir)
    
    # Create a sample preview
    create_sample_preview(output_path, out_dir)
    
    return output_path

def generate_mapping_files(out_dir: str):
    """Generate mapping files for perfect dataset reference."""
    mapping_dir = os.path.join(out_dir, "mappings")
    os.makedirs(mapping_dir, exist_ok=True)
    
    # Character mapping
    char_mapping_path = os.path.join(mapping_dir, "character_mapping.json")
    with open(char_mapping_path, 'w', encoding='utf-8-sig') as f:
        json.dump(CHARACTER_MAPPING, f, ensure_ascii=False, indent=2)
    
    # Emotion mapping
    emotion_mapping_path = os.path.join(mapping_dir, "emotion_mapping.json")
    with open(emotion_mapping_path, 'w', encoding='utf-8-sig') as f:
        json.dump(EMOTION_MAPPING, f, ensure_ascii=False, indent=2)
    
    # Category mapping
    category_mapping_path = os.path.join(mapping_dir, "category_mapping.json")
    with open(category_mapping_path, 'w', encoding='utf-8-sig') as f:
        json.dump(CATEGORY_MAPPING, f, ensure_ascii=False, indent=2)
    
    # Character details
    char_details_path = os.path.join(mapping_dir, "character_details.json")
    char_details = {}
    for char_ar in ALL_CHARACTERS:
        char_en = [k for k, v in CHARACTER_MAPPING.items() if v == char_ar][0]
        char_details[char_ar] = {
            "english_name": char_en,
            "emotion": CHARACTER_PATTERNS[char_ar]["emotion"],
            "category": CHARACTER_PATTERNS[char_ar]["category"],
            "description": f"Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø°ÙŠ ÙŠÙ…Ø«Ù„ {char_ar} ÙÙŠ Ù†Ù…ÙˆØ°Ø¬ Ù†Ø¸Ø§Ù… Ø§Ù„Ø¹Ø§Ø¦Ù„Ø© Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©",
            "core_phrases_count": len(CHARACTER_PATTERNS[char_ar]["core_phrases"]),
            "templates_count": len(CHARACTER_PATTERNS[char_ar]["templates"])
        }
    
    with open(char_details_path, 'w', encoding='utf-8-sig') as f:
        json.dump(char_details, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ“„ Ù…Ù„ÙØ§Øª Ø§Ù„ØªØ¹ÙŠÙŠÙ†:")
    print(f"   âœ“ character_mapping.json - {len(CHARACTER_MAPPING)} ØªØ¹ÙŠÙŠÙ†")
    print(f"   âœ“ emotion_mapping.json - {len(EMOTION_MAPPING)} Ø¹Ø§Ø·ÙØ©")
    print(f"   âœ“ category_mapping.json - {len(CATEGORY_MAPPING)} ÙØ¦Ø©")
    print(f"   âœ“ character_details.json - ØªÙØ§ØµÙŠÙ„ {len(char_details)} Ø´Ø®ØµÙŠØ©")

def create_sample_preview(csv_path: str, out_dir: str, sample_size: int = 50):
    """Create a sample preview of the dataset."""
    preview_path = os.path.join(out_dir, "dataset_preview.txt")
    
    with codecs.open(csv_path, 'r', encoding='utf-8-sig') as f:
        reader = csv.reader(f)
        headers = next(reader)
        samples = []
        
        for i, row in enumerate(reader):
            if i < sample_size:
                samples.append(row)
            else:
                break
    
    with codecs.open(preview_path, 'w', encoding='utf-8-sig') as f:
        f.write("=" * 70 + "\n")
        f.write("Ø¹ÙŠÙ†Ø© Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª IFS Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠ\n")
        f.write("=" * 70 + "\n\n")
        
        for i, row in enumerate(samples, 1):
            f.write(f"Ù…Ø«Ø§Ù„ #{i}:\n")
            f.write(f"  Ø§Ù„Ù†Øµ: {row[1]}\n")
            f.write(f"  Ø§Ù„Ø¹Ø§Ø·ÙØ©: {row[2]}\n")
            f.write(f"  Ø§Ù„Ø´Ø®ØµÙŠØ©: {row[3]}\n")
            f.write(f"  Ø§Ù„ÙØ¦Ø©: {row[5]}\n")
            f.write("-" * 50 + "\n")
    
    print(f"   âœ“ dataset_preview.txt - Ø¹ÙŠÙ†Ø© Ù…Ù† {len(samples)} Ù…Ø«Ø§Ù„")

# ---------------- Test Function ----------------
def test_arabic_encoding():
    """Test Arabic encoding to ensure it's working correctly."""
    test_text = "Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø© Ø¥Ù†Ù‘ Ø¨Ø­Ø³ Ø¥Ù†ÙŠ Ù…Ø´ ÙƒÙØ§ÙŠØ© ÙƒÙˆÙŠØ³."
    
    print("ğŸ” Ø§Ø®ØªØ¨Ø§Ø± ØªØ±Ù…ÙŠØ² Ø§Ù„Ù†Øµ Ø§Ù„Ø¹Ø±Ø¨ÙŠ:")
    print(f"   Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ: {test_text}")
    print(f"   Ø§Ù„Ø·ÙˆÙ„: {len(test_text)} Ø­Ø±Ù")
    print(f"   Ø§Ù„ØªØ±Ù…ÙŠØ² UTF-8: {test_text.encode('utf-8')}")
    print(f"   ØªØµØ­ÙŠØ­ Ø§Ù„ØªØ±Ù…ÙŠØ²: {ensure_arabic_encoding(test_text)}")
    print(f"   ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¹Ø±Ø¨ÙŠ: {validate_arabic_text(test_text)}")
    
    # Test garbled text
    garbled = "Ã˜Â§Ã™â€Ã˜Â­Ã™â€šÃ™Å Ã™â€šÃ˜Â© Ã˜Â¥Ã™â€ Ã™â€˜ Ã˜Â¨Ã˜Â­Ã˜Â³ Ã˜Â¥Ã™â€ Ã™Å  Ã™â€¦Ã˜Â´ Ã™Æ’Ã™ÂÃ˜Â§Ã™Å Ã˜Â© Ã™Æ’Ã™Ë†Ã™Å Ã˜Â³."
    print(f"\n   Ø§Ø®ØªØ¨Ø§Ø± Ù†Øµ Ù…Ø´ÙˆÙ‡: {garbled}")
    fixed = ensure_arabic_encoding(garbled)
    print(f"   Ø¨Ø¹Ø¯ Ø§Ù„ØªØµØ­ÙŠØ­: {fixed}")
    print(f"   Ù‡Ù„ Ù‡Ùˆ Ø¹Ø±Ø¨ÙŠ ØµØ­ÙŠØ­ØŸ: {validate_arabic_text(fixed)}")

# ---------------- Main ----------------
def main():
    parser = argparse.ArgumentParser(description="Generate High-Quality Arabic IFS Character Dataset")
    parser.add_argument("--raw_dir", type=str, default=RAW_DATA_DIR, help="Raw data directory")
    parser.add_argument("--out_dir", type=str, default=OUT_DIR_DEFAULT, help="Output directory")
    parser.add_argument("--examples_per_char", type=int, default=EXAMPLES_PER_CHARACTER, 
                       help=f"Examples per character (default: {EXAMPLES_PER_CHARACTER:,})")
    parser.add_argument("--seed", type=int, default=SEED, help="Random seed")
    parser.add_argument("--test", action="store_true", help="Test Arabic encoding only")
    
    args = parser.parse_args()
    
    # Set seed
    random.seed(args.seed)
    
    if args.test:
        test_arabic_encoding()
        return
    
    print("ğŸ¯ Ù…ÙˆÙ„Ø¯ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø´Ø®ØµÙŠØ§Øª IFS Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø¹Ø§Ù„ÙŠ Ø§Ù„Ø¬ÙˆØ¯Ø©")
    print("=" * 70)
    print(f"Ù‡ÙŠØ¹Ù…Ù„ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… Ù…Ù†: {args.raw_dir}")
    print(f"Ù‡ÙŠÙˆÙ„Ù‘ÙØ¯ {args.examples_per_char * len(ALL_CHARACTERS):,} Ù…Ø«Ø§Ù„")
    print(f"({args.examples_per_char:,} Ù„ÙƒÙ„ Ø´Ø®ØµÙŠØ© Ù…Ù† {len(ALL_CHARACTERS)} Ø´Ø®ØµÙŠØ©)")
    print("Ù…Ù† ØºÙŠØ± ØªÙƒØ±Ø§Ø±Ø§Øª ÙˆÙˆØ§Ù‚Ø¹ÙŠØ© Ù…Ø¹Ø²Ø²Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù….")
    print("=" * 70)
    
    # Check for raw data
    if not os.path.exists(args.raw_dir):
        print(f"\nâš ï¸ ØªØ­Ø°ÙŠØ±: Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… '{args.raw_dir}' Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯.")
        print("   Ø§Ù„Ù†Ø¸Ø§Ù… Ù‡ÙŠØµÙ†Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§ØµØ·Ù†Ø§Ø¹ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©.")
        print("   Ù„Ø£ÙØ¶Ù„ Ù†ØªÙŠØ¬Ø©ØŒ Ø¶ÙŠÙ Ù…Ù„ÙØ§Øª Ù†ØµÙŠØ© Ù„Ù„Ù…Ø¬Ù„Ø¯ Ø¯Ù‡.")
        print("   Ø§Ù„ØµÙŠØº Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø©: .txt, .csv, .json, .jsonl")
        response = input("\nØªÙƒÙ…Ù„ Ø¨Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØ©ØŸ (y/n): ")
        if response.lower() != 'y':
            print("Ø¨Ù†Ø®Ø±Ø¬...")
            return
    
    # Generate dataset
    dataset_path = generate_high_quality_arabic_dataset(args.raw_dir, args.out_dir, args.examples_per_char)
    
    print("\n" + "=" * 70)
    print("ğŸš€ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ø§Ù‡Ø²Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨!")
    print("=" * 70)
    
    # Calculate file size
    try:
        file_size = os.path.getsize(dataset_path) / 1024 / 1024
        with codecs.open(dataset_path, 'r', encoding='utf-8-sig') as f:
            line_count = sum(1 for _ in f) - 1
        
        print(f"\nğŸ“ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§ØªØ®Ø²Ù†Øª ÙÙŠ: {dataset_path}")
        print(f"   Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù: ~{file_size:.1f} Ù…ÙŠØ¬Ø§Ø¨Ø§ÙŠØª")
        print(f"   Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ù…Ø«Ù„Ø©: {line_count:,}")
        print(f"   ØªÙ‚Ø±ÙŠØ¨Ø§Ù‹ {file_size * 1024 / line_count:.1f} ÙƒÙŠÙ„ÙˆØ¨Ø§ÙŠØª Ù„ÙƒÙ„ Ù…Ø«Ø§Ù„")
    except:
        print(f"\nğŸ“ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§ØªØ®Ø²Ù†Øª ÙÙŠ: {dataset_path}")
    
    print("\nğŸ’¡ Ù†ØµØ§Ø¦Ø­ Ù„Ù„ØªØ¯Ø±ÙŠØ¨:")
    print("1. Ø§Ø³ØªØ®Ø¯Ù… Ù…Ø¹ Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª (AraBERT/CAMeLBERT) Ù„Ø£ÙØ¶Ù„ Ù†ØªÙŠØ¬Ø©")
    print("2. Ø¹Ù…ÙˆØ¯ 'character_cues' Ù…Ù…ÙƒÙ† ÙŠØ­Ø³Ù† ÙÙ‡Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")
    print("3. ÙÙƒÙ‘ÙØ± ÙÙŠ Ø§Ù„ØªØ¹Ù„Ù… Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù…Ù‡Ø§Ù… Ù…Ø¹ ÙƒØ´Ù Ø§Ù„Ù…Ø´Ø§Ø¹Ø±")
    print("4. Ù…Ø¹ 360,000 Ù…Ø«Ø§Ù„ØŒ Ø¯Ø±Ù‘ÙØ¨ Ù„Ù…Ø¯Ø© 3-5 Ø¹ØµÙˆØ±")
    print("5. Ø§Ø³ØªØ®Ø¯Ù… Ù…Ø¹Ø¯Ù„ ØªØ¹Ù„Ù… 2e-5 Ù…Ø¹ ØªØ³Ø®ÙŠÙ† Ø®Ø·ÙŠ")
    print("\nğŸ”§ Ù…ÙŠØ²Ø§Øª Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:")
    print("   â€¢ Ù…Ø­Ø³Ù‘ÙÙ† Ø¨Ø£Ù†Ù…Ø§Ø· Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù…")
    print("   â€¢ ØªØ±Ø§ÙƒÙŠØ¨ Ø¬Ù…Ù„ ÙˆØ§Ù‚Ø¹ÙŠØ© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ù…ØµØ±ÙŠ")
    print("   â€¢ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØµÙˆØµ Ù…ÙƒØ±Ø±Ø©")
    print("   â€¢ Ù…Ø·Ø§Ø¨Ù‚Ø© Ù…Ø«Ø§Ù„ÙŠØ© Ø¨ÙŠÙ† Ø§Ù„Ø¹Ø§Ø·ÙØ© ÙˆØ§Ù„Ø´Ø®ØµÙŠØ©")
    print("   â€¢ Ù†ØµÙˆØµ Ø¹Ø±Ø¨ÙŠØ© Ø³Ù„ÙŠÙ…Ø© Ø¨Ø¯ÙˆÙ† Ø±Ù…ÙˆØ² Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©")
    print("   â€¢ Ù…Ù„ÙØ§Øª ØªØ¹ÙŠÙŠÙ† ÙƒØ§Ù…Ù„Ø© Ù„Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„ÙŠÙ‡Ø§")
    print("   â€¢ ØªØ±Ù…ÙŠØ² UTF-8-BOM Ù…ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø¥ÙƒØ³Ù„")
    print("\nğŸ“– ÙƒÙŠÙ ØªØ´ÙˆÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:")
    print(f"   Ø§Ø°Ø§ Ø§Ø³ØªØ®Ø¯Ù…Øª Ø¥ÙƒØ³Ù„: Ø§ÙØªØ­ {dataset_path} Ù…Ø¨Ø§Ø´Ø±Ø©")
    print(f"   Ø§Ø°Ø§ Ø§Ø³ØªØ®Ø¯Ù…Øª Python: pd.read_csv('{dataset_path}', encoding='utf-8-sig')")
    print(f"   Ø§Ø°Ø§ Ø§Ø³ØªØ®Ø¯Ù…Øª Excel Ø¹Ù„Ù‰ Mac: Ø§ÙØªØ­ Ø§Ù„Ù…Ù„Ù ÙˆØ§Ø®ØªØ§Ø± ØªØ±Ù…ÙŠØ² UTF-8")

if __name__ == "__main__":
    main()